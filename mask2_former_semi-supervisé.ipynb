{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bccb277e",
   "metadata": {},
   "source": [
    "## Mask2former(semi-supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa83615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Labeled train dir: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\X_train_uDRk9z9\\images\n",
      "Unlabeled dir:     C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\X_unlabeled_mtkxUlo\\images\n",
      "Test dir:          C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\X_test_xNbnvIa\\images\n",
      "Pretrained:        facebook/mask2former-swin-tiny-ade-semantic\n",
      "Pseudo TH=0.85, lambda_u=0.5\n",
      "Labeled train: 2790 | Val: 1620 | Unlabeled: 1980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\deep-torch\\lib\\site-packages\\transformers\\image_processing_base.py:417: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size', 'reduce_labels'\n",
      "  image_processor = cls(**image_processor_dict)\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-tiny-ade-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([4, 256]) in the model instantiated\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/10 | train_l=18.6849 | train_u=41.4849 | val=15.4774\n",
      "  -> Best saved: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_mask2former_semi.pth\n",
      "Epoch 02/10 | train_l=14.1083 | train_u=30.6973 | val=14.3693\n",
      "  -> Best saved: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_mask2former_semi.pth\n",
      "Epoch 03/10 | train_l=13.2919 | train_u=31.3983 | val=14.7217\n",
      "Epoch 04/10 | train_l=12.4842 | train_u=23.7818 | val=15.1125\n",
      "Epoch 05/10 | train_l=11.9995 | train_u=22.2180 | val=15.1217\n",
      "Epoch 06/10 | train_l=11.6591 | train_u=20.8557 | val=14.9213\n",
      "Epoch 07/10 | train_l=11.2987 | train_u=18.4966 | val=15.2570\n",
      "Epoch 08/10 | train_l=10.9792 | train_u=17.3398 | val=14.8278\n",
      "Epoch 09/10 | train_l=10.6290 | train_u=17.2760 | val=14.8505\n",
      "Epoch 10/10 | train_l=10.3889 | train_u=17.5382 | val=14.5758\n",
      "[OK] submission saved to: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Mask2Former + Semi-Supervised Pseudo Label (Full runnable)0.6663556049927326\n",
    "\n",
    "Data:\n",
    "- Labeled train images:   X_train_uDRk9z9/images (well1-6)\n",
    "- Labeled train labels:   Y_train_T9NrBYo.csv (flatten + -1 padding)\n",
    "- Unlabeled images:       X_unlabeled_mtkxUlo/images (well12-14)\n",
    "- Test images:            X_test_xNbnvIa/images (well7-11)\n",
    "\n",
    "Split:\n",
    "- Train labeled: well1-5\n",
    "- Val labeled:   well6\n",
    "- Unlabeled:     well12-14 (no labels)\n",
    "\n",
    "Output:\n",
    "- submission.csv, each row = one patch\n",
    "- flattened mask, padded to 160*272 with -1\n",
    "\n",
    "Install:\n",
    "    pip install transformers accelerate\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    Mask2FormerForUniversalSegmentation,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 0) Paths & Hyperparameters\n",
    "# =========================\n",
    "DATA_ROOT = Path(r\"C:\\Users\\lenovo\\Desktop\\deep_datachallenge\")\n",
    "\n",
    "TRAIN_IMAGES_DIR = DATA_ROOT / \"X_train_uDRk9z9\" / \"images\"\n",
    "TEST_IMAGES_DIR  = DATA_ROOT / \"X_test_xNbnvIa\" / \"images\"\n",
    "UNLABELED_DIR     = DATA_ROOT / \"X_unlabeled_mtkxUlo\" / \"images\"\n",
    "Y_TRAIN_CSV       = DATA_ROOT / \"Y_train_T9NrBYo.csv\"\n",
    "\n",
    "# submission size\n",
    "TARGET_H = 160\n",
    "TARGET_W = 272\n",
    "\n",
    "# model size\n",
    "MODEL_H = 224\n",
    "MODEL_W = 224\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "IGNORE_INDEX = -1\n",
    "\n",
    "BATCH_SIZE_L = 2         # labeled batch\n",
    "BATCH_SIZE_U = 2         # unlabeled batch\n",
    "LR = 5e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 10\n",
    "\n",
    "# semi-supervised hyperparams\n",
    "PSEUDO_TH = 0.85         # 伪标签置信度阈值(越高越保守)\n",
    "LAMBDA_U = 0.5           # 无标签loss权重(0.2~1.0可调)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "PRETRAINED = \"facebook/mask2former-swin-tiny-ade-semantic\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Utils\n",
    "# =========================\n",
    "def parse_well_id(name: str) -> int:\n",
    "    \"\"\"well_12_section_0_patch_0 -> 12\"\"\"\n",
    "    m = re.search(r\"well_(\\d+)_\", name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "\n",
    "def minmax_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    mn = float(x.min())\n",
    "    mx = float(x.max())\n",
    "    if mx - mn < 1e-6:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return (x - mn) / (mx - mn)\n",
    "\n",
    "\n",
    "def pad_to_160x272(img: np.ndarray, fill_value: float = 0.0) -> np.ndarray:\n",
    "    h, w = img.shape\n",
    "    assert h == TARGET_H, f\"Expected height {TARGET_H}, got {h}\"\n",
    "    if w == TARGET_W:\n",
    "        return img\n",
    "    if w < TARGET_W:\n",
    "        out = np.full((TARGET_H, TARGET_W), fill_value, dtype=img.dtype)\n",
    "        out[:, :w] = img\n",
    "        return out\n",
    "    return img[:, :TARGET_W]\n",
    "\n",
    "\n",
    "def decode_mask_from_csv_row(row_values: np.ndarray) -> np.ndarray:\n",
    "    valid = row_values[row_values != IGNORE_INDEX]\n",
    "    assert len(valid) % TARGET_H == 0, f\"Valid mask length {len(valid)} not divisible by 160\"\n",
    "    w = len(valid) // TARGET_H\n",
    "    return valid.reshape(TARGET_H, w).astype(np.int64)\n",
    "\n",
    "\n",
    "def pad_mask_to_160x272(mask: np.ndarray) -> np.ndarray:\n",
    "    h, w = mask.shape\n",
    "    assert h == TARGET_H\n",
    "    if w == TARGET_W:\n",
    "        return mask\n",
    "    out = np.full((TARGET_H, TARGET_W), IGNORE_INDEX, dtype=np.int64)\n",
    "    out[:, :w] = mask\n",
    "    return out\n",
    "\n",
    "\n",
    "def resize_image_torch(img_1hw: torch.Tensor, h: int, w: int) -> torch.Tensor:\n",
    "    \"\"\"(1,H,W)->(1,h,w) bilinear\"\"\"\n",
    "    x = img_1hw.unsqueeze(0)  # (1,1,H,W)\n",
    "    x = F.interpolate(x, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
    "    return x.squeeze(0)\n",
    "\n",
    "\n",
    "def resize_mask_torch(mask_hw: torch.Tensor, h: int, w: int) -> torch.Tensor:\n",
    "    \"\"\"(H,W)->(h,w) nearest\"\"\"\n",
    "    y = mask_hw.unsqueeze(0).unsqueeze(0).float()\n",
    "    y = F.interpolate(y, size=(h, w), mode=\"nearest\")\n",
    "    return y.squeeze(0).squeeze(0).long()\n",
    "\n",
    "\n",
    "def semantic_to_mask2former_targets(\n",
    "    semantic_mask: torch.Tensor,\n",
    "    num_classes: int,\n",
    "    ignore_index: int = -1,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    semantic_mask: (H,W) with ignore_index\n",
    "    return:\n",
    "      class_labels: (N,)\n",
    "      mask_labels:  (N,H,W) float(0/1)\n",
    "    \"\"\"\n",
    "    valid = semantic_mask != ignore_index\n",
    "    if valid.sum() == 0:\n",
    "        class_labels = torch.tensor([0], dtype=torch.long)\n",
    "        mask_labels = torch.zeros((1, semantic_mask.shape[0], semantic_mask.shape[1]), dtype=torch.float32)\n",
    "        return class_labels, mask_labels\n",
    "\n",
    "    present = torch.unique(semantic_mask[valid]).tolist()\n",
    "    present = [int(c) for c in present if 0 <= int(c) < num_classes]\n",
    "    if len(present) == 0:\n",
    "        class_labels = torch.tensor([0], dtype=torch.long)\n",
    "        mask_labels = torch.zeros((1, semantic_mask.shape[0], semantic_mask.shape[1]), dtype=torch.float32)\n",
    "        return class_labels, mask_labels\n",
    "\n",
    "    masks, classes = [], []\n",
    "    for c in present:\n",
    "        m = (semantic_mask == c) & valid\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        masks.append(m.float())\n",
    "        classes.append(c)\n",
    "\n",
    "    if len(classes) == 0:\n",
    "        class_labels = torch.tensor([0], dtype=torch.long)\n",
    "        mask_labels = torch.zeros((1, semantic_mask.shape[0], semantic_mask.shape[1]), dtype=torch.float32)\n",
    "        return class_labels, mask_labels\n",
    "\n",
    "    class_labels = torch.tensor(classes, dtype=torch.long)\n",
    "    mask_labels = torch.stack(masks, dim=0).float()\n",
    "    return class_labels, mask_labels\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) 简单增强（无标签用）\n",
    "# =========================\n",
    "def aug_weak(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"弱增强：随机左右翻转 + 轻噪声\"\"\"\n",
    "    # x: (1,224,224)\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        x = torch.flip(x, dims=[2])\n",
    "    noise = 0.02 * torch.randn_like(x)\n",
    "    return torch.clamp(x + noise, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def aug_strong(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"强增强：随机翻转 + 更强噪声 + 亮度对比度扰动\"\"\"\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        x = torch.flip(x, dims=[2])\n",
    "    # brightness/contrast\n",
    "    contrast = 0.8 + 0.4 * torch.rand(1).item()   # [0.8,1.2]\n",
    "    brightness = -0.1 + 0.2 * torch.rand(1).item() # [-0.1,0.1]\n",
    "    x = x * contrast + brightness\n",
    "    # noise\n",
    "    noise = 0.05 * torch.randn_like(x)\n",
    "    x = x + noise\n",
    "    return torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Dataset\n",
    "# =========================\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, y_csv_path: Path):\n",
    "        self.image_paths = sorted(images_dir.glob(\"*.npy\"))\n",
    "        self.names = [p.stem for p in self.image_paths]\n",
    "        self.y_df = pd.read_csv(y_csv_path, index_col=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        name = self.names[idx]\n",
    "        img = np.load(self.image_paths[idx])\n",
    "        raw_w = int(img.shape[1])\n",
    "\n",
    "        img = minmax_normalize(img)\n",
    "        img = pad_to_160x272(img, fill_value=0.0)\n",
    "        img_t = torch.from_numpy(img).unsqueeze(0).float()         # (1,160,272)\n",
    "        img_t = resize_image_torch(img_t, MODEL_H, MODEL_W)        # (1,224,224)\n",
    "\n",
    "        row = self.y_df.loc[name].values.astype(np.int64)\n",
    "        mask = decode_mask_from_csv_row(row)                       # (160,w)\n",
    "        mask = pad_mask_to_160x272(mask)                           # (160,272)\n",
    "        mask_t = torch.from_numpy(mask).long()                     # (160,272)\n",
    "        mask_t = resize_mask_torch(mask_t, MODEL_H, MODEL_W)        # (224,224)\n",
    "\n",
    "        return {\"name\": name, \"image\": img_t, \"mask\": mask_t, \"raw_w\": raw_w}\n",
    "\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path):\n",
    "        self.image_paths = sorted(images_dir.glob(\"*.npy\"))\n",
    "        self.names = [p.stem for p in self.image_paths]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        name = self.names[idx]\n",
    "        img = np.load(self.image_paths[idx])\n",
    "        raw_w = int(img.shape[1])\n",
    "\n",
    "        img = minmax_normalize(img)\n",
    "        img = pad_to_160x272(img, fill_value=0.0)\n",
    "        img_t = torch.from_numpy(img).unsqueeze(0).float()         # (1,160,272)\n",
    "        img_t = resize_image_torch(img_t, MODEL_H, MODEL_W)         # (1,224,224)\n",
    "\n",
    "        # 返回 base 图像（增强在 collate 做）\n",
    "        return {\"name\": name, \"image\": img_t, \"raw_w\": raw_w}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Collate\n",
    "# =========================\n",
    "def collate_labeled(batch: List[Dict]) -> Dict:\n",
    "    names = [b[\"name\"] for b in batch]\n",
    "    raw_ws = torch.tensor([b[\"raw_w\"] for b in batch], dtype=torch.long)\n",
    "\n",
    "    imgs_1 = torch.stack([b[\"image\"] for b in batch], dim=0)       # (B,1,224,224)\n",
    "    pixel_values = imgs_1.repeat(1, 3, 1, 1)                       # (B,3,224,224)\n",
    "    pixel_mask = torch.ones((pixel_values.shape[0], MODEL_H, MODEL_W), dtype=torch.long)\n",
    "\n",
    "    class_labels_list, mask_labels_list = [], []\n",
    "    for b in batch:\n",
    "        y = b[\"mask\"]  # (224,224)\n",
    "        cls, msk = semantic_to_mask2former_targets(y, NUM_CLASSES, IGNORE_INDEX)\n",
    "        class_labels_list.append(cls)\n",
    "        mask_labels_list.append(msk)\n",
    "\n",
    "    return {\n",
    "        \"names\": names,\n",
    "        \"raw_ws\": raw_ws,\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"pixel_mask\": pixel_mask,\n",
    "        \"class_labels\": class_labels_list,\n",
    "        \"mask_labels\": mask_labels_list,\n",
    "    }\n",
    "\n",
    "\n",
    "def collate_unlabeled(batch: List[Dict]) -> Dict:\n",
    "    names = [b[\"name\"] for b in batch]\n",
    "\n",
    "    imgs = [b[\"image\"] for b in batch]  # list of (1,224,224)\n",
    "\n",
    "    # weak / strong augmentation\n",
    "    imgs_w = torch.stack([aug_weak(x.clone()) for x in imgs], dim=0)    # (B,1,224,224)\n",
    "    imgs_s = torch.stack([aug_strong(x.clone()) for x in imgs], dim=0)  # (B,1,224,224)\n",
    "\n",
    "    pixel_values_w = imgs_w.repeat(1, 3, 1, 1)  # (B,3,224,224)\n",
    "    pixel_values_s = imgs_s.repeat(1, 3, 1, 1)\n",
    "\n",
    "    pixel_mask = torch.ones((pixel_values_w.shape[0], MODEL_H, MODEL_W), dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"names\": names,\n",
    "        \"pixel_values_w\": pixel_values_w,\n",
    "        \"pixel_values_s\": pixel_values_s,\n",
    "        \"pixel_mask\": pixel_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Model builder\n",
    "# =========================\n",
    "def build_model(num_classes: int):\n",
    "    id2label = {0: \"class0\", 1: \"class1\", 2: \"class2\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    processor = AutoImageProcessor.from_pretrained(PRETRAINED)\n",
    "    model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "        PRETRAINED,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        num_labels=num_classes,\n",
    "    )\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) Pseudo label from Mask2Former outputs\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def pseudo_from_outputs(outputs, num_classes: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    从 Mask2Former 输出构造像素级类别概率：\n",
    "    class_probs (softmax) * mask_probs (sigmoid) -> per-pixel scores\n",
    "\n",
    "    outputs.class_queries_logits: (B, Q, C+1)  (最后一个通常是 no-object)\n",
    "    outputs.masks_queries_logits: (B, Q, H, W)\n",
    "\n",
    "    return:\n",
    "      pseudo: (B,H,W) long  (0..C-1)\n",
    "      conf:   (B,H,W) float (max score)\n",
    "    \"\"\"\n",
    "    class_logits = outputs.class_queries_logits  # (B,Q,C+1)\n",
    "    mask_logits = outputs.masks_queries_logits   # (B,Q,H,W)\n",
    "\n",
    "    class_prob = class_logits.softmax(dim=-1)[..., :num_classes]   # (B,Q,C)\n",
    "    mask_prob = mask_logits.sigmoid()                              # (B,Q,H,W)\n",
    "\n",
    "    # (B,C,H,W)  einsum: sum_q class_prob[b,q,c] * mask_prob[b,q,h,w]\n",
    "    score = torch.einsum(\"bqc,bqhw->bchw\", class_prob, mask_prob)\n",
    "    conf, pseudo = torch.max(score, dim=1)  # (B,H,W)\n",
    "    return pseudo.long(), conf.float()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7) Train / Validate (Semi-Supervised)\n",
    "# =========================\n",
    "def train_one_epoch_semi(model, labeled_loader, unlabeled_loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    total_l, total_u = 0.0, 0.0\n",
    "    n_l, n_u = 0, 0\n",
    "\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "\n",
    "    for batch_l in labeled_loader:\n",
    "        # ---- labeled step ----\n",
    "        pixel_values = batch_l[\"pixel_values\"].to(DEVICE)\n",
    "        pixel_mask = batch_l[\"pixel_mask\"].to(DEVICE)\n",
    "        class_labels = [x.to(DEVICE) for x in batch_l[\"class_labels\"]]\n",
    "        mask_labels = [x.to(DEVICE) for x in batch_l[\"mask_labels\"]]\n",
    "\n",
    "        out_l = model(\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            class_labels=class_labels,\n",
    "            mask_labels=mask_labels,\n",
    "        )\n",
    "        loss_l = out_l.loss\n",
    "\n",
    "        # ---- unlabeled step (pseudo-label) ----\n",
    "        try:\n",
    "            batch_u = next(unlabeled_iter)\n",
    "        except StopIteration:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            batch_u = next(unlabeled_iter)\n",
    "\n",
    "        pv_w = batch_u[\"pixel_values_w\"].to(DEVICE)  # weak\n",
    "        pv_s = batch_u[\"pixel_values_s\"].to(DEVICE)  # strong\n",
    "        pm_u = batch_u[\"pixel_mask\"].to(DEVICE)\n",
    "\n",
    "        # teacher prediction on weak\n",
    "        model.eval()\n",
    "        out_u_teacher = model(pixel_values=pv_w, pixel_mask=pm_u)\n",
    "        pseudo, conf = pseudo_from_outputs(out_u_teacher, NUM_CLASSES)  # (B,224,224)\n",
    "\n",
    "        # 置信度过滤：低于阈值的像素设为 IGNORE\n",
    "        pseudo = pseudo.clone()\n",
    "        pseudo[conf < PSEUDO_TH] = IGNORE_INDEX\n",
    "\n",
    "        # 将 pseudo semantic mask -> mask2former targets(list)\n",
    "        class_labels_u, mask_labels_u = [], []\n",
    "        for i in range(pseudo.shape[0]):\n",
    "            cls_i, msk_i = semantic_to_mask2former_targets(pseudo[i], NUM_CLASSES, IGNORE_INDEX)\n",
    "            class_labels_u.append(cls_i.to(DEVICE))\n",
    "            mask_labels_u.append(msk_i.to(DEVICE))\n",
    "\n",
    "        model.train()\n",
    "        out_u_student = model(\n",
    "            pixel_values=pv_s,\n",
    "            pixel_mask=pm_u,\n",
    "            class_labels=class_labels_u,\n",
    "            mask_labels=mask_labels_u,\n",
    "        )\n",
    "        loss_u = out_u_student.loss\n",
    "\n",
    "        # ---- total loss ----\n",
    "        loss = loss_l + LAMBDA_U * loss_u\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_l += float(loss_l.item()) * pixel_values.size(0)\n",
    "        total_u += float(loss_u.item()) * pv_s.size(0)\n",
    "        n_l += pixel_values.size(0)\n",
    "        n_u += pv_s.size(0)\n",
    "\n",
    "    return total_l / max(1, n_l), total_u / max(1, n_u)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\n",
    "        class_labels = [x.to(DEVICE) for x in batch[\"class_labels\"]]\n",
    "        mask_labels = [x.to(DEVICE) for x in batch[\"mask_labels\"]]\n",
    "\n",
    "        out = model(\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            class_labels=class_labels,\n",
    "            mask_labels=mask_labels,\n",
    "        )\n",
    "        total += float(out.loss.item()) * pixel_values.size(0)\n",
    "        n += pixel_values.size(0)\n",
    "\n",
    "    return total / max(1, n)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8) Inference & submission\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def predict_and_make_submission(model, processor, test_images_dir: Path, out_csv_path: Path):\n",
    "    model.eval()\n",
    "\n",
    "    # 这里复用 UnlabeledDataset 结构（只有图像，无标签）\n",
    "    test_ds = UnlabeledDataset(test_images_dir)\n",
    "\n",
    "    def collate_test(batch: List[Dict]) -> Dict:\n",
    "        names = [b[\"name\"] for b in batch]\n",
    "        raw_ws = torch.tensor([b[\"raw_w\"] for b in batch], dtype=torch.long)\n",
    "        imgs_1 = torch.stack([b[\"image\"] for b in batch], dim=0)    # (B,1,224,224)\n",
    "        pixel_values = imgs_1.repeat(1, 3, 1, 1)\n",
    "        pixel_mask = torch.ones((pixel_values.shape[0], MODEL_H, MODEL_W), dtype=torch.long)\n",
    "        return {\"names\": names, \"raw_ws\": raw_ws, \"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask}\n",
    "\n",
    "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_test)\n",
    "\n",
    "    preds_dict = {}\n",
    "\n",
    "    for batch in test_loader:\n",
    "        name = batch[\"names\"][0]\n",
    "        raw_w = int(batch[\"raw_ws\"][0].item())\n",
    "\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "        # 使用 processor 的语义后处理（稳定）\n",
    "        seg_list = processor.post_process_semantic_segmentation(outputs, target_sizes=[(MODEL_H, MODEL_W)])\n",
    "        seg_224 = seg_list[0].to(torch.int64)  # (224,224)\n",
    "\n",
    "        seg_224 = seg_224.unsqueeze(0).unsqueeze(0).float()\n",
    "        seg_160_272 = F.interpolate(seg_224, size=(TARGET_H, TARGET_W), mode=\"nearest\").squeeze(0).squeeze(0)\n",
    "        seg_160_272 = seg_160_272.cpu().numpy().astype(np.int64)\n",
    "\n",
    "        pred = seg_160_272[:, :raw_w]\n",
    "\n",
    "        padded = np.full((TARGET_H * TARGET_W,), IGNORE_INDEX, dtype=np.int64)\n",
    "        padded[: TARGET_H * raw_w] = pred.flatten()\n",
    "        preds_dict[name] = padded\n",
    "\n",
    "    sub = pd.DataFrame(preds_dict, dtype=\"int64\").T\n",
    "    sub.to_csv(out_csv_path)\n",
    "    print(f\"[OK] submission saved to: {out_csv_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 9) Main\n",
    "# =========================\n",
    "def main():\n",
    "    print(f\"DEVICE: {DEVICE}\")\n",
    "    print(f\"Labeled train dir: {TRAIN_IMAGES_DIR}\")\n",
    "    print(f\"Unlabeled dir:     {UNLABELED_DIR}\")\n",
    "    print(f\"Test dir:          {TEST_IMAGES_DIR}\")\n",
    "    print(f\"Pretrained:        {PRETRAINED}\")\n",
    "    print(f\"Pseudo TH={PSEUDO_TH}, lambda_u={LAMBDA_U}\")\n",
    "\n",
    "    # ---- labeled dataset (well1-6) ----\n",
    "    ds_all = LabeledDataset(TRAIN_IMAGES_DIR, Y_TRAIN_CSV)\n",
    "\n",
    "    # split by well (val=6)\n",
    "    train_idx, val_idx = [], []\n",
    "    for i, name in enumerate(ds_all.names):\n",
    "        w = parse_well_id(name)\n",
    "        if w == 6:\n",
    "            val_idx.append(i)\n",
    "        else:\n",
    "            train_idx.append(i)\n",
    "\n",
    "    train_ds = Subset(ds_all, train_idx)  # well1-5\n",
    "    val_ds   = Subset(ds_all, val_idx)    # well6\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE_L,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_labeled,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE_L,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_labeled,\n",
    "    )\n",
    "\n",
    "    # ---- unlabeled dataset (well12-14) ----\n",
    "    unlab_ds = UnlabeledDataset(UNLABELED_DIR)\n",
    "    unlab_loader = DataLoader(\n",
    "        unlab_ds,\n",
    "        batch_size=BATCH_SIZE_U,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_unlabeled,\n",
    "    )\n",
    "\n",
    "    print(f\"Labeled train: {len(train_ds)} | Val: {len(val_ds)} | Unlabeled: {len(unlab_ds)}\")\n",
    "\n",
    "    model, processor = build_model(NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val = 1e9\n",
    "    best_path = DATA_ROOT / \"best_mask2former_semi.pth\"\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_l, tr_u = train_one_epoch_semi(model, train_loader, unlab_loader, optimizer)\n",
    "        va = valid_one_epoch(model, val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}/{EPOCHS} | train_l={tr_l:.4f} | train_u={tr_u:.4f} | val={va:.4f}\")\n",
    "\n",
    "        if va < best_val:\n",
    "            best_val = va\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  -> Best saved: {best_path}\")\n",
    "\n",
    "    # ---- inference ----\n",
    "    out_csv = DATA_ROOT / \"submission.csv\"\n",
    "    model.load_state_dict(torch.load(best_path, map_location=DEVICE, weights_only=True))\n",
    "    predict_and_make_submission(model, processor, TEST_IMAGES_DIR, out_csv)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
