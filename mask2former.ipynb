{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f77dc9",
   "metadata": {},
   "source": [
    "## Mask2former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5988541",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\deep-torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Train dir: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\X_train_uDRk9z9\\images\n",
      "Test dir:  C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\X_test_xNbnvIa\\images\n",
      "Model input size: 224x224\n",
      "Pretrained: facebook/mask2former-swin-tiny-ade-semantic\n",
      "Train samples: 2790 | Val samples: 1620 | val_wells={6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "c:\\Users\\lenovo\\anaconda3\\envs\\deep-torch\\lib\\site-packages\\transformers\\image_processing_base.py:417: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size', 'reduce_labels'\n",
      "  image_processor = cls(**image_processor_dict)\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-tiny-ade-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([4, 256]) in the model instantiated\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | train_loss=17.3520 | val_loss=15.0105\n",
      "  -> Best model saved: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_mask2former.pth\n",
      "Epoch 02/20 | train_loss=13.7775 | val_loss=15.2349\n",
      "Epoch 03/20 | train_loss=12.6401 | val_loss=14.5714\n",
      "  -> Best model saved: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_mask2former.pth\n",
      "Epoch 04/20 | train_loss=12.0788 | val_loss=14.2691\n",
      "  -> Best model saved: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_mask2former.pth\n",
      "Epoch 05/20 | train_loss=11.6676 | val_loss=15.0381\n",
      "Epoch 06/20 | train_loss=11.4770 | val_loss=14.3574\n",
      "Epoch 07/20 | train_loss=10.9957 | val_loss=14.1694\n",
      "  -> Best model saved: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_mask2former.pth\n",
      "Epoch 08/20 | train_loss=10.6843 | val_loss=14.9301\n",
      "Epoch 09/20 | train_loss=10.3577 | val_loss=15.2518\n",
      "Epoch 10/20 | train_loss=10.2286 | val_loss=15.0672\n",
      "Epoch 11/20 | train_loss=9.6430 | val_loss=15.5822\n",
      "Epoch 12/20 | train_loss=9.4476 | val_loss=14.6725\n",
      "Epoch 13/20 | train_loss=9.0929 | val_loss=15.5858\n",
      "Epoch 14/20 | train_loss=9.0179 | val_loss=15.7629\n",
      "Epoch 15/20 | train_loss=8.4409 | val_loss=16.1311\n",
      "Epoch 16/20 | train_loss=8.3673 | val_loss=16.3036\n",
      "Epoch 17/20 | train_loss=8.0378 | val_loss=17.1427\n",
      "Epoch 18/20 | train_loss=7.8582 | val_loss=17.3488\n",
      "Epoch 19/20 | train_loss=7.5451 | val_loss=17.6415\n",
      "Epoch 20/20 | train_loss=7.1668 | val_loss=16.5966\n",
      "[OK] submission saved to: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mask2Former (HuggingFace Transformers) - Full runnable semantic segmentation code\n",
    "\n",
    "What this script does:\n",
    "- Reads .npy ultrasound patches\n",
    "- Trains on well1-5, validates on well6 (from X_train_uDRk9z9/images)\n",
    "- Predicts on X_test_xNbnvIa/images (well7-11)\n",
    "- Writes submission.csv with the SAME format as before:\n",
    "  - each row = one patch name\n",
    "  - flattened mask\n",
    "  - padded to 160*272 with -1\n",
    "\n",
    "Why we resize:\n",
    "- Mask2Former backbones are usually trained on larger resolutions.\n",
    "- To keep it simple and fit RTX 4060 (8GB), we resize inputs to 224x224 during training/inference,\n",
    "  then upsample predictions back to (160,272) for submission.\n",
    "\n",
    "Install (in your CUDA environment):\n",
    "    pip install transformers accelerate\n",
    "\n",
    "Notes:\n",
    "- If your machine cannot download pretrained weights (no internet), set PRETRAINED=None and it will start from scratch.\n",
    "- Mask2Former expects instance-style labels: a set of binary masks + class ids per image.\n",
    "  We convert your (H,W) semantic mask into that format automatically.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    Mask2FormerForUniversalSegmentation,\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0. Paths & Hyperparameters\n",
    "# =========================\n",
    "DATA_ROOT = Path(r\"C:\\Users\\lenovo\\Desktop\\deep_datachallenge\")  # change to your path\n",
    "\n",
    "TRAIN_IMAGES_DIR = DATA_ROOT / \"X_train_uDRk9z9\" / \"images\"\n",
    "TEST_IMAGES_DIR = DATA_ROOT / \"X_test_xNbnvIa\" / \"images\"\n",
    "Y_TRAIN_CSV = DATA_ROOT / \"Y_train_T9NrBYo.csv\"\n",
    "\n",
    "# Original submission size\n",
    "TARGET_H = 160\n",
    "TARGET_W = 272\n",
    "\n",
    "# Model input size (keep small for 4060)\n",
    "MODEL_H = 224\n",
    "MODEL_W = 224\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "IGNORE_INDEX = -1\n",
    "\n",
    "BATCH_SIZE = 2          # Mask2Former is heavy; start with 1~2 on 4060 8GB\n",
    "LR = 5e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 20\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# A strong semantic pretrained checkpoint (Swin-T backbone)\n",
    "# If you have no internet, set PRETRAINED = None\n",
    "PRETRAINED = \"facebook/mask2former-swin-tiny-ade-semantic\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. Utils\n",
    "# =========================\n",
    "def parse_well_id(name: str) -> int:\n",
    "    \"\"\"Extract well id from: well_1_section_0_patch_0 -> 1\"\"\"\n",
    "    m = re.search(r\"well_(\\d+)_\", name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "\n",
    "def minmax_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Min-max normalize; replace NaN/inf with 0.\"\"\"\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    x_min = float(x.min())\n",
    "    x_max = float(x.max())\n",
    "    if x_max - x_min < 1e-6:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return (x - x_min) / (x_max - x_min)\n",
    "\n",
    "\n",
    "def pad_to_160x272(img: np.ndarray, fill_value: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"Pad (160,160) or (160,272) to (160,272).\"\"\"\n",
    "    h, w = img.shape\n",
    "    assert h == TARGET_H, f\"Expected height {TARGET_H}, got {h}\"\n",
    "    if w == TARGET_W:\n",
    "        return img\n",
    "    if w < TARGET_W:\n",
    "        out = np.full((TARGET_H, TARGET_W), fill_value, dtype=img.dtype)\n",
    "        out[:, :w] = img\n",
    "        return out\n",
    "    return img[:, :TARGET_W]\n",
    "\n",
    "\n",
    "def decode_mask_from_csv_row(row_values: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Decode one CSV row -> (160,w) semantic mask\n",
    "    - row_values: flattened mask with -1 padding\n",
    "    \"\"\"\n",
    "    valid = row_values[row_values != IGNORE_INDEX]\n",
    "    assert len(valid) % TARGET_H == 0, f\"Valid mask length {len(valid)} not divisible by 160\"\n",
    "    w = len(valid) // TARGET_H\n",
    "    return valid.reshape(TARGET_H, w).astype(np.int64)\n",
    "\n",
    "\n",
    "def pad_mask_to_160x272(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Pad (160,w) -> (160,272) using -1 for padding.\"\"\"\n",
    "    h, w = mask.shape\n",
    "    assert h == TARGET_H\n",
    "    if w == TARGET_W:\n",
    "        return mask\n",
    "    out = np.full((TARGET_H, TARGET_W), IGNORE_INDEX, dtype=np.int64)\n",
    "    out[:, :w] = mask\n",
    "    return out\n",
    "\n",
    "\n",
    "def resize_image_torch(img_1hw: torch.Tensor, h: int, w: int) -> torch.Tensor:\n",
    "    \"\"\"Resize image tensor (1,H,W) -> (1,h,w) (bilinear).\"\"\"\n",
    "    x = img_1hw.unsqueeze(0)  # (1,1,H,W)\n",
    "    x = F.interpolate(x, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
    "    return x.squeeze(0)       # (1,h,w)\n",
    "\n",
    "\n",
    "def resize_mask_torch(mask_hw: torch.Tensor, h: int, w: int) -> torch.Tensor:\n",
    "    \"\"\"Resize mask tensor (H,W) -> (h,w) (nearest).\"\"\"\n",
    "    y = mask_hw.unsqueeze(0).unsqueeze(0).float()  # (1,1,H,W)\n",
    "    y = F.interpolate(y, size=(h, w), mode=\"nearest\")\n",
    "    return y.squeeze(0).squeeze(0).long()\n",
    "\n",
    "\n",
    "def semantic_to_mask2former_targets(\n",
    "    semantic_mask: torch.Tensor,\n",
    "    num_classes: int,\n",
    "    ignore_index: int = -1,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Convert a semantic mask (H,W) into Mask2Former targets:\n",
    "    - class_labels: (N,) long\n",
    "    - mask_labels:  (N,H,W) float (0/1)\n",
    "\n",
    "    We create one binary mask per class present in the image (excluding ignore_index).\n",
    "    \"\"\"\n",
    "    # semantic_mask: (H,W)\n",
    "    valid = semantic_mask != ignore_index\n",
    "    if valid.sum() == 0:\n",
    "        # If everything is ignore, create a dummy empty target (rare).\n",
    "        # Use background class 0 with an all-zero mask.\n",
    "        class_labels = torch.tensor([0], dtype=torch.long)\n",
    "        mask_labels = torch.zeros((1, semantic_mask.shape[0], semantic_mask.shape[1]), dtype=torch.float32)\n",
    "        return class_labels, mask_labels\n",
    "\n",
    "    present_classes = torch.unique(semantic_mask[valid]).tolist()\n",
    "    present_classes = [int(c) for c in present_classes if 0 <= int(c) < num_classes]\n",
    "\n",
    "    if len(present_classes) == 0:\n",
    "        class_labels = torch.tensor([0], dtype=torch.long)\n",
    "        mask_labels = torch.zeros((1, semantic_mask.shape[0], semantic_mask.shape[1]), dtype=torch.float32)\n",
    "        return class_labels, mask_labels\n",
    "\n",
    "    masks = []\n",
    "    classes = []\n",
    "    for c in present_classes:\n",
    "        m = (semantic_mask == c) & valid\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        masks.append(m.float())\n",
    "        classes.append(c)\n",
    "\n",
    "    if len(classes) == 0:\n",
    "        class_labels = torch.tensor([0], dtype=torch.long)\n",
    "        mask_labels = torch.zeros((1, semantic_mask.shape[0], semantic_mask.shape[1]), dtype=torch.float32)\n",
    "        return class_labels, mask_labels\n",
    "\n",
    "    class_labels = torch.tensor(classes, dtype=torch.long)\n",
    "    mask_labels = torch.stack(masks, dim=0).float()  # (N,H,W)\n",
    "    return class_labels, mask_labels\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. Dataset\n",
    "# =========================\n",
    "class WellSegDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, y_csv_path: Path = None):\n",
    "        self.images_dir = images_dir\n",
    "        self.has_label = y_csv_path is not None\n",
    "\n",
    "        self.image_paths = sorted(images_dir.glob(\"*.npy\"))\n",
    "        self.names = [p.stem for p in self.image_paths]\n",
    "\n",
    "        self.y_df = pd.read_csv(y_csv_path, index_col=0) if self.has_label else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        name = self.names[idx]\n",
    "        img_path = self.image_paths[idx]\n",
    "\n",
    "        img = np.load(img_path)                 # (160,160) or (160,272)\n",
    "        raw_w = int(img.shape[1])               # used to crop back for submission\n",
    "        img = minmax_normalize(img)\n",
    "        img = pad_to_160x272(img, fill_value=0.0)\n",
    "\n",
    "        img_t = torch.from_numpy(img).unsqueeze(0).float()      # (1,160,272)\n",
    "        img_t = resize_image_torch(img_t, MODEL_H, MODEL_W)     # (1,224,224)\n",
    "\n",
    "        if not self.has_label:\n",
    "            return {\"name\": name, \"image\": img_t, \"raw_w\": raw_w}\n",
    "\n",
    "        row = self.y_df.loc[name].values.astype(np.int64)\n",
    "        mask = decode_mask_from_csv_row(row)                    # (160,w)\n",
    "        mask = pad_mask_to_160x272(mask)                        # (160,272)\n",
    "        mask_t = torch.from_numpy(mask).long()                  # (160,272)\n",
    "        mask_t = resize_mask_torch(mask_t, MODEL_H, MODEL_W)    # (224,224)\n",
    "\n",
    "        return {\"name\": name, \"image\": img_t, \"mask\": mask_t, \"raw_w\": raw_w}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. Collate for Mask2Former\n",
    "# =========================\n",
    "def collate_mask2former(batch: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Build a batch dict for Mask2Former:\n",
    "    - pixel_values: (B,3,224,224) float\n",
    "    - pixel_mask:   (B,224,224)   bool/long (1=valid)\n",
    "    - mask_labels:  list of (Ni,224,224) float\n",
    "    - class_labels: list of (Ni,) long\n",
    "    \"\"\"\n",
    "    names = [b[\"name\"] for b in batch]\n",
    "    raw_ws = torch.tensor([b[\"raw_w\"] for b in batch], dtype=torch.long)\n",
    "\n",
    "    # image: (1,224,224) -> (3,224,224) by repeating channel\n",
    "    imgs_1 = torch.stack([b[\"image\"] for b in batch], dim=0)  # (B,1,224,224)\n",
    "    pixel_values = imgs_1.repeat(1, 3, 1, 1)                  # (B,3,224,224)\n",
    "\n",
    "    pixel_mask = torch.ones((pixel_values.shape[0], MODEL_H, MODEL_W), dtype=torch.long)\n",
    "\n",
    "    out = {\n",
    "        \"names\": names,\n",
    "        \"raw_ws\": raw_ws,\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"pixel_mask\": pixel_mask,\n",
    "    }\n",
    "\n",
    "    if \"mask\" in batch[0]:\n",
    "        class_labels_list = []\n",
    "        mask_labels_list = []\n",
    "        for b in batch:\n",
    "            y = b[\"mask\"]  # (224,224)\n",
    "            cls, msk = semantic_to_mask2former_targets(y, NUM_CLASSES, IGNORE_INDEX)\n",
    "            class_labels_list.append(cls)\n",
    "            mask_labels_list.append(msk)\n",
    "\n",
    "        out[\"class_labels\"] = class_labels_list\n",
    "        out[\"mask_labels\"] = mask_labels_list\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. Model builder\n",
    "# =========================\n",
    "def build_model_and_processor(num_classes: int):\n",
    "    id2label = {0: \"class0\", 1: \"class1\", 2: \"class2\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    if PRETRAINED is None:\n",
    "        # Train from scratch\n",
    "        processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-tiny-ade-semantic\")\n",
    "        model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "            \"facebook/mask2former-swin-tiny-ade-semantic\",\n",
    "            ignore_mismatched_sizes=True,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            num_labels=num_classes,\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "    else:\n",
    "        processor = AutoImageProcessor.from_pretrained(PRETRAINED)\n",
    "        model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "            PRETRAINED,\n",
    "            ignore_mismatched_sizes=True,   # allow changing num_labels\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            num_labels=num_classes,\n",
    "        )\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. Train / Validate\n",
    "# =========================\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)  # (B,3,224,224)\n",
    "        pixel_mask = batch[\"pixel_mask\"].to(DEVICE)      # (B,224,224)\n",
    "\n",
    "        # Mask2Former expects lists for labels (length B)\n",
    "        class_labels = [x.to(DEVICE) for x in batch[\"class_labels\"]]\n",
    "        mask_labels = [x.to(DEVICE) for x in batch[\"mask_labels\"]]\n",
    "\n",
    "        outputs = model(\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            class_labels=class_labels,\n",
    "            mask_labels=mask_labels,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item()) * pixel_values.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\n",
    "        class_labels = [x.to(DEVICE) for x in batch[\"class_labels\"]]\n",
    "        mask_labels = [x.to(DEVICE) for x in batch[\"mask_labels\"]]\n",
    "\n",
    "        outputs = model(\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            class_labels=class_labels,\n",
    "            mask_labels=mask_labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        total_loss += float(loss.item()) * pixel_values.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6. Inference & submission\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def predict_and_make_submission(model, processor, test_images_dir: Path, out_csv_path: Path):\n",
    "    \"\"\"\n",
    "    Predict all test patches and write submission.csv.\n",
    "    Steps:\n",
    "    - model predicts at 224x224\n",
    "    - we use processor.post_process_semantic_segmentation to get semantic map\n",
    "    - upsample semantic map to (160,272)\n",
    "    - crop to raw width and pad to 160*272 with -1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    test_ds = WellSegDataset(test_images_dir, y_csv_path=None)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_mask2former,\n",
    "    )\n",
    "\n",
    "    preds_dict = {}\n",
    "\n",
    "    for batch in test_loader:\n",
    "        name = batch[\"names\"][0]\n",
    "        raw_w = int(batch[\"raw_ws\"][0].item())\n",
    "\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)  # (1,3,224,224)\n",
    "        pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "        # Post-process semantic segmentation\n",
    "        target_sizes = [(MODEL_H, MODEL_W)]\n",
    "        seg_list = processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n",
    "        seg_224 = seg_list[0].to(torch.int64)  # (224,224)\n",
    "\n",
    "        # Upsample to (160,272) using nearest\n",
    "        seg_224 = seg_224.unsqueeze(0).unsqueeze(0).float()  # (1,1,224,224)\n",
    "        seg_160_272 = F.interpolate(seg_224, size=(TARGET_H, TARGET_W), mode=\"nearest\").squeeze(0).squeeze(0)\n",
    "        seg_160_272 = seg_160_272.cpu().numpy().astype(np.int64)  # (160,272)\n",
    "\n",
    "        # Crop back to original width\n",
    "        pred = seg_160_272[:, :raw_w]\n",
    "\n",
    "        if raw_w < TARGET_W:\n",
    "            padded = np.full((TARGET_H * TARGET_W,), IGNORE_INDEX, dtype=np.int64)\n",
    "            padded[: TARGET_H * raw_w] = pred.flatten()\n",
    "            preds_dict[name] = padded\n",
    "        else:\n",
    "            preds_dict[name] = pred.flatten()\n",
    "\n",
    "    sub = pd.DataFrame(preds_dict, dtype=\"int64\").T\n",
    "    sub.to_csv(out_csv_path)\n",
    "    print(f\"[OK] submission saved to: {out_csv_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7. Main\n",
    "# =========================\n",
    "def main():\n",
    "    print(f\"DEVICE: {DEVICE}\")\n",
    "    print(f\"Train dir: {TRAIN_IMAGES_DIR}\")\n",
    "    print(f\"Test dir:  {TEST_IMAGES_DIR}\")\n",
    "    print(f\"Model input size: {MODEL_H}x{MODEL_W}\")\n",
    "    print(f\"Pretrained: {PRETRAINED}\")\n",
    "\n",
    "    # Load all train data (well1-6)\n",
    "    train_ds_all = WellSegDataset(TRAIN_IMAGES_DIR, Y_TRAIN_CSV)\n",
    "\n",
    "    # Split by well: well6 as validation\n",
    "    VAL_WELLS = {6}\n",
    "    train_indices, val_indices = [], []\n",
    "    for i, name in enumerate(train_ds_all.names):\n",
    "        w = parse_well_id(name)\n",
    "        if w in VAL_WELLS:\n",
    "            val_indices.append(i)\n",
    "        else:\n",
    "            train_indices.append(i)\n",
    "\n",
    "    train_ds = Subset(train_ds_all, train_indices)  # well1-5\n",
    "    val_ds = Subset(train_ds_all, val_indices)      # well6\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_mask2former,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_mask2former,\n",
    "    )\n",
    "\n",
    "    print(f\"Train samples: {len(train_ds)} | Val samples: {len(val_ds)} | val_wells={VAL_WELLS}\")\n",
    "\n",
    "    model, processor = build_model_and_processor(NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val = 1e9\n",
    "    best_path = DATA_ROOT / \"best_mask2former.pth\"\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "        va_loss = valid_one_epoch(model, val_loader)\n",
    "        print(f\"Epoch {epoch:02d}/{EPOCHS} | train_loss={tr_loss:.4f} | val_loss={va_loss:.4f}\")\n",
    "\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  -> Best model saved: {best_path}\")\n",
    "\n",
    "    # Predict test and write submission\n",
    "    out_csv = DATA_ROOT / \"submission.csv\"\n",
    "    state_dict = torch.load(best_path, map_location=DEVICE, weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    predict_and_make_submission(model, processor, TEST_IMAGES_DIR, out_csv)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
