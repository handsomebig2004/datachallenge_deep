{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ec4b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本数: 2790 | 验证样本数: 1620 | val_wells={6}\n",
      "Epoch 01/5 | train_loss=0.1464 | val_loss=0.0906\n",
      "  -> 保存最优模型: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_resnet34_unet.pth\n",
      "Epoch 02/5 | train_loss=0.0871 | val_loss=0.0923\n",
      "Epoch 03/5 | train_loss=0.0784 | val_loss=0.0879\n",
      "  -> 保存最优模型: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_resnet34_unet.pth\n",
      "Epoch 04/5 | train_loss=0.0767 | val_loss=0.0893\n",
      "Epoch 05/5 | train_loss=0.0745 | val_loss=0.1063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Temp\\ipykernel_12820\\130010335.py:345: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] submission 已保存: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ResNet34-UNet（简化版，可直接跑）\n",
    "- 训练图像目录：X_train_uDRk9z9/images（well1-6）\n",
    "- 测试图像目录：X_test_xNbnvIa/images（well7-11）\n",
    "- 训练标签：Y_train_T9NrBYo.csv（flatten + -1 padding）\n",
    "- 验证：从训练集中按井划分（例：well6 为 val，其余为 train）\n",
    "- 输出：submission.csv（每行一个 patch，flatten，pad 到 160*272 用 -1）\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from torchvision.models import resnet34, ResNet34_Weights\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0. 超参数与路径\n",
    "# =========================\n",
    "DATA_ROOT = Path(r\"C:\\Users\\lenovo\\Desktop\\deep_datachallenge\")  # 改成你的真实路径\n",
    "\n",
    "TRAIN_IMAGES_DIR = DATA_ROOT / \"X_train_uDRk9z9\" / \"images\"\n",
    "TEST_IMAGES_DIR = DATA_ROOT / \"X_test_xNbnvIa\" / \"images\"\n",
    "Y_TRAIN_CSV = DATA_ROOT / \"Y_train_T9NrBYo.csv\"\n",
    "\n",
    "TARGET_H = 160\n",
    "TARGET_W = 272\n",
    "\n",
    "NUM_CLASSES = 3          # 你确认 CSV 里只有 0/1/2\n",
    "IGNORE_INDEX = -1        # CSV padding\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 5               # 训练慢就先设 5，跑通后再加\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. 工具函数\n",
    "# =========================\n",
    "def parse_well_id(name: str) -> int:\n",
    "    \"\"\"从 well_1_section_0_patch_0 提取 well id=1\"\"\"\n",
    "    m = re.search(r\"well_(\\d+)_\", name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "\n",
    "def minmax_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"min-max 归一化；NaN/inf 置 0\"\"\"\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    x_min = float(x.min())\n",
    "    x_max = float(x.max())\n",
    "    if x_max - x_min < 1e-6:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return (x - x_min) / (x_max - x_min)\n",
    "\n",
    "\n",
    "def pad_to_160x272(img: np.ndarray, fill_value: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"把 (160,160) 或 (160,272) pad 到 (160,272)\"\"\"\n",
    "    h, w = img.shape\n",
    "    assert h == TARGET_H, f\"期望高度 {TARGET_H}，但拿到 {h}\"\n",
    "    if w == TARGET_W:\n",
    "        return img\n",
    "    if w < TARGET_W:\n",
    "        out = np.full((TARGET_H, TARGET_W), fill_value, dtype=img.dtype)\n",
    "        out[:, :w] = img\n",
    "        return out\n",
    "    return img[:, :TARGET_W]\n",
    "\n",
    "\n",
    "def decode_mask_from_csv_row(row_values: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    从 CSV 一行恢复 mask：\n",
    "    - row_values: flatten + -1 padding\n",
    "    - 去掉 -1 后 reshape 成 (160, w)\n",
    "    \"\"\"\n",
    "    valid = row_values[row_values != IGNORE_INDEX]\n",
    "    assert len(valid) % TARGET_H == 0, f\"mask 有效长度 {len(valid)} 不能被 160 整除\"\n",
    "    w = len(valid) // TARGET_H\n",
    "    return valid.reshape(TARGET_H, w).astype(np.int64)\n",
    "\n",
    "\n",
    "def pad_mask_to_160x272(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"把 (160,w) pad 到 (160,272)，pad 用 -1（ignore）\"\"\"\n",
    "    h, w = mask.shape\n",
    "    assert h == TARGET_H\n",
    "    if w == TARGET_W:\n",
    "        return mask\n",
    "    out = np.full((TARGET_H, TARGET_W), IGNORE_INDEX, dtype=np.int64)\n",
    "    out[:, :w] = mask\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. Dataset（训练/测试共用）\n",
    "# =========================\n",
    "class WellSegDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, y_csv_path: Path = None):\n",
    "        \"\"\"\n",
    "        y_csv_path=None 表示无标签（测试）\n",
    "        \"\"\"\n",
    "        self.images_dir = images_dir\n",
    "        self.has_label = y_csv_path is not None\n",
    "\n",
    "        self.image_paths = sorted(images_dir.glob(\"*.npy\"))\n",
    "        self.names = [p.stem for p in self.image_paths]\n",
    "\n",
    "        if self.has_label:\n",
    "            # CSV index 通常就是 patch 名（不含 .npy）\n",
    "            self.y_df = pd.read_csv(y_csv_path, index_col=0)\n",
    "        else:\n",
    "            self.y_df = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        name = self.names[idx]\n",
    "        img_path = self.image_paths[idx]\n",
    "\n",
    "        img = np.load(img_path)        # (160,160) or (160,272)\n",
    "        raw_w = img.shape[1]           # 记录原始宽度（推理时裁回去）\n",
    "        img = minmax_normalize(img)\n",
    "        img = pad_to_160x272(img, fill_value=0.0)\n",
    "        img_t = torch.from_numpy(img).unsqueeze(0).float()  # (1,160,272)\n",
    "\n",
    "        if not self.has_label:\n",
    "            return {\"name\": name, \"image\": img_t, \"raw_w\": raw_w}\n",
    "\n",
    "        row = self.y_df.loc[name].values.astype(np.int64)\n",
    "        mask = decode_mask_from_csv_row(row)     # (160,w)\n",
    "        mask = pad_mask_to_160x272(mask)         # (160,272)\n",
    "        mask_t = torch.from_numpy(mask).long()\n",
    "\n",
    "        return {\"name\": name, \"image\": img_t, \"mask\": mask_t, \"raw_w\": raw_w}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. ResNet34-UNet（简化实现）\n",
    "# =========================\n",
    "class ConvRelu(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_ch, skip_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv1 = ConvRelu(in_ch + skip_ch, out_ch)\n",
    "        self.conv2 = ConvRelu(out_ch, out_ch)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = F.interpolate(x, size=skip.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet34UNet(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        backbone = resnet34(weights=ResNet34_Weights.IMAGENET1K_V1)\n",
    "\n",
    "        # 输入单通道：第一层卷积改成 1 通道（用原权重均值初始化）\n",
    "        old_conv1 = backbone.conv1\n",
    "        new_conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        with torch.no_grad():\n",
    "            new_conv1.weight[:] = old_conv1.weight.mean(dim=1, keepdim=True)\n",
    "        backbone.conv1 = new_conv1\n",
    "\n",
    "        self.enc0 = nn.Sequential(backbone.conv1, backbone.bn1, backbone.relu)  # /2\n",
    "        self.pool0 = backbone.maxpool                                           # /4\n",
    "        self.enc1 = backbone.layer1                                             # /4\n",
    "        self.enc2 = backbone.layer2                                             # /8\n",
    "        self.enc3 = backbone.layer3                                             # /16\n",
    "        self.enc4 = backbone.layer4                                             # /32\n",
    "\n",
    "        self.center = nn.Sequential(ConvRelu(512, 512), ConvRelu(512, 512))\n",
    "        self.up4 = UpBlock(512, 256, 256)\n",
    "        self.up3 = UpBlock(256, 128, 128)\n",
    "        self.up2 = UpBlock(128, 64, 64)\n",
    "        self.up1 = UpBlock(64, 64, 64)\n",
    "\n",
    "        self.head = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e0 = self.enc0(x)\n",
    "        e1 = self.enc1(self.pool0(e0))\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "\n",
    "        c = self.center(e4)\n",
    "        d4 = self.up4(c, e3)\n",
    "        d3 = self.up3(d4, e2)\n",
    "        d2 = self.up2(d3, e1)\n",
    "        d1 = self.up1(d2, e0)\n",
    "\n",
    "        out = self.head(d1)\n",
    "        out = F.interpolate(out, size=(TARGET_H, TARGET_W), mode=\"bilinear\", align_corners=False)\n",
    "        return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. 训练与验证（最简单）\n",
    "# =========================\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        x = batch[\"image\"].to(DEVICE)\n",
    "        y = batch[\"mask\"].to(DEVICE)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y, ignore_index=IGNORE_INDEX)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        x = batch[\"image\"].to(DEVICE)\n",
    "        y = batch[\"mask\"].to(DEVICE)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y, ignore_index=IGNORE_INDEX)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. 推理并生成提交 CSV\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def predict_and_make_submission(model, test_images_dir: Path, out_csv_path: Path):\n",
    "    \"\"\"\n",
    "    对 test_images_dir 全部 npy 预测并生成提交 CSV\n",
    "    - 每行：一个 patch\n",
    "    - 长度：160*272\n",
    "    - 如果原始宽度 < 272，剩余用 -1 padding\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    test_ds = WellSegDataset(test_images_dir, y_csv_path=None)\n",
    "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    preds_dict = {}\n",
    "\n",
    "    for batch in test_loader:\n",
    "        name = batch[\"name\"][0]\n",
    "        raw_w = int(batch[\"raw_w\"][0])\n",
    "        x = batch[\"image\"].to(DEVICE)\n",
    "\n",
    "        logits = model(x)\n",
    "        pred_full = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy().astype(np.int64)  # (160,272)\n",
    "\n",
    "        pred = pred_full[:, :raw_w]  # 裁回原始宽度\n",
    "        if raw_w < TARGET_W:\n",
    "            padded = np.full((TARGET_H * TARGET_W,), IGNORE_INDEX, dtype=np.int64)\n",
    "            padded[: TARGET_H * raw_w] = pred.flatten()\n",
    "            preds_dict[name] = padded\n",
    "        else:\n",
    "            preds_dict[name] = pred.flatten()\n",
    "\n",
    "    sub = pd.DataFrame(preds_dict, dtype=\"int64\").T\n",
    "    sub.to_csv(out_csv_path)\n",
    "    print(f\"[OK] submission 已保存: {out_csv_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6. 主函数：训练(井1-5) + 验证(井6) + 预测test(井7-11目录)\n",
    "# =========================\n",
    "def main():\n",
    "    # (A) 构建训练集（well1-6）\n",
    "    train_ds_all = WellSegDataset(TRAIN_IMAGES_DIR, Y_TRAIN_CSV)\n",
    "\n",
    "    # (B) 按井划分 train/val：well6 做验证\n",
    "    VAL_WELLS = {6}\n",
    "    train_indices, val_indices = [], []\n",
    "    for i, name in enumerate(train_ds_all.names):\n",
    "        w = parse_well_id(name)\n",
    "        if w in VAL_WELLS:\n",
    "            val_indices.append(i)\n",
    "        else:\n",
    "            train_indices.append(i)\n",
    "\n",
    "    train_ds = Subset(train_ds_all, train_indices)  # well1-5\n",
    "    val_ds = Subset(train_ds_all, val_indices)      # well6\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"训练样本数: {len(train_ds)} | 验证样本数: {len(val_ds)} | val_wells={VAL_WELLS}\")\n",
    "\n",
    "    # (C) 模型与优化器\n",
    "    model = ResNet34UNet(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    # (D) 训练\n",
    "    best_val = 1e9\n",
    "    best_path = DATA_ROOT / \"best_resnet34_unet.pth\"\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "        va_loss = valid_one_epoch(model, val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}/{EPOCHS} | train_loss={tr_loss:.4f} | val_loss={va_loss:.4f}\")\n",
    "\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  -> 保存最优模型: {best_path}\")\n",
    "\n",
    "    # (E) 生成提交（测试目录 well7-11）\n",
    "    out_csv = DATA_ROOT / \"submission.csv\"\n",
    "    model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "    predict_and_make_submission(model, TEST_IMAGES_DIR, out_csv)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
