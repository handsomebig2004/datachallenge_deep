{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c61fc682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "SUP_BEST exists: True exp_outputs\\Exp02_SegFormer_Supervised\\best_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell 0 — Exp03 (Method 3) SegFormer + Semi-Supervised (EMA Teacher)\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# ====== Your provided paths ======\n",
    "X_TEST_DIR  = Path(r\"C:\\Users\\asus\\Desktop\\ECN\\DEEP\\DataChallenge\\data\\X_test_xNbnvIa\")\n",
    "X_TRAIN_DIR = Path(r\"C:\\Users\\asus\\Desktop\\ECN\\DEEP\\DataChallenge\\data\\X_train_uDRk9z9\")\n",
    "X_UNLAB_DIR = Path(r\"C:\\Users\\asus\\Desktop\\ECN\\DEEP\\DataChallenge\\data\\X_unlabeled_mtkxUlo\")\n",
    "Y_TRAIN_CSV = Path(r\"C:\\Users\\asus\\Desktop\\ECN\\DEEP\\DataChallenge\\data\\Y_train_T9NrBYo.csv\")\n",
    "SAMPLE_SUB  = Path(r\"C:\\Users\\asus\\Desktop\\ECN\\DEEP\\DataChallenge\\data\\submission_csv_file_random_example_3qPSCtv.csv\")\n",
    "\n",
    "# ====== Exp03 output ======\n",
    "OUT_DIR = Path(r\"exp_outputs\\Exp03_SegFormer_SemiSupervised\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ====== Segmentation constants ======\n",
    "NUM_CLASSES  = 3\n",
    "IGNORE_INDEX = 255\n",
    "H            = 160\n",
    "W_PAD        = 288   # pad to 288; later crop back to original width (160/272)\n",
    "\n",
    "# ====== Load method2 supervised best to initialize (CHANGE if your path differs) ======\n",
    "SUP_BEST = Path(r\"exp_outputs\\Exp02_SegFormer_Supervised\\best_state_dict.pt\")\n",
    "print(\"SUP_BEST exists:\", SUP_BEST.exists(), SUP_BEST)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "586e42d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 — Utilities: name parsing, file listing (de-dup), X loading, padding, Y loading/padding\n",
    "NAME_RE = re.compile(r\"well_(\\d+)_section_(\\d+)_patch_(\\d+)$\")\n",
    "\n",
    "def parse_name(stem: str):\n",
    "    m = NAME_RE.match(stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Bad patch name: {stem}\")\n",
    "    return int(m.group(1)), int(m.group(2)), int(m.group(3))\n",
    "\n",
    "def list_npy_files(dir_path: Path):\n",
    "    # de-duplicate robustly (prevents Windows *.npy/*.NPY double counting)\n",
    "    files = list(dir_path.rglob(\"*.npy\")) + list(dir_path.rglob(\"*.NPY\"))\n",
    "    uniq = sorted({Path(p).resolve() for p in files})\n",
    "    return [Path(p) for p in uniq]\n",
    "\n",
    "def load_x(path: Path) -> np.ndarray:\n",
    "    x = np.load(path)\n",
    "    if x.ndim == 3 and x.shape[0] == 1:\n",
    "        x = x[0]\n",
    "    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    mn, mx = float(x.min()), float(x.max())\n",
    "    if mx > mn:\n",
    "        x = (x - mn) / (mx - mn)\n",
    "    else:\n",
    "        x = np.zeros_like(x, dtype=np.float32)\n",
    "    return x  # (160,w)\n",
    "\n",
    "def pad_x_to_wpad(x: np.ndarray) -> np.ndarray:\n",
    "    h, w = x.shape\n",
    "    out = np.zeros((h, W_PAD), dtype=np.float32)\n",
    "    out[:, :w] = x\n",
    "    return out\n",
    "\n",
    "y_df = pd.read_csv(Y_TRAIN_CSV, index_col=0)\n",
    "\n",
    "def restore_mask_from_row(row_values: np.ndarray) -> np.ndarray:\n",
    "    vals = row_values[row_values != -1]\n",
    "    return vals.reshape(H, -1).astype(np.int64)  # (160,160) or (160,272)\n",
    "\n",
    "def pad_mask_to_wpad(mask: np.ndarray, w: int) -> np.ndarray:\n",
    "    out = np.full((H, W_PAD), IGNORE_INDEX, dtype=np.int64)\n",
    "    out[:, :w] = mask\n",
    "    return out\n",
    "\n",
    "def make_valid_mask(w: int) -> np.ndarray:\n",
    "    valid = np.zeros((H, W_PAD), dtype=np.bool_)\n",
    "    valid[:, :w] = True\n",
    "    return valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9714606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labeled: 4410 w: {272: 3726, 160: 684} wells: [np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)]\n",
      "unlabeled   : 1980 w: {160: 1476, 272: 504}\n",
      "test        : 972 w: {272: 972}\n",
      "train_split: 4122 val_split: 288\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — Build manifests + split labeled train/val by well\n",
    "def build_manifest(x_dir: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for p in list_npy_files(x_dir):\n",
    "        stem = p.stem\n",
    "        try:\n",
    "            well, section, patch = parse_name(stem)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        arr = np.load(p, mmap_mode=\"r\")\n",
    "        if arr.ndim == 3 and arr.shape[0] == 1:\n",
    "            w = int(arr.shape[2])\n",
    "        elif arr.ndim == 2:\n",
    "            w = int(arr.shape[1])\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected shape {arr.shape} for {p}\")\n",
    "        rows.append({\"name\": stem, \"well\": well, \"section\": section, \"patch\": patch, \"w\": w, \"path\": str(p)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "train_df = build_manifest(X_TRAIN_DIR)\n",
    "unlab_df = build_manifest(X_UNLAB_DIR)\n",
    "test_df  = build_manifest(X_TEST_DIR)\n",
    "\n",
    "# keep only labeled train patches\n",
    "train_df = train_df[train_df[\"name\"].isin(y_df.index)].reset_index(drop=True)\n",
    "\n",
    "print(\"train labeled:\", len(train_df), \"w:\", train_df[\"w\"].value_counts().to_dict(), \"wells:\", sorted(train_df[\"well\"].unique()))\n",
    "print(\"unlabeled   :\", len(unlab_df), \"w:\", unlab_df[\"w\"].value_counts().to_dict())\n",
    "print(\"test        :\", len(test_df),  \"w:\", test_df[\"w\"].value_counts().to_dict())\n",
    "\n",
    "# split by well (you can change VAL_WELLS)\n",
    "VAL_WELLS = {5}\n",
    "train_split = train_df[~train_df[\"well\"].isin(VAL_WELLS)].reset_index(drop=True)\n",
    "val_split   = train_df[train_df[\"well\"].isin(VAL_WELLS)].reset_index(drop=True)\n",
    "print(\"train_split:\", len(train_split), \"val_split:\", len(val_split))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b4f575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled x: torch.Size([8, 1, 160, 288]) y: torch.Size([8, 160, 288]) valid: torch.Size([8, 160, 288])\n",
      "unlab   x: torch.Size([8, 1, 160, 288]) valid: torch.Size([8, 160, 288])\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — Datasets & DataLoaders\n",
    "class LabeledSegDataset(Dataset):\n",
    "    \"\"\"Returns: x(1,H,W_PAD), y(H,W_PAD), valid(H,W_PAD), meta\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, y_df: pd.DataFrame, train_mode: bool, seed: int = 42):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.y_df = y_df\n",
    "        self.train_mode = train_mode\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        name = row[\"name\"]\n",
    "        w = int(row[\"w\"])\n",
    "\n",
    "        x = load_x(Path(row[\"path\"]))      # (160,w)\n",
    "        x = pad_x_to_wpad(x)               # (160,288)\n",
    "\n",
    "        y_raw = restore_mask_from_row(self.y_df.loc[name].values)\n",
    "        y = pad_mask_to_wpad(y_raw, w=w)   # (160,288)\n",
    "\n",
    "        # safe aug: horizontal flip (geometry) — apply to both x/y\n",
    "        if self.train_mode and self.rng.rand() < 0.5:\n",
    "            x = np.flip(x, axis=1).copy()\n",
    "            y = np.flip(y, axis=1).copy()\n",
    "\n",
    "        x_t = torch.from_numpy(x).unsqueeze(0)          # (1,160,288)\n",
    "        y_t = torch.from_numpy(y).long()                # (160,288)\n",
    "        valid_t = torch.from_numpy(make_valid_mask(w))  # (160,288)\n",
    "\n",
    "        meta = {\"name\": name, \"well\": int(row[\"well\"]), \"orig_w\": w}\n",
    "        return x_t, y_t, valid_t, meta\n",
    "\n",
    "class UnlabeledSegDataset(Dataset):\n",
    "    \"\"\"Returns: x(1,H,W_PAD), valid(H,W_PAD), meta\"\"\"\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        name = row[\"name\"]\n",
    "        w = int(row[\"w\"])\n",
    "\n",
    "        x = load_x(Path(row[\"path\"]))      # (160,w)\n",
    "        x = pad_x_to_wpad(x)               # (160,288)\n",
    "\n",
    "        x_t = torch.from_numpy(x).unsqueeze(0)          # (1,160,288)\n",
    "        valid_t = torch.from_numpy(make_valid_mask(w))  # (160,288)\n",
    "        meta = {\"name\": name, \"orig_w\": w}\n",
    "        return x_t, valid_t, meta\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    LabeledSegDataset(train_split, y_df, train_mode=True, seed=123),\n",
    "    batch_size=8, shuffle=True, num_workers=0, pin_memory=(DEVICE==\"cuda\")\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    LabeledSegDataset(val_split, y_df, train_mode=False, seed=123),\n",
    "    batch_size=8, shuffle=False, num_workers=0, pin_memory=(DEVICE==\"cuda\")\n",
    ")\n",
    "unlab_loader = DataLoader(\n",
    "    UnlabeledSegDataset(unlab_df),\n",
    "    batch_size=8, shuffle=True, num_workers=0, pin_memory=(DEVICE==\"cuda\")\n",
    ")\n",
    "\n",
    "# quick sanity\n",
    "xb, yb, vb, mb = next(iter(train_loader))\n",
    "xu, vu, mu = next(iter(unlab_loader))\n",
    "print(\"labeled x:\", xb.shape, \"y:\", yb.shape, \"valid:\", vb.shape)\n",
    "print(\"unlab   x:\", xu.shape, \"valid:\", vu.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88611f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 — Loss/Metric + logit upsampling\n",
    "# labeled loss: CE(weighted) + Dice; unlabeled pseudo loss: CE(unweighted)\n",
    "ce_weights = torch.tensor([1.0, 3.0, 4.0], dtype=torch.float32).to(DEVICE)\n",
    "ce_l = nn.CrossEntropyLoss(weight=ce_weights, ignore_index=IGNORE_INDEX)\n",
    "ce_u = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
    "\n",
    "def upsample_logits(logits: torch.Tensor, target_hw) -> torch.Tensor:\n",
    "    return F.interpolate(logits, size=target_hw, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "def soft_dice_loss(logits: torch.Tensor, target: torch.Tensor, smooth: float = 1.0) -> torch.Tensor:\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    valid = (target != IGNORE_INDEX).unsqueeze(1)\n",
    "\n",
    "    t = target.clone()\n",
    "    t[t == IGNORE_INDEX] = 0\n",
    "    onehot = F.one_hot(t, num_classes=NUM_CLASSES).permute(0,3,1,2).float()\n",
    "\n",
    "    probs = probs * valid\n",
    "    onehot = onehot * valid\n",
    "\n",
    "    inter = (probs * onehot).sum((0,2,3))\n",
    "    denom = (probs + onehot).sum((0,2,3))\n",
    "    dice = (2*inter + smooth) / (denom + smooth)\n",
    "    return 1.0 - dice.mean()\n",
    "\n",
    "def labeled_loss(logits_up: torch.Tensor, y: torch.Tensor, dice_w: float = 0.5) -> torch.Tensor:\n",
    "    return (1-dice_w) * ce_l(logits_up, y) + dice_w * soft_dice_loss(logits_up, y)\n",
    "\n",
    "def mean_iou(pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "    valid = (target != IGNORE_INDEX)\n",
    "    ious = []\n",
    "    for c in range(NUM_CLASSES):\n",
    "        p = (pred == c) & valid\n",
    "        t = (target == c) & valid\n",
    "        inter = (p & t).sum().float()\n",
    "        union = (p | t).sum().float()\n",
    "        ious.append(torch.tensor(1.0, device=pred.device) if union.item() == 0 else inter / union)\n",
    "    return torch.stack(ious).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08c92d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b2-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b2-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_22368\\3809490178.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(SUP_BEST, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized student/teacher from: exp_outputs\\Exp02_SegFormer_Supervised\\best_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 — Model: student + EMA teacher (SegFormer), init from supervised best\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "BACKBONE = \"nvidia/segformer-b2-finetuned-ade-512-512\"\n",
    "\n",
    "student = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    BACKBONE, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "\n",
    "teacher = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    BACKBONE, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "\n",
    "# init both from supervised checkpoint (method2 best)\n",
    "state = torch.load(SUP_BEST, map_location=DEVICE)\n",
    "student.load_state_dict(state)\n",
    "teacher.load_state_dict(state)\n",
    "\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "print(\"Initialized student/teacher from:\", SUP_BEST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a63bedb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_22368\\3700262783.py:53: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
      "[Exp03] train ep1 (lam_u=0.20):   0%|          | 0/516 [00:00<?, ?it/s]C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_22368\\3700262783.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
      "                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp03] ep01/10  val_mIoU=0.7957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp03] ep02/10  val_mIoU=0.8044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp03] ep03/10  val_mIoU=0.8079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp03] ep04/10  val_mIoU=0.8010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp03] ep05/10  val_mIoU=0.8007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp03] ep06/10  val_mIoU=0.7977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp03] ep07/10  val_mIoU=0.8112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp03] ep08/10  val_mIoU=0.7996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp03] ep09/10  val_mIoU=0.8012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp03] ep10/10  val_mIoU=0.7968\n",
      "[Exp03] BEST val mIoU: 0.8112312638097339 saved: exp_outputs\\Exp03_SegFormer_SemiSupervised\\best_state_dict.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Cell 6 — Semi-supervised training (EMA Teacher + high-conf pseudo labels)\n",
    "def rampup(epoch: int, ramp_epochs: int = 5) -> float:\n",
    "    if epoch < ramp_epochs:\n",
    "        return float(epoch + 1) / float(ramp_epochs)\n",
    "    return 1.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(teacher_model, student_model, alpha: float):\n",
    "    # EMA for parameters\n",
    "    for t_p, s_p in zip(teacher_model.parameters(), student_model.parameters()):\n",
    "        t_p.data.mul_(alpha).add_(s_p.data, alpha=1.0 - alpha)\n",
    "    # buffers: copy (stable)\n",
    "    for t_b, s_b in zip(teacher_model.buffers(), student_model.buffers()):\n",
    "        t_b.copy_(s_b)\n",
    "\n",
    "def weak_aug(x: torch.Tensor) -> torch.Tensor:\n",
    "    # x: (B,1,H,W) in [0,1]\n",
    "    B = x.size(0)\n",
    "    a = torch.empty((B,1,1,1), device=x.device).uniform_(0.95, 1.05)\n",
    "    b = torch.empty((B,1,1,1), device=x.device).uniform_(-0.03, 0.03)\n",
    "    return torch.clamp(x * a + b, 0.0, 1.0)\n",
    "\n",
    "def strong_aug(x: torch.Tensor) -> torch.Tensor:\n",
    "    # intensity + noise + cutout (NO geometry to keep pseudo-label alignment)\n",
    "    B, _, Hh, Ww = x.shape\n",
    "    a = torch.empty((B,1,1,1), device=x.device).uniform_(0.85, 1.15)\n",
    "    b = torch.empty((B,1,1,1), device=x.device).uniform_(-0.08, 0.08)\n",
    "    out = torch.clamp(x * a + b, 0.0, 1.0)\n",
    "    sigma = torch.empty((B,1,1,1), device=x.device).uniform_(0.0, 0.06)\n",
    "    out = torch.clamp(out + torch.randn_like(out) * sigma, 0.0, 1.0)\n",
    "\n",
    "    # cutout per-sample\n",
    "    for i in range(B):\n",
    "        if torch.rand((), device=x.device).item() < 0.5:\n",
    "            ch = int(torch.randint(low=10, high=50, size=(1,), device=x.device).item())\n",
    "            cw = int(torch.randint(low=10, high=80, size=(1,), device=x.device).item())\n",
    "            y0 = int(torch.randint(low=0, high=Hh-ch+1, size=(1,), device=x.device).item())\n",
    "            x0 = int(torch.randint(low=0, high=Ww-cw+1, size=(1,), device=x.device).item())\n",
    "            out[i, :, y0:y0+ch, x0:x0+cw] = 0.0\n",
    "    return out\n",
    "\n",
    "# Hyperparams (you can tune)\n",
    "EPOCHS    = 10\n",
    "LR        = 6e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "TAU       = 0.95      # pseudo-label confidence threshold\n",
    "LAMBDA_U  = 1.0       # max weight for unlabeled loss\n",
    "RAMP_E    = 5         # ramp-up epochs for unlabeled weight\n",
    "EMA_ALPHA = 0.996     # teacher EMA momentum\n",
    "\n",
    "best_path = OUT_DIR / \"best_state_dict.pt\"\n",
    "opt = torch.optim.AdamW(student.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "best_miou = -1.0\n",
    "\n",
    "# iterator helper\n",
    "def cycle_loader(loader):\n",
    "    while True:\n",
    "        for batch in loader:\n",
    "            yield batch\n",
    "\n",
    "unlab_iter = cycle_loader(unlab_loader)\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    lam_u = LAMBDA_U * rampup(ep-1, ramp_epochs=RAMP_E)\n",
    "\n",
    "    tr_loss, tr_l, tr_u, n = 0.0, 0.0, 0.0, 0\n",
    "    pbar = tqdm(train_loader, desc=f\"[Exp03] train ep{ep} (lam_u={lam_u:.2f})\", leave=False)\n",
    "\n",
    "    for x_l, y_l, valid_l, meta_l in pbar:\n",
    "        x_u, valid_u, meta_u = next(unlab_iter)\n",
    "\n",
    "        x_l = x_l.to(DEVICE)                 # (B,1,160,288)\n",
    "        y_l = y_l.to(DEVICE)                 # (B,160,288)\n",
    "        valid_u = valid_u.to(DEVICE)         # (B,160,288) bool\n",
    "        x_u = x_u.to(DEVICE)                 # (B,1,160,288)\n",
    "\n",
    "        # 3-ch for SegFormer\n",
    "        x_l3 = x_l.repeat(1,3,1,1)           # (B,3,160,288)\n",
    "\n",
    "        # unlabeled: weak for teacher, strong for student\n",
    "        x_u_w = weak_aug(x_u).repeat(1,3,1,1)\n",
    "        x_u_s = strong_aug(x_u).repeat(1,3,1,1)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "            # ----- labeled loss -----\n",
    "            logits_l = student(pixel_values=x_l3).logits\n",
    "            logits_l = upsample_logits(logits_l, y_l.shape[-2:])      # (B,C,160,288)\n",
    "            loss_l = labeled_loss(logits_l, y_l, dice_w=0.5)\n",
    "\n",
    "            # ----- pseudo labels from teacher (weak) -----\n",
    "            with torch.no_grad():\n",
    "                logits_t = teacher(pixel_values=x_u_w).logits\n",
    "                logits_t = upsample_logits(logits_t, (H, W_PAD))\n",
    "                probs_t = torch.softmax(logits_t, dim=1)\n",
    "                conf, pseudo = torch.max(probs_t, dim=1)             # (B,160,288)\n",
    "\n",
    "                # mask: high confidence & valid (exclude padded columns)\n",
    "                mask = (conf >= TAU) & valid_u\n",
    "                pseudo_pl = pseudo.clone()\n",
    "                pseudo_pl[~mask] = IGNORE_INDEX\n",
    "\n",
    "            # ----- unlabeled loss on student (strong) -----\n",
    "            if lam_u > 0:\n",
    "                logits_u = student(pixel_values=x_u_s).logits\n",
    "                logits_u = upsample_logits(logits_u, (H, W_PAD))\n",
    "                loss_u = ce_u(logits_u, pseudo_pl)\n",
    "            else:\n",
    "                loss_u = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "            loss = loss_l + lam_u * loss_u\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "\n",
    "        # EMA teacher update (after student step)\n",
    "        ema_update(teacher, student, alpha=EMA_ALPHA)\n",
    "\n",
    "        bs = x_l.size(0)\n",
    "        tr_loss += float(loss.item()) * bs\n",
    "        tr_l    += float(loss_l.item()) * bs\n",
    "        tr_u    += float(loss_u.item()) * bs\n",
    "        n += bs\n",
    "        pbar.set_postfix({\"loss\": tr_loss/max(1,n), \"L\": tr_l/max(1,n), \"U\": tr_u/max(1,n)})\n",
    "\n",
    "    # ----- validation on labeled val -----\n",
    "    student.eval()\n",
    "    miou_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, valid, meta in tqdm(val_loader, desc=f\"[Exp03] val ep{ep}\", leave=False):\n",
    "            x = x.to(DEVICE)                 # (B,1,160,288)\n",
    "            y = y.to(DEVICE)                 # (B,160,288)\n",
    "            x3 = x.repeat(1,3,1,1)\n",
    "            logits = student(pixel_values=x3).logits\n",
    "            logits = upsample_logits(logits, y.shape[-2:])\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            miou_sum += float(mean_iou(pred, y).item()) * x.size(0)\n",
    "            n += x.size(0)\n",
    "    val_miou = miou_sum / max(1, n)\n",
    "    print(f\"[Exp03] ep{ep:02d}/{EPOCHS}  val_mIoU={val_miou:.4f}\")\n",
    "\n",
    "    if val_miou > best_miou:\n",
    "        best_miou = val_miou\n",
    "        torch.save(student.state_dict(), best_path)\n",
    "\n",
    "print(\"[Exp03] BEST val mIoU:\", best_miou, \"saved:\", best_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd870042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample rows: 972 name_col: Unnamed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_22368\\1968903237.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(best_path, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best: exp_outputs\\Exp03_SegFormer_SemiSupervised\\best_state_dict.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved npy predictions to: exp_outputs\\Exp03_SegFormer_SemiSupervised\\test_predictions\n",
      "Saved submission: exp_outputs\\Exp03_SegFormer_SemiSupervised\\y_test_submission_MATCH_SAMPLE.csv shape: (972, 43521)\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — Predict X_test strictly by SAMPLE_SUB order + save npy + build correct submission CSV\n",
    "# 1) ordered names from sample\n",
    "sample = pd.read_csv(SAMPLE_SUB)\n",
    "name_col = sample.columns[0]\n",
    "ordered_names_raw = sample[name_col].astype(str).tolist()\n",
    "\n",
    "def norm_name(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    if s.lower().endswith(\".npy\"):\n",
    "        s = s[:-4]\n",
    "    return s\n",
    "\n",
    "ordered_names = [norm_name(n) for n in ordered_names_raw]\n",
    "print(\"sample rows:\", len(ordered_names), \"name_col:\", name_col)\n",
    "\n",
    "# 2) build test index (de-dup)\n",
    "test_files = list_npy_files(X_TEST_DIR)\n",
    "test_index = {p.stem: p for p in test_files}\n",
    "test_index.update({p.stem.lower(): p for p in test_files})\n",
    "\n",
    "# 3) load best student\n",
    "student.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "student.eval()\n",
    "print(\"Loaded best:\", best_path)\n",
    "\n",
    "pred_dir = OUT_DIR / \"test_predictions\"\n",
    "pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "for p in pred_dir.glob(\"*.npy\"):\n",
    "    p.unlink()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name in tqdm(ordered_names, desc=\"[Exp03] predict test\", leave=False):\n",
    "        key = name if name in test_index else name.lower()\n",
    "        if key not in test_index:\n",
    "            hits = list(X_TEST_DIR.rglob(f\"{name}.npy\")) + list(X_TEST_DIR.rglob(f\"{name}.NPY\"))\n",
    "            if len(hits) == 0:\n",
    "                raise FileNotFoundError(f\"X_test missing: {name}.npy\")\n",
    "            x_path = hits[0]\n",
    "        else:\n",
    "            x_path = test_index[key]\n",
    "\n",
    "        x = load_x(x_path)           # (160,w)\n",
    "        w = x.shape[1]\n",
    "        x_pad = pad_x_to_wpad(x)     # (160,288)\n",
    "        x_t = torch.from_numpy(x_pad).unsqueeze(0).unsqueeze(0).to(DEVICE)  # (1,1,160,288)\n",
    "        x_t = x_t.repeat(1,3,1,1)    # (1,3,160,288)\n",
    "\n",
    "        logits = student(pixel_values=x_t).logits\n",
    "        logits = upsample_logits(logits, (H, W_PAD))\n",
    "        pred = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy().astype(np.int64)  # (160,288)\n",
    "        pred = pred[:, :w]  # crop back to original width\n",
    "        np.save(pred_dir / f\"{name}.npy\", pred)\n",
    "\n",
    "print(\"saved npy predictions to:\", pred_dir)\n",
    "\n",
    "# 4) build submission exactly matching sample format\n",
    "size_labels = 272\n",
    "flat_len = H * size_labels\n",
    "\n",
    "pred_map = {}\n",
    "for p in pred_dir.glob(\"*.npy\"):\n",
    "    nm = p.stem\n",
    "    pred = np.load(p)\n",
    "    if pred.shape[1] != size_labels:\n",
    "        aux = -1 + np.zeros(flat_len, dtype=np.int64)\n",
    "        aux[0:H*H] = pred.flatten()\n",
    "    else:\n",
    "        aux = pred.flatten().astype(np.int64)\n",
    "    pred_map[nm] = aux\n",
    "\n",
    "missing = [n for n in ordered_names if n not in pred_map]\n",
    "assert len(missing) == 0, f\"missing predictions: {missing[:10]}\"\n",
    "\n",
    "data = np.stack([pred_map[n] for n in ordered_names], axis=0)  # (972, 43520)\n",
    "col_names = [str(i) for i in range(flat_len)]\n",
    "sub_df = pd.DataFrame(data, columns=col_names)\n",
    "sub_df.insert(0, name_col, ordered_names_raw)\n",
    "\n",
    "out_csv = OUT_DIR / \"y_test_submission_MATCH_SAMPLE.csv\"\n",
    "sub_df.to_csv(out_csv, index=False)\n",
    "print(\"Saved submission:\", out_csv, \"shape:\", sub_df.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
