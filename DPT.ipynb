{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d12efdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\deep-torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Train: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\X_train_uDRk9z9\\images\n",
      "Test : C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\X_test_xNbnvIa\\images\n",
      "Train samples: 2790 | Val samples: 1620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\deep-torch\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lenovo\\.cache\\huggingface\\hub\\models--timm--vit_base_patch16_224.augreg2_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | train_loss=0.1310 | val_loss=0.0810\n",
      "  -> best saved: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_dpt.pth\n",
      "Epoch 02/20 | train_loss=0.0776 | val_loss=0.0825\n",
      "Epoch 03/20 | train_loss=0.0669 | val_loss=0.0937\n",
      "Epoch 04/20 | train_loss=0.0643 | val_loss=0.0849\n",
      "Epoch 05/20 | train_loss=0.0606 | val_loss=0.1009\n",
      "Epoch 06/20 | train_loss=0.0598 | val_loss=0.0824\n",
      "Epoch 07/20 | train_loss=0.0532 | val_loss=0.0823\n",
      "Epoch 08/20 | train_loss=0.0509 | val_loss=0.0885\n",
      "Epoch 09/20 | train_loss=0.0520 | val_loss=0.1033\n",
      "Epoch 10/20 | train_loss=0.0472 | val_loss=0.0876\n",
      "Epoch 11/20 | train_loss=0.0444 | val_loss=0.0891\n",
      "Epoch 12/20 | train_loss=0.0443 | val_loss=0.0897\n",
      "Epoch 13/20 | train_loss=0.0406 | val_loss=0.0911\n",
      "Epoch 14/20 | train_loss=0.0420 | val_loss=0.0902\n",
      "Epoch 15/20 | train_loss=0.0395 | val_loss=0.0965\n",
      "Epoch 16/20 | train_loss=0.0346 | val_loss=0.0980\n",
      "Epoch 17/20 | train_loss=0.0315 | val_loss=0.1069\n",
      "Epoch 18/20 | train_loss=0.0291 | val_loss=0.1004\n",
      "Epoch 19/20 | train_loss=0.0279 | val_loss=0.1170\n",
      "Epoch 20/20 | train_loss=0.0275 | val_loss=0.1033\n",
      "[OK] submission saved to: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "DPT (Dense Prediction Transformer) - Full runnable code (NO cv2)\n",
    "Using segmentation-models-pytorch (smp) built-in DPT.\n",
    "\n",
    "Why this works:\n",
    "- DPT uses a ViT-like backbone, usually requires fixed input size (e.g., 224x224).\n",
    "- We follow the same safe pipeline as before:\n",
    "    (160, w) -> pad to (160,272) -> resize to (224,224) -> model\n",
    "    model output (224,224) -> resize back to (160,272) -> crop to raw_w\n",
    "- submission.csv must be 160*272 flattened with -1 padding\n",
    "\n",
    "Data:\n",
    "- Train images: Desktop/deep_datachallenge/X_train_uDRk9z9/images (well1-6)\n",
    "- Train labels: Desktop/deep_datachallenge/Y_train_T9NrBYo.csv\n",
    "- Test images:  Desktop/deep_datachallenge/X_test_xNbnvIa/images (well7-11)\n",
    "\n",
    "Split:\n",
    "- Train: well1-5\n",
    "- Val:   well6\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) Config\n",
    "# =========================\n",
    "DATA_ROOT = Path(r\"C:\\Users\\lenovo\\Desktop\\deep_datachallenge\")\n",
    "\n",
    "TRAIN_IMG_DIR = DATA_ROOT / \"X_train_uDRk9z9\" / \"images\"\n",
    "TEST_IMG_DIR  = DATA_ROOT / \"X_test_xNbnvIa\" / \"images\"\n",
    "Y_TRAIN_CSV   = DATA_ROOT / \"Y_train_T9NrBYo.csv\"\n",
    "\n",
    "# submission resolution (fixed by challenge)\n",
    "H_SUB, W_SUB = 160, 272\n",
    "\n",
    "# model input resolution (ViT/DPT often expects fixed size)\n",
    "H_MODEL, W_MODEL = 224, 224\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "IGNORE_INDEX = -1\n",
    "\n",
    "BATCH_SIZE = 4          # 4060(8GB): start with 2~4\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 20\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Utils\n",
    "# =========================\n",
    "def parse_well_id(name: str) -> int:\n",
    "    \"\"\"Extract well id from 'well_6_section_...' -> 6\"\"\"\n",
    "    m = re.search(r\"well_(\\d+)_\", name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "\n",
    "def minmax_norm(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Normalize to [0,1], handle NaN/Inf.\"\"\"\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    mn, mx = float(x.min()), float(x.max())\n",
    "    if mx - mn < 1e-6:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return (x - mn) / (mx - mn)\n",
    "\n",
    "\n",
    "def pad_to_160x272(img: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"(160,w)->(160,272), pad right with 0.\"\"\"\n",
    "    h, w = img.shape\n",
    "    assert h == H_SUB, f\"Expected height {H_SUB}, got {h}\"\n",
    "    out = np.zeros((H_SUB, W_SUB), dtype=img.dtype)\n",
    "    out[:, :min(w, W_SUB)] = img[:, :min(w, W_SUB)]\n",
    "    return out\n",
    "\n",
    "\n",
    "def decode_mask_from_csv_row(row: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    CSV row is flattened mask with -1 padding.\n",
    "    Remove -1 then reshape to (160, w).\n",
    "    \"\"\"\n",
    "    valid = row[row != IGNORE_INDEX]\n",
    "    assert len(valid) % H_SUB == 0, f\"Valid mask length {len(valid)} not divisible by {H_SUB}\"\n",
    "    w = len(valid) // H_SUB\n",
    "    return valid.reshape(H_SUB, w).astype(np.int64)\n",
    "\n",
    "\n",
    "def pad_mask_to_160x272(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"(160,w)->(160,272) with -1 padding on the right.\"\"\"\n",
    "    h, w = mask.shape\n",
    "    assert h == H_SUB\n",
    "    out = np.full((H_SUB, W_SUB), IGNORE_INDEX, dtype=np.int64)\n",
    "    out[:, :min(w, W_SUB)] = mask[:, :min(w, W_SUB)]\n",
    "    return out\n",
    "\n",
    "\n",
    "def resize_img_np_to_model(img_160x272: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"(160,272)->(224,224), bilinear\"\"\"\n",
    "    x = torch.from_numpy(img_160x272).unsqueeze(0).unsqueeze(0).float()  # (1,1,160,272)\n",
    "    x = F.interpolate(x, size=(H_MODEL, W_MODEL), mode=\"bilinear\", align_corners=False)\n",
    "    return x.squeeze(0).squeeze(0).numpy()\n",
    "\n",
    "\n",
    "def resize_mask_np_to_model(mask_160x272: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"(160,272)->(224,224), nearest (for labels)\"\"\"\n",
    "    x = torch.from_numpy(mask_160x272).unsqueeze(0).unsqueeze(0).float()  # (1,1,160,272)\n",
    "    x = F.interpolate(x, size=(H_MODEL, W_MODEL), mode=\"nearest\")\n",
    "    return x.squeeze(0).squeeze(0).long().numpy()\n",
    "\n",
    "\n",
    "def resize_pred_np_to_sub(pred_224x224: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"(224,224)->(160,272), nearest\"\"\"\n",
    "    x = torch.from_numpy(pred_224x224).unsqueeze(0).unsqueeze(0).float()\n",
    "    x = F.interpolate(x, size=(H_SUB, W_SUB), mode=\"nearest\")\n",
    "    return x.squeeze(0).squeeze(0).long().numpy()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) Dataset\n",
    "# =========================\n",
    "class WellSegDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, y_csv_path: Path = None):\n",
    "        self.images_dir = images_dir\n",
    "        self.paths = sorted(images_dir.glob(\"*.npy\"))\n",
    "        self.names = [p.stem for p in self.paths]\n",
    "\n",
    "        self.has_label = y_csv_path is not None\n",
    "        self.y_df = pd.read_csv(y_csv_path, index_col=0) if self.has_label else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        name = self.names[idx]\n",
    "        img = np.load(self.paths[idx])      # (160,160) or (160,272)\n",
    "        raw_w = int(img.shape[1])\n",
    "\n",
    "        img = minmax_norm(img)\n",
    "        img_160x272 = pad_to_160x272(img)\n",
    "        img_224 = resize_img_np_to_model(img_160x272)\n",
    "        x = torch.from_numpy(img_224).unsqueeze(0).float()  # (1,224,224)\n",
    "\n",
    "        if not self.has_label:\n",
    "            return {\"name\": name, \"image\": x, \"raw_w\": raw_w}\n",
    "\n",
    "        row = self.y_df.loc[name].values.astype(np.int64)\n",
    "        mask = decode_mask_from_csv_row(row)     # (160,w)\n",
    "        mask_160x272 = pad_mask_to_160x272(mask) # (160,272)\n",
    "        mask_224 = resize_mask_np_to_model(mask_160x272)    # (224,224)\n",
    "        y = torch.from_numpy(mask_224).long()\n",
    "\n",
    "        return {\"name\": name, \"image\": x, \"mask\": y, \"raw_w\": raw_w}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Model: DPT (smp)\n",
    "# =========================\n",
    "def build_dpt(num_classes: int) -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    SMP has DPT model. You choose a ViT / transformer encoder from timm.\n",
    "\n",
    "    Common safe encoders (usually available):\n",
    "    - \"tu-vit_base_patch16_224\"\n",
    "    - \"vit_base_patch16_224\"\n",
    "    If pretrained weights download fails, set encoder_weights=None.\n",
    "    \"\"\"\n",
    "    encoder_name = \"tu-vit_base_patch16_224\"   # you can also try \"vit_base_patch16_224\"\n",
    "    encoder_weights = \"imagenet\"              # if this fails, set to None\n",
    "\n",
    "    model = smp.DPT(\n",
    "        encoder_name=encoder_name,\n",
    "        encoder_weights=encoder_weights,\n",
    "        in_channels=1,\n",
    "        classes=num_classes,\n",
    "        activation=None,\n",
    "        # If you want to try variable sizes (if supported by encoder):\n",
    "        # dynamic_img_size=True,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Train / Val\n",
    "# =========================\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "\n",
    "    for b in loader:\n",
    "        x = b[\"image\"].to(DEVICE)   # (B,1,224,224)\n",
    "        y = b[\"mask\"].to(DEVICE)    # (B,224,224)\n",
    "\n",
    "        logits = model(x)           # (B,C,224,224)\n",
    "        loss = F.cross_entropy(logits, y, ignore_index=IGNORE_INDEX)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += float(loss.item()) * x.size(0)\n",
    "\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_one_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "\n",
    "    for b in loader:\n",
    "        x = b[\"image\"].to(DEVICE)\n",
    "        y = b[\"mask\"].to(DEVICE)\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y, ignore_index=IGNORE_INDEX)\n",
    "        total += float(loss.item()) * x.size(0)\n",
    "\n",
    "    return total / len(loader.dataset)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Inference & submission\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def predict_and_make_submission(model, test_images_dir: Path, out_csv: Path):\n",
    "    model.eval()\n",
    "\n",
    "    test_ds = WellSegDataset(test_images_dir, y_csv_path=None)\n",
    "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "    preds = {}\n",
    "\n",
    "    for b in test_loader:\n",
    "        name = b[\"name\"][0]\n",
    "        raw_w = int(b[\"raw_w\"][0])\n",
    "        x = b[\"image\"].to(DEVICE)   # (1,1,224,224)\n",
    "\n",
    "        logits = model(x)           # (1,C,224,224)\n",
    "        pred224 = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy().astype(np.int64)  # (224,224)\n",
    "\n",
    "        pred160x272 = resize_pred_np_to_sub(pred224)      # (160,272)\n",
    "        pred160 = pred160x272[:, :raw_w]                  # (160,raw_w)\n",
    "\n",
    "        flat = np.full((H_SUB * W_SUB,), IGNORE_INDEX, dtype=np.int64)\n",
    "        flat[: H_SUB * raw_w] = pred160.flatten()\n",
    "        preds[name] = flat\n",
    "\n",
    "    pd.DataFrame(preds, dtype=\"int64\").T.to_csv(out_csv)\n",
    "    print(f\"[OK] submission saved to: {out_csv}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) Main\n",
    "# =========================\n",
    "def main():\n",
    "    print(\"DEVICE:\", DEVICE)\n",
    "    print(\"Train:\", TRAIN_IMG_DIR)\n",
    "    print(\"Test :\", TEST_IMG_DIR)\n",
    "\n",
    "    ds_all = WellSegDataset(TRAIN_IMG_DIR, Y_TRAIN_CSV)\n",
    "\n",
    "    train_idx, val_idx = [], []\n",
    "    for i, n in enumerate(ds_all.names):\n",
    "        if parse_well_id(n) == 6:\n",
    "            val_idx.append(i)\n",
    "        else:\n",
    "            train_idx.append(i)\n",
    "\n",
    "    train_ds = Subset(ds_all, train_idx)  # well1-5\n",
    "    val_ds   = Subset(ds_all, val_idx)    # well6\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    print(f\"Train samples: {len(train_ds)} | Val samples: {len(val_ds)}\")\n",
    "\n",
    "    model = build_dpt(NUM_CLASSES).to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val = 1e9\n",
    "    best_path = DATA_ROOT / \"best_dpt.pth\"\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr = train_one_epoch(model, train_loader, optimizer)\n",
    "        va = eval_one_epoch(model, val_loader)\n",
    "        print(f\"Epoch {epoch:02d}/{EPOCHS} | train_loss={tr:.4f} | val_loss={va:.4f}\")\n",
    "\n",
    "        if va < best_val:\n",
    "            best_val = va\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(\"  -> best saved:\", best_path)\n",
    "\n",
    "    # test inference\n",
    "    model.load_state_dict(torch.load(best_path, map_location=DEVICE, weights_only=True))\n",
    "    out_csv = DATA_ROOT / \"submission.csv\"\n",
    "    predict_and_make_submission(model, TEST_IMG_DIR, out_csv)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
