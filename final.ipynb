{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "184147d5",
   "metadata": {},
   "source": [
    "## SegNet\n",
    "\n",
    "Main program\n",
    "(Many of the comments here are in Chinese, so I used Chatgpt to translate them into English.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0024b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from functools import partial\n",
    "\n",
    "# -----------------------\n",
    "# split utils (by well)\n",
    "# -----------------------\n",
    "_WELL_RE = re.compile(r\"^well_(\\d+)_section_(\\d+)_patch_(\\d+)$\")\n",
    "\n",
    "def parse_patch_id(patch_id: str) -> tuple[int, int, int]:\n",
    "    m = _WELL_RE.match(patch_id)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Bad patch id format: {patch_id}\")\n",
    "    return int(m.group(1)), int(m.group(2)), int(m.group(3))\n",
    "\n",
    "def split_ids_by_well(all_ids, val_wells: list[int]):\n",
    "    val_wells = set(val_wells)\n",
    "    tr_ids, va_ids = [], []\n",
    "    for pid in all_ids:\n",
    "        w, _, _ = parse_patch_id(str(pid))\n",
    "        (va_ids if w in val_wells else tr_ids).append(str(pid))\n",
    "    return tr_ids, va_ids\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# small utils\n",
    "# -----------------------\n",
    "def patch_minmax(x: np.ndarray, eps: float = 1e-6) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.nan_to_num(x, nan=0.0)\n",
    "    mn, mx = float(x.min()), float(x.max())\n",
    "    if mx - mn < eps:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return (x - mn) / (mx - mn)\n",
    "\n",
    "def infer_num_classes(y_df: pd.DataFrame) -> int:\n",
    "    v = y_df.values.reshape(-1)\n",
    "    v = v[v != -1]\n",
    "    v = v[~pd.isna(v)]\n",
    "    return int(v.max()) + 1\n",
    "\n",
    "def resize_img(x: torch.Tensor, size_hw):\n",
    "    return F.interpolate(x, size=size_hw, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "def resize_mask(y: torch.Tensor, size_hw):\n",
    "    # y: [B,H,W] int64\n",
    "    y = F.interpolate(y.unsqueeze(1).float(), size=size_hw, mode=\"nearest\")\n",
    "    return y.squeeze(1).long()\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# dataset\n",
    "# -----------------------\n",
    "class WellDataset(Dataset):\n",
    "    def __init__(self, x_dir: Path, y_df: pd.DataFrame | None = None, include_ids=None):\n",
    "        self.x_dir = Path(x_dir)\n",
    "        self.y_df = y_df\n",
    "        self.is_train = y_df is not None\n",
    "\n",
    "        files = sorted(self.x_dir.rglob(\"*.npy\"))\n",
    "\n",
    "        if self.is_train:\n",
    "            idx = set(self.y_df.index.astype(str))\n",
    "            files = [p for p in files if p.stem in idx]\n",
    "\n",
    "        if include_ids is not None:\n",
    "            keep = set(map(str, include_ids))\n",
    "            files = [p for p in files if p.stem in keep]\n",
    "\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        p = self.files[i]\n",
    "        stem = p.stem\n",
    "        img = np.load(p)\n",
    "        if img.ndim == 3 and img.shape[-1] == 1:\n",
    "            img = img[..., 0]\n",
    "        img = patch_minmax(img)                     # NaN->0 + minmax\n",
    "        H, W = img.shape\n",
    "        item = {\n",
    "            \"id\": stem,\n",
    "            \"orig_hw\": (H, W),\n",
    "            \"img\": torch.from_numpy(img).unsqueeze(0),  # [1,H,W]\n",
    "        }\n",
    "        if self.is_train:\n",
    "            row = self.y_df.loc[stem].to_numpy()\n",
    "            row = row[row != -1]                   # remove padding -1 :contentReference[oaicite:1]{index=1}\n",
    "            mask = row.reshape(160, -1).astype(np.int64)\n",
    "            item[\"mask\"] = torch.from_numpy(mask)  # [160,W]\n",
    "        return item\n",
    "\n",
    "def collate(batch, size_hw=(160, 272), is_train=True):\n",
    "    Ht, Wt = size_hw  # (160,272)\n",
    "\n",
    "    # Pad all images to width 272\n",
    "    imgs = []\n",
    "    for b in batch:\n",
    "        x = b[\"img\"]\n",
    "        H, W = x.shape[-2], x.shape[-1]\n",
    "        # Height is theoretically always 160; keep this as a safety check\n",
    "        if H < Ht:\n",
    "            x = F.pad(x, (0, 0, 0, Ht - H), value=0.0)\n",
    "        elif H > Ht:\n",
    "            x = x[..., :Ht, :]\n",
    "\n",
    "        if W < Wt:\n",
    "            x = F.pad(x, (0, Wt - W, 0, 0), value=0.0)\n",
    "        elif W > Wt:\n",
    "            x = x[..., :, :Wt]\n",
    "\n",
    "        imgs.append(x)\n",
    "    imgs = torch.stack(imgs, 0)\n",
    "\n",
    "    out = {\"id\":[b[\"id\"] for b in batch], \"orig_hw\":[b[\"orig_hw\"] for b in batch], \"img\":imgs}\n",
    "\n",
    "    if is_train:\n",
    "        masks = []\n",
    "        for b in batch:\n",
    "            y = b[\"mask\"]  # [160,W]\n",
    "            H, W = y.shape[-2], y.shape[-1]\n",
    "\n",
    "            if H < Ht:\n",
    "                y = F.pad(y, (0, 0, 0, Ht - H), value=-1)\n",
    "            elif H > Ht:\n",
    "                y = y[:Ht, :]\n",
    "            if W < Wt:\n",
    "                y = F.pad(y, (0, Wt - W, 0, 0), value=-1) # pad -1 for ignore_index\n",
    "            elif W > Wt:\n",
    "                y = y[:, :Wt]\n",
    "            masks.append(y)\n",
    "        masks = torch.stack(masks, 0)         # [B,160,Wmax]\n",
    "        out[\"mask\"] = masks\n",
    "\n",
    "    return out\n",
    "\n",
    "# -----------------------\n",
    "# SegNet (VGG-like)\n",
    "# -----------------------\n",
    "def vgg_block(in_ch, out_ch, n_conv):\n",
    "    layers = []\n",
    "    ch = in_ch\n",
    "    for _ in range(n_conv):\n",
    "        layers += [nn.Conv2d(ch, out_ch, 3, padding=1),\n",
    "                   nn.BatchNorm2d(out_ch),\n",
    "                   nn.ReLU(inplace=True)]\n",
    "        ch = out_ch\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class SegNetVGG(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.enc1 = vgg_block(in_channels, 64, 2)\n",
    "        self.enc2 = vgg_block(64, 128, 2)\n",
    "        self.enc3 = vgg_block(128, 256, 3)\n",
    "        self.enc4 = vgg_block(256, 512, 3)\n",
    "        self.enc5 = vgg_block(512, 512, 3)\n",
    "        self.pool = nn.MaxPool2d(2,2, return_indices=True)\n",
    "\n",
    "        self.unpool = nn.MaxUnpool2d(2,2)\n",
    "        self.dec5 = vgg_block(512, 512, 3)\n",
    "        self.dec4 = vgg_block(512, 256, 3)\n",
    "        self.dec3 = vgg_block(256, 128, 3)\n",
    "        self.dec2 = vgg_block(128, 64, 2)\n",
    "        self.dec1 = vgg_block(64, 64, 2)\n",
    "        self.head = nn.Conv2d(64, num_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        sizes, idxs = [], []\n",
    "\n",
    "        x = self.enc1(x); sizes.append(x.size()); x, i = self.pool(x); idxs.append(i)\n",
    "        x = self.enc2(x); sizes.append(x.size()); x, i = self.pool(x); idxs.append(i)\n",
    "        x = self.enc3(x); sizes.append(x.size()); x, i = self.pool(x); idxs.append(i)\n",
    "        x = self.enc4(x); sizes.append(x.size()); x, i = self.pool(x); idxs.append(i)\n",
    "        x = self.enc5(x); sizes.append(x.size()); x, i = self.pool(x); idxs.append(i)\n",
    "\n",
    "        x = self.unpool(x, idxs.pop(), output_size=sizes.pop()); x = self.dec5(x)\n",
    "        x = self.unpool(x, idxs.pop(), output_size=sizes.pop()); x = self.dec4(x)\n",
    "        x = self.unpool(x, idxs.pop(), output_size=sizes.pop()); x = self.dec3(x)\n",
    "        x = self.unpool(x, idxs.pop(), output_size=sizes.pop()); x = self.dec2(x)\n",
    "        x = self.unpool(x, idxs.pop(), output_size=sizes.pop()); x = self.dec1(x)\n",
    "\n",
    "        return self.head(x)\n",
    "\n",
    "# IoU validation\n",
    "@torch.no_grad()\n",
    "def mean_iou_batch(logits, target, num_classes: int, ignore_index: int = -1):\n",
    "    # logits: [B,C,H,W], target: [B,H,W]\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "\n",
    "    ious = []\n",
    "    for b in range(pred.shape[0]):\n",
    "        p = pred[b].reshape(-1)\n",
    "        t = target[b].reshape(-1)\n",
    "        valid = (t != ignore_index)\n",
    "        p = p[valid]\n",
    "        t = t[valid]\n",
    "\n",
    "        per_class = []\n",
    "        for c in range(1, num_classes):  # skip background 0\n",
    "            inter = torch.sum((p == c) & (t == c)).item()\n",
    "            union = torch.sum((p == c) | (t == c)).item()\n",
    "            if union > 0:\n",
    "                per_class.append(inter / union)\n",
    "\n",
    "        ious.append(1.0 if len(per_class) == 0 else sum(per_class) / len(per_class))\n",
    "\n",
    "    return float(sum(ious) / max(1, len(ious)))\n",
    "\n",
    "# -----------------------\n",
    "# train + predict + submit\n",
    "# -----------------------\n",
    "@torch.no_grad()\n",
    "def make_submission(model, loader, device, out_csv: Path, size_labels=272):\n",
    "    if len(loader.dataset) == 0:\n",
    "        raise RuntimeError(\"Test dataset is empty. Check X_TEST_DIR path or use rglob for nested files.\")\n",
    "    \n",
    "    model.eval()\n",
    "    rows = {}\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[\"img\"].to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        pred = torch.argmax(logits, dim=1).cpu()   # [B,160,160]\n",
    "\n",
    "        for k, pid in enumerate(batch[\"id\"]):\n",
    "            H0, W0 = batch[\"orig_hw\"][k]\n",
    "\n",
    "            # resize back to original (160x160 or 160x272)\n",
    "            mk = resize_mask(pred[k].unsqueeze(0), (H0, W0)).squeeze(0).numpy()\n",
    "\n",
    "            # csv must be 160*272 with -1 padding for shorter widths :contentReference[oaicite:3]{index=3}\n",
    "            if W0 != size_labels:\n",
    "                aux = (-1 + np.zeros(160 * size_labels, dtype=np.int64))\n",
    "                aux[: 160 * W0] = mk.reshape(-1)\n",
    "                rows[pid] = aux\n",
    "            else:\n",
    "                rows[pid] = mk.reshape(-1).astype(np.int64)\n",
    "\n",
    "    pd.DataFrame(rows, dtype=\"int\").T.to_csv(out_csv)\n",
    "    print(\"saved:\", out_csv)\n",
    "\n",
    "def main():\n",
    "    ROOT = Path(\"data\")\n",
    "    X_TRAIN_DIR = ROOT / \"X_train_uDRk9z9\"\n",
    "    X_TEST_DIR  = ROOT / \"X_test_xNbnvIa\"\n",
    "    Y_TRAIN_CSV = ROOT / \"Y_train_T9NrBYo.csv\"\n",
    "\n",
    "    train_size = (160,272)\n",
    "    batch_size = 16\n",
    "    epochs = 10\n",
    "    lr = 1e-4\n",
    "    collate_train = partial(collate, size_hw=train_size, is_train=True)\n",
    "    collate_test  = partial(collate, size_hw=train_size, is_train=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = True  # speed up on fixed input size\n",
    "\n",
    "    y_df = pd.read_csv(Y_TRAIN_CSV, index_col=0)\n",
    "    num_classes = infer_num_classes(y_df)\n",
    "\n",
    "    # ---- split by well ----\n",
    "    val_wells = []  # list of well IDs used for validation\n",
    "    tr_ids, va_ids = split_ids_by_well(y_df.index.astype(str), val_wells)\n",
    "\n",
    "    ds_tr = WellDataset(X_TRAIN_DIR, y_df=y_df, include_ids=tr_ids)\n",
    "    ds_va = WellDataset(X_TRAIN_DIR, y_df=y_df, include_ids=va_ids)\n",
    "    ds_te = WellDataset(X_TEST_DIR, y_df=None)\n",
    "\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True,\n",
    "                    num_workers=2, pin_memory=torch.cuda.is_available(),\n",
    "                    collate_fn=collate_train)\n",
    "\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False,\n",
    "                    num_workers=2, pin_memory=torch.cuda.is_available(),\n",
    "                    collate_fn=collate_train)  # use the same collate function for validation\n",
    "    \n",
    "    dl_te = DataLoader(ds_te, batch_size=batch_size, shuffle=False,\n",
    "                    num_workers=2, pin_memory=torch.cuda.is_available(),\n",
    "                    collate_fn=collate_test)\n",
    "\n",
    "    # print(\"len(ds_te) =\", len(ds_te))\n",
    "    # print(\"len(dl_te) =\", len(dl_te))\n",
    "\n",
    "    model = SegNetVGG(in_channels=1, num_classes=num_classes).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    ce = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "    # ---- load SSL encoder weights if available ----\n",
    "    ssl_ckpt = Path(\"model/segnet_ssl_dae.pth\")\n",
    "    ckpt = torch.load(ssl_ckpt, map_location=device)\n",
    "    sd = ckpt[\"model\"]\n",
    "    own = model.state_dict()\n",
    "    for k in sd:\n",
    "        if k.startswith(\"enc\") and k in own and own[k].shape == sd[k].shape:\n",
    "            own[k].copy_(sd[k])\n",
    "    model.load_state_dict(own, strict=True)\n",
    "    print(\"Loaded SSL encoder weights from\", ssl_ckpt)\n",
    "\n",
    "    best = -1.0\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running, n = 0.0, 0\n",
    "        for batch in dl_tr:\n",
    "            x = batch[\"img\"].to(device, non_blocking=True)\n",
    "            y = batch[\"mask\"].to(device, non_blocking=True)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = model(x)\n",
    "            loss = ce(logits, y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            running += float(loss.item())\n",
    "            n += 1\n",
    "\n",
    "        # ---- val ----\n",
    "        model.eval()\n",
    "        miou_sum, m = 0.0, 0\n",
    "        for batch in dl_va:\n",
    "            x = batch[\"img\"].to(device, non_blocking=True)\n",
    "            y = batch[\"mask\"].to(device, non_blocking=True)\n",
    "            logits = model(x)\n",
    "            miou_sum += mean_iou_batch(logits, y, num_classes=num_classes, ignore_index=-1)\n",
    "            m += 1\n",
    "        val_miou = miou_sum / max(1, m)\n",
    "\n",
    "        print(f\"epoch {ep:03d}/{epochs} loss={running/max(1,n):.4f}  val_mIoU={val_miou:.4f}\")\n",
    "\n",
    "        if val_miou > best:\n",
    "            best = val_miou\n",
    "            torch.save({\"model\": model.state_dict(), \"num_classes\": num_classes}, \"model/segnet_vgg_best.pth\")\n",
    "            print(\"  saved best:\", best)\n",
    "\n",
    "    # 1) Use the last epoch model\n",
    "    make_submission(model, dl_te, device, Path(\"model/submission_last.csv\"), size_labels=272)\n",
    "\n",
    "    # 2) Use the best checkpoint\n",
    "    ckpt = torch.load(\"model/segnet_vgg_best.pth\", map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"])\n",
    "    make_submission(model, dl_te, device, Path(\"model/submission_best.csv\"), size_labels=272)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import torch.multiprocessing as mp\n",
    "    mp.freeze_support()\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853daaae",
   "metadata": {},
   "source": [
    "Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3866c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from functools import partial\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from segnet import SegNetVGG, patch_minmax, collate\n",
    "\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    \"\"\"Loads *.npy from unlabeled_data (or any folder) and returns dict compatible with collate().\"\"\"\n",
    "    def __init__(self, x_dir: Path):\n",
    "        self.x_dir = Path(x_dir)\n",
    "        self.files = sorted(self.x_dir.rglob(\"*.npy\"))\n",
    "        if len(self.files) == 0:\n",
    "            raise RuntimeError(f\"No .npy found under: {self.x_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        p = self.files[i]\n",
    "        stem = p.stem\n",
    "        img = np.load(p)\n",
    "\n",
    "        if img.ndim == 3 and img.shape[-1] == 1:\n",
    "            img = img[..., 0]\n",
    "\n",
    "        img = patch_minmax(img)  # NaN->0 + minmax to [0,1]\n",
    "        H, W = img.shape\n",
    "        return {\n",
    "            \"id\": stem,\n",
    "            \"orig_hw\": (H, W),\n",
    "            \"img\": torch.from_numpy(img).unsqueeze(0),  # [1,H,W]\n",
    "        }\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def random_mask_like(x: torch.Tensor, mask_ratio: float, block: int = 8):\n",
    "    \"\"\"\n",
    "    x: [B,1,H,W] in [0,1]\n",
    "    Return x_masked, mask (1 means masked).\n",
    "    Block masking: create mask on a coarse grid then upsample.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    assert C == 1\n",
    "\n",
    "    gh = math.ceil(H / block)\n",
    "    gw = math.ceil(W / block)\n",
    "\n",
    "    # coarse mask\n",
    "    m = (torch.rand(B, 1, gh, gw, device=x.device) < mask_ratio).float()\n",
    "    # upsample to full res\n",
    "    m = torch.nn.functional.interpolate(m, size=(H, W), mode=\"nearest\")\n",
    "    x_masked = x * (1.0 - m)  # masked region -> 0\n",
    "    return x_masked, m\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def load_encoder_weights_from_ssl(seg_model: nn.Module, ssl_ckpt_path: Path, device):\n",
    "    \"\"\"Load only enc1~enc5 from ssl checkpoint into segmentation model.\"\"\"\n",
    "    ckpt = torch.load(ssl_ckpt_path, map_location=device)\n",
    "    sd = ckpt[\"model\"]\n",
    "\n",
    "    own = seg_model.state_dict()\n",
    "    for k in list(sd.keys()):\n",
    "        if k.startswith(\"enc\"):\n",
    "            if k in own and own[k].shape == sd[k].shape:\n",
    "                own[k].copy_(sd[k])\n",
    "    seg_model.load_state_dict(own, strict=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--x_unlabeled_dir\", type=str, default=\"data/X_unlabeled_mtkxUlo\")\n",
    "    ap.add_argument(\"--out_ckpt\", type=str, default=\"model/segnet_ssl_dae.pth\")\n",
    "    ap.add_argument(\"--size_h\", type=int, default=160)\n",
    "    ap.add_argument(\"--size_w\", type=int, default=272)\n",
    "    ap.add_argument(\"--epochs\", type=int, default=30)\n",
    "    ap.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    ap.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "    ap.add_argument(\"--mask_ratio\", type=float, default=0.4)\n",
    "    ap.add_argument(\"--mask_block\", type=int, default=8)\n",
    "    ap.add_argument(\"--num_workers\", type=int, default=2)\n",
    "    ap.add_argument(\"--seed\", type=int, default=42)\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    set_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    ds = UnlabeledDataset(Path(args.x_unlabeled_dir))\n",
    "    collate_u = partial(collate, size_hw=(args.size_h, args.size_w), is_train=False)\n",
    "\n",
    "    dl = DataLoader(\n",
    "        ds,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=args.num_workers,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        collate_fn=collate_u,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    # Autoencoder: reuse SegNetVGG, but output 1 channel for reconstruction\n",
    "    model = SegNetVGG(in_channels=1, num_classes=1).to(device)  # <-- 1 channel output\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    mse = nn.MSELoss()\n",
    "\n",
    "    best = float(\"inf\")\n",
    "    out_path = Path(args.out_ckpt)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for ep in range(1, args.epochs + 1):\n",
    "        model.train()\n",
    "        run, n = 0.0, 0\n",
    "\n",
    "        for batch in dl:\n",
    "            x = batch[\"img\"].to(device, non_blocking=True)  # [B,1,160,272] in [0,1]\n",
    "            x_masked, m = random_mask_like(x, args.mask_ratio, args.mask_block)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            pred = model(x_masked)           # [B,1,H,W]\n",
    "            pred = torch.sigmoid(pred)       # keep in [0,1]\n",
    "            loss = mse(pred, x)              # reconstruct full image\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            run += float(loss.item())\n",
    "            n += 1\n",
    "\n",
    "        avg = run / max(1, n)\n",
    "        print(f\"[SSL] epoch {ep:03d}/{args.epochs} mse={avg:.6f}\")\n",
    "\n",
    "        if avg < best:\n",
    "            best = avg\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"ssl_type\": \"dae\",\n",
    "                    \"size_hw\": (args.size_h, args.size_w),\n",
    "                    \"mask_ratio\": args.mask_ratio,\n",
    "                    \"mask_block\": args.mask_block,\n",
    "                },\n",
    "                out_path,\n",
    "            )\n",
    "            print(\"  saved best ssl:\", best, \"->\", out_path)\n",
    "\n",
    "    print(\"done, best ssl mse:\", best)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2876151",
   "metadata": {},
   "source": [
    "## Mask2Former"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46db57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\deep-torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Train dir: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\X_train_uDRk9z9\\images\n",
      "Test dir:  C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\X_test_xNbnvIa\\images\n",
      "Model input size: 224x224\n",
      "Pretrained: facebook/mask2former-swin-tiny-ade-semantic\n",
      "Train samples: 2790 | Val samples: 1620 | val_wells={6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "c:\\Users\\lenovo\\anaconda3\\envs\\deep-torch\\lib\\site-packages\\transformers\\image_processing_base.py:417: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size', 'reduce_labels'\n",
      "  image_processor = cls(**image_processor_dict)\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-tiny-ade-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([4, 256]) in the model instantiated\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/20 | train_loss=17.3520 | val_loss=15.0105\n",
      "  -> Best model saved: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_mask2former.pth\n",
      "Epoch 02/20 | train_loss=13.7775 | val_loss=15.2349\n",
      "Epoch 03/20 | train_loss=12.6401 | val_loss=14.5714\n",
      "  -> Best model saved: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_mask2former.pth\n",
      "Epoch 04/20 | train_loss=12.0788 | val_loss=14.2691\n",
      "  -> Best model saved: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_mask2former.pth\n",
      "Epoch 05/20 | train_loss=11.6676 | val_loss=15.0381\n",
      "Epoch 06/20 | train_loss=11.4770 | val_loss=14.3574\n",
      "Epoch 07/20 | train_loss=10.9957 | val_loss=14.1694\n",
      "  -> Best model saved: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_mask2former.pth\n",
      "Epoch 08/20 | train_loss=10.6843 | val_loss=14.9301\n",
      "Epoch 09/20 | train_loss=10.3577 | val_loss=15.2518\n",
      "Epoch 10/20 | train_loss=10.2286 | val_loss=15.0672\n",
      "Epoch 11/20 | train_loss=9.6430 | val_loss=15.5822\n",
      "Epoch 12/20 | train_loss=9.4476 | val_loss=14.6725\n",
      "Epoch 13/20 | train_loss=9.0929 | val_loss=15.5858\n",
      "Epoch 14/20 | train_loss=9.0179 | val_loss=15.7629\n",
      "Epoch 15/20 | train_loss=8.4409 | val_loss=16.1311\n",
      "Epoch 16/20 | train_loss=8.3673 | val_loss=16.3036\n",
      "Epoch 17/20 | train_loss=8.0378 | val_loss=17.1427\n",
      "Epoch 18/20 | train_loss=7.8582 | val_loss=17.3488\n",
      "Epoch 19/20 | train_loss=7.5451 | val_loss=17.6415\n",
      "Epoch 20/20 | train_loss=7.1668 | val_loss=16.5966\n",
      "[OK] submission saved to: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Mask2Former (HuggingFace Transformers) - Full runnable semantic segmentation code\n",
    "\n",
    "What this script does:\n",
    "- Reads .npy ultrasound patches\n",
    "- Trains on well1-5, validates on well6 (from X_train_uDRk9z9/images)\n",
    "- Predicts on X_test_xNbnvIa/images (well7-11)\n",
    "- Writes submission.csv with the SAME format as before:\n",
    "  - each row = one patch name\n",
    "  - flattened mask\n",
    "  - padded to 160*272 with -1\n",
    "\n",
    "Why we resize:\n",
    "- Mask2Former backbones are usually trained on larger resolutions.\n",
    "- To keep it simple and fit RTX 4060 (8GB), we resize inputs to 224x224 during training/inference,\n",
    "  then upsample predictions back to (160,272) for submission.\n",
    "\n",
    "Install (in your CUDA environment):\n",
    "    pip install transformers accelerate\n",
    "\n",
    "Notes:\n",
    "- If your machine cannot download pretrained weights (no internet), set PRETRAINED=None and it will start from scratch.\n",
    "- Mask2Former expects instance-style labels: a set of binary masks + class ids per image.\n",
    "  We convert your (H,W) semantic mask into that format automatically.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    Mask2FormerForUniversalSegmentation,\n",
    ")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0. Paths & Hyperparameters\n",
    "# =========================\n",
    "DATA_ROOT = Path(r\"C:\\Users\\lenovo\\Desktop\\deep_datachallenge\")  # change to your path\n",
    "\n",
    "TRAIN_IMAGES_DIR = DATA_ROOT / \"X_train_uDRk9z9\" / \"images\"\n",
    "TEST_IMAGES_DIR = DATA_ROOT / \"X_test_xNbnvIa\" / \"images\"\n",
    "Y_TRAIN_CSV = DATA_ROOT / \"Y_train_T9NrBYo.csv\"\n",
    "\n",
    "# Original submission size\n",
    "TARGET_H = 160\n",
    "TARGET_W = 272\n",
    "\n",
    "# Model input size (keep small for 4060)\n",
    "MODEL_H = 224\n",
    "MODEL_W = 224\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "IGNORE_INDEX = -1\n",
    "\n",
    "BATCH_SIZE = 2          # Mask2Former is heavy; start with 1~2 on 4060 8GB\n",
    "LR = 5e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 20\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# A strong semantic pretrained checkpoint (Swin-T backbone)\n",
    "# If you have no internet, set PRETRAINED = None\n",
    "PRETRAINED = \"facebook/mask2former-swin-tiny-ade-semantic\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1. Utils\n",
    "# =========================\n",
    "def parse_well_id(name: str) -> int:\n",
    "    \"\"\"Extract well id from: well_1_section_0_patch_0 -> 1\"\"\"\n",
    "    m = re.search(r\"well_(\\d+)_\", name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "\n",
    "def minmax_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Min-max normalize; replace NaN/inf with 0.\"\"\"\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    x_min = float(x.min())\n",
    "    x_max = float(x.max())\n",
    "    if x_max - x_min < 1e-6:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return (x - x_min) / (x_max - x_min)\n",
    "\n",
    "\n",
    "def pad_to_160x272(img: np.ndarray, fill_value: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"Pad (160,160) or (160,272) to (160,272).\"\"\"\n",
    "    h, w = img.shape\n",
    "    assert h == TARGET_H, f\"Expected height {TARGET_H}, got {h}\"\n",
    "    if w == TARGET_W:\n",
    "        return img\n",
    "    if w < TARGET_W:\n",
    "        out = np.full((TARGET_H, TARGET_W), fill_value, dtype=img.dtype)\n",
    "        out[:, :w] = img\n",
    "        return out\n",
    "    return img[:, :TARGET_W]\n",
    "\n",
    "\n",
    "def decode_mask_from_csv_row(row_values: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Decode one CSV row -> (160,w) semantic mask\n",
    "    - row_values: flattened mask with -1 padding\n",
    "    \"\"\"\n",
    "    valid = row_values[row_values != IGNORE_INDEX]\n",
    "    assert len(valid) % TARGET_H == 0, f\"Valid mask length {len(valid)} not divisible by 160\"\n",
    "    w = len(valid) // TARGET_H\n",
    "    return valid.reshape(TARGET_H, w).astype(np.int64)\n",
    "\n",
    "\n",
    "def pad_mask_to_160x272(mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Pad (160,w) -> (160,272) using -1 for padding.\"\"\"\n",
    "    h, w = mask.shape\n",
    "    assert h == TARGET_H\n",
    "    if w == TARGET_W:\n",
    "        return mask\n",
    "    out = np.full((TARGET_H, TARGET_W), IGNORE_INDEX, dtype=np.int64)\n",
    "    out[:, :w] = mask\n",
    "    return out\n",
    "\n",
    "\n",
    "def resize_image_torch(img_1hw: torch.Tensor, h: int, w: int) -> torch.Tensor:\n",
    "    \"\"\"Resize image tensor (1,H,W) -> (1,h,w) (bilinear).\"\"\"\n",
    "    x = img_1hw.unsqueeze(0)  # (1,1,H,W)\n",
    "    x = F.interpolate(x, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
    "    return x.squeeze(0)       # (1,h,w)\n",
    "\n",
    "\n",
    "def resize_mask_torch(mask_hw: torch.Tensor, h: int, w: int) -> torch.Tensor:\n",
    "    \"\"\"Resize mask tensor (H,W) -> (h,w) (nearest).\"\"\"\n",
    "    y = mask_hw.unsqueeze(0).unsqueeze(0).float()  # (1,1,H,W)\n",
    "    y = F.interpolate(y, size=(h, w), mode=\"nearest\")\n",
    "    return y.squeeze(0).squeeze(0).long()\n",
    "\n",
    "\n",
    "def semantic_to_mask2former_targets(\n",
    "    semantic_mask: torch.Tensor,\n",
    "    num_classes: int,\n",
    "    ignore_index: int = -1,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Convert a semantic mask (H,W) into Mask2Former targets:\n",
    "    - class_labels: (N,) long\n",
    "    - mask_labels:  (N,H,W) float (0/1)\n",
    "\n",
    "    We create one binary mask per class present in the image (excluding ignore_index).\n",
    "    \"\"\"\n",
    "    # semantic_mask: (H,W)\n",
    "    valid = semantic_mask != ignore_index\n",
    "    if valid.sum() == 0:\n",
    "        # If everything is ignore, create a dummy empty target (rare).\n",
    "        # Use background class 0 with an all-zero mask.\n",
    "        class_labels = torch.tensor([0], dtype=torch.long)\n",
    "        mask_labels = torch.zeros((1, semantic_mask.shape[0], semantic_mask.shape[1]), dtype=torch.float32)\n",
    "        return class_labels, mask_labels\n",
    "\n",
    "    present_classes = torch.unique(semantic_mask[valid]).tolist()\n",
    "    present_classes = [int(c) for c in present_classes if 0 <= int(c) < num_classes]\n",
    "\n",
    "    if len(present_classes) == 0:\n",
    "        class_labels = torch.tensor([0], dtype=torch.long)\n",
    "        mask_labels = torch.zeros((1, semantic_mask.shape[0], semantic_mask.shape[1]), dtype=torch.float32)\n",
    "        return class_labels, mask_labels\n",
    "\n",
    "    masks = []\n",
    "    classes = []\n",
    "    for c in present_classes:\n",
    "        m = (semantic_mask == c) & valid\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        masks.append(m.float())\n",
    "        classes.append(c)\n",
    "\n",
    "    if len(classes) == 0:\n",
    "        class_labels = torch.tensor([0], dtype=torch.long)\n",
    "        mask_labels = torch.zeros((1, semantic_mask.shape[0], semantic_mask.shape[1]), dtype=torch.float32)\n",
    "        return class_labels, mask_labels\n",
    "\n",
    "    class_labels = torch.tensor(classes, dtype=torch.long)\n",
    "    mask_labels = torch.stack(masks, dim=0).float()  # (N,H,W)\n",
    "    return class_labels, mask_labels\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2. Dataset\n",
    "# =========================\n",
    "class WellSegDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, y_csv_path: Path = None):\n",
    "        self.images_dir = images_dir\n",
    "        self.has_label = y_csv_path is not None\n",
    "\n",
    "        self.image_paths = sorted(images_dir.glob(\"*.npy\"))\n",
    "        self.names = [p.stem for p in self.image_paths]\n",
    "\n",
    "        self.y_df = pd.read_csv(y_csv_path, index_col=0) if self.has_label else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        name = self.names[idx]\n",
    "        img_path = self.image_paths[idx]\n",
    "\n",
    "        img = np.load(img_path)                 # (160,160) or (160,272)\n",
    "        raw_w = int(img.shape[1])               # used to crop back for submission\n",
    "        img = minmax_normalize(img)\n",
    "        img = pad_to_160x272(img, fill_value=0.0)\n",
    "\n",
    "        img_t = torch.from_numpy(img).unsqueeze(0).float()      # (1,160,272)\n",
    "        img_t = resize_image_torch(img_t, MODEL_H, MODEL_W)     # (1,224,224)\n",
    "\n",
    "        if not self.has_label:\n",
    "            return {\"name\": name, \"image\": img_t, \"raw_w\": raw_w}\n",
    "\n",
    "        row = self.y_df.loc[name].values.astype(np.int64)\n",
    "        mask = decode_mask_from_csv_row(row)                    # (160,w)\n",
    "        mask = pad_mask_to_160x272(mask)                        # (160,272)\n",
    "        mask_t = torch.from_numpy(mask).long()                  # (160,272)\n",
    "        mask_t = resize_mask_torch(mask_t, MODEL_H, MODEL_W)    # (224,224)\n",
    "\n",
    "        return {\"name\": name, \"image\": img_t, \"mask\": mask_t, \"raw_w\": raw_w}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3. Collate for Mask2Former\n",
    "# =========================\n",
    "def collate_mask2former(batch: List[Dict]) -> Dict:\n",
    "    \"\"\"\n",
    "    Build a batch dict for Mask2Former:\n",
    "    - pixel_values: (B,3,224,224) float\n",
    "    - pixel_mask:   (B,224,224)   bool/long (1=valid)\n",
    "    - mask_labels:  list of (Ni,224,224) float\n",
    "    - class_labels: list of (Ni,) long\n",
    "    \"\"\"\n",
    "    names = [b[\"name\"] for b in batch]\n",
    "    raw_ws = torch.tensor([b[\"raw_w\"] for b in batch], dtype=torch.long)\n",
    "\n",
    "    # image: (1,224,224) -> (3,224,224) by repeating channel\n",
    "    imgs_1 = torch.stack([b[\"image\"] for b in batch], dim=0)  # (B,1,224,224)\n",
    "    pixel_values = imgs_1.repeat(1, 3, 1, 1)                  # (B,3,224,224)\n",
    "\n",
    "    pixel_mask = torch.ones((pixel_values.shape[0], MODEL_H, MODEL_W), dtype=torch.long)\n",
    "\n",
    "    out = {\n",
    "        \"names\": names,\n",
    "        \"raw_ws\": raw_ws,\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"pixel_mask\": pixel_mask,\n",
    "    }\n",
    "\n",
    "    if \"mask\" in batch[0]:\n",
    "        class_labels_list = []\n",
    "        mask_labels_list = []\n",
    "        for b in batch:\n",
    "            y = b[\"mask\"]  # (224,224)\n",
    "            cls, msk = semantic_to_mask2former_targets(y, NUM_CLASSES, IGNORE_INDEX)\n",
    "            class_labels_list.append(cls)\n",
    "            mask_labels_list.append(msk)\n",
    "\n",
    "        out[\"class_labels\"] = class_labels_list\n",
    "        out[\"mask_labels\"] = mask_labels_list\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4. Model builder\n",
    "# =========================\n",
    "def build_model_and_processor(num_classes: int):\n",
    "    id2label = {0: \"class0\", 1: \"class1\", 2: \"class2\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    if PRETRAINED is None:\n",
    "        # Train from scratch\n",
    "        processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-tiny-ade-semantic\")\n",
    "        model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "            \"facebook/mask2former-swin-tiny-ade-semantic\",\n",
    "            ignore_mismatched_sizes=True,\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            num_labels=num_classes,\n",
    "            use_safetensors=True,\n",
    "        )\n",
    "    else:\n",
    "        processor = AutoImageProcessor.from_pretrained(PRETRAINED)\n",
    "        model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "            PRETRAINED,\n",
    "            ignore_mismatched_sizes=True,   # allow changing num_labels\n",
    "            id2label=id2label,\n",
    "            label2id=label2id,\n",
    "            num_labels=num_classes,\n",
    "        )\n",
    "\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5. Train / Validate\n",
    "# =========================\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)  # (B,3,224,224)\n",
    "        pixel_mask = batch[\"pixel_mask\"].to(DEVICE)      # (B,224,224)\n",
    "\n",
    "        # Mask2Former expects lists for labels (length B)\n",
    "        class_labels = [x.to(DEVICE) for x in batch[\"class_labels\"]]\n",
    "        mask_labels = [x.to(DEVICE) for x in batch[\"mask_labels\"]]\n",
    "\n",
    "        outputs = model(\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            class_labels=class_labels,\n",
    "            mask_labels=mask_labels,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(loss.item()) * pixel_values.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\n",
    "        class_labels = [x.to(DEVICE) for x in batch[\"class_labels\"]]\n",
    "        mask_labels = [x.to(DEVICE) for x in batch[\"mask_labels\"]]\n",
    "\n",
    "        outputs = model(\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            class_labels=class_labels,\n",
    "            mask_labels=mask_labels,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        total_loss += float(loss.item()) * pixel_values.size(0)\n",
    "\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6. Inference & submission\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def predict_and_make_submission(model, processor, test_images_dir: Path, out_csv_path: Path):\n",
    "    \"\"\"\n",
    "    Predict all test patches and write submission.csv.\n",
    "    Steps:\n",
    "    - model predicts at 224x224\n",
    "    - we use processor.post_process_semantic_segmentation to get semantic map\n",
    "    - upsample semantic map to (160,272)\n",
    "    - crop to raw width and pad to 160*272 with -1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    test_ds = WellSegDataset(test_images_dir, y_csv_path=None)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_mask2former,\n",
    "    )\n",
    "\n",
    "    preds_dict = {}\n",
    "\n",
    "    for batch in test_loader:\n",
    "        name = batch[\"names\"][0]\n",
    "        raw_w = int(batch[\"raw_ws\"][0].item())\n",
    "\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)  # (1,3,224,224)\n",
    "        pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "        # Post-process semantic segmentation\n",
    "        target_sizes = [(MODEL_H, MODEL_W)]\n",
    "        seg_list = processor.post_process_semantic_segmentation(outputs, target_sizes=target_sizes)\n",
    "        seg_224 = seg_list[0].to(torch.int64)  # (224,224)\n",
    "\n",
    "        # Upsample to (160,272) using nearest\n",
    "        seg_224 = seg_224.unsqueeze(0).unsqueeze(0).float()  # (1,1,224,224)\n",
    "        seg_160_272 = F.interpolate(seg_224, size=(TARGET_H, TARGET_W), mode=\"nearest\").squeeze(0).squeeze(0)\n",
    "        seg_160_272 = seg_160_272.cpu().numpy().astype(np.int64)  # (160,272)\n",
    "\n",
    "        # Crop back to original width\n",
    "        pred = seg_160_272[:, :raw_w]\n",
    "\n",
    "        if raw_w < TARGET_W:\n",
    "            padded = np.full((TARGET_H * TARGET_W,), IGNORE_INDEX, dtype=np.int64)\n",
    "            padded[: TARGET_H * raw_w] = pred.flatten()\n",
    "            preds_dict[name] = padded\n",
    "        else:\n",
    "            preds_dict[name] = pred.flatten()\n",
    "\n",
    "    sub = pd.DataFrame(preds_dict, dtype=\"int64\").T\n",
    "    sub.to_csv(out_csv_path)\n",
    "    print(f\"[OK] submission saved to: {out_csv_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7. Main\n",
    "# =========================\n",
    "def main():\n",
    "    print(f\"DEVICE: {DEVICE}\")\n",
    "    print(f\"Train dir: {TRAIN_IMAGES_DIR}\")\n",
    "    print(f\"Test dir:  {TEST_IMAGES_DIR}\")\n",
    "    print(f\"Model input size: {MODEL_H}x{MODEL_W}\")\n",
    "    print(f\"Pretrained: {PRETRAINED}\")\n",
    "\n",
    "    # Load all train data (well1-6)\n",
    "    train_ds_all = WellSegDataset(TRAIN_IMAGES_DIR, Y_TRAIN_CSV)\n",
    "\n",
    "    # Split by well: well6 as validation\n",
    "    VAL_WELLS = {6}\n",
    "    train_indices, val_indices = [], []\n",
    "    for i, name in enumerate(train_ds_all.names):\n",
    "        w = parse_well_id(name)\n",
    "        if w in VAL_WELLS:\n",
    "            val_indices.append(i)\n",
    "        else:\n",
    "            train_indices.append(i)\n",
    "\n",
    "    train_ds = Subset(train_ds_all, train_indices)  # well1-5\n",
    "    val_ds = Subset(train_ds_all, val_indices)      # well6\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_mask2former,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_mask2former,\n",
    "    )\n",
    "\n",
    "    print(f\"Train samples: {len(train_ds)} | Val samples: {len(val_ds)} | val_wells={VAL_WELLS}\")\n",
    "\n",
    "    model, processor = build_model_and_processor(NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val = 1e9\n",
    "    best_path = DATA_ROOT / \"best_mask2former.pth\"\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "        va_loss = valid_one_epoch(model, val_loader)\n",
    "        print(f\"Epoch {epoch:02d}/{EPOCHS} | train_loss={tr_loss:.4f} | val_loss={va_loss:.4f}\")\n",
    "\n",
    "        if va_loss < best_val:\n",
    "            best_val = va_loss\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  -> Best model saved: {best_path}\")\n",
    "\n",
    "    # Predict test and write submission\n",
    "    out_csv = DATA_ROOT / \"submission.csv\"\n",
    "    state_dict = torch.load(best_path, map_location=DEVICE, weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    predict_and_make_submission(model, processor, TEST_IMAGES_DIR, out_csv)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505db77a",
   "metadata": {},
   "source": [
    "## Mask2former(semi-supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5cfe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Labeled train dir: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\X_train_uDRk9z9\\images\n",
      "Unlabeled dir:     C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\X_unlabeled_mtkxUlo\\images\n",
      "Test dir:          C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\X_test_xNbnvIa\\images\n",
      "Pretrained:        facebook/mask2former-swin-tiny-ade-semantic\n",
      "Pseudo TH=0.85, lambda_u=0.5\n",
      "Labeled train: 2790 | Val: 1620 | Unlabeled: 1980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\anaconda3\\envs\\deep-torch\\lib\\site-packages\\transformers\\image_processing_base.py:417: UserWarning: The following named arguments are not valid for `Mask2FormerImageProcessor.__init__` and were ignored: '_max_size', 'reduce_labels'\n",
      "  image_processor = cls(**image_processor_dict)\n",
      "Some weights of Mask2FormerForUniversalSegmentation were not initialized from the model checkpoint at facebook/mask2former-swin-tiny-ade-semantic and are newly initialized because the shapes did not match:\n",
      "- class_predictor.weight: found shape torch.Size([151, 256]) in the checkpoint and torch.Size([4, 256]) in the model instantiated\n",
      "- class_predictor.bias: found shape torch.Size([151]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([151]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01/10 | train_l=18.6849 | train_u=41.4849 | val=15.4774\n",
      "  -> Best saved: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_mask2former_semi.pth\n",
      "Epoch 02/10 | train_l=14.1083 | train_u=30.6973 | val=14.3693\n",
      "  -> Best saved: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\best_mask2former_semi.pth\n",
      "Epoch 03/10 | train_l=13.2919 | train_u=31.3983 | val=14.7217\n",
      "Epoch 04/10 | train_l=12.4842 | train_u=23.7818 | val=15.1125\n",
      "Epoch 05/10 | train_l=11.9995 | train_u=22.2180 | val=15.1217\n",
      "Epoch 06/10 | train_l=11.6591 | train_u=20.8557 | val=14.9213\n",
      "Epoch 07/10 | train_l=11.2987 | train_u=18.4966 | val=15.2570\n",
      "Epoch 08/10 | train_l=10.9792 | train_u=17.3398 | val=14.8278\n",
      "Epoch 09/10 | train_l=10.6290 | train_u=17.2760 | val=14.8505\n",
      "Epoch 10/10 | train_l=10.3889 | train_u=17.5382 | val=14.5758\n",
      "[OK] submission saved to: C:\\Users\\lenovo\\Desktop\\deep_datachallenge\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Mask2Former + Semi-Supervised Pseudo Label (Full runnable)0.6663556049927326\n",
    "\n",
    "Data:\n",
    "- Labeled train images:   X_train_uDRk9z9/images (well1-6)\n",
    "- Labeled train labels:   Y_train_T9NrBYo.csv (flatten + -1 padding)\n",
    "- Unlabeled images:       X_unlabeled_mtkxUlo/images (well12-14)\n",
    "- Test images:            X_test_xNbnvIa/images (well7-11)\n",
    "\n",
    "Split:\n",
    "- Train labeled: well1-5\n",
    "- Val labeled:   well6\n",
    "- Unlabeled:     well12-14 (no labels)\n",
    "\n",
    "Output:\n",
    "- submission.csv, each row = one patch\n",
    "- flattened mask, padded to 160*272 with -1\n",
    "\n",
    "Install:\n",
    "    pip install transformers accelerate\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    Mask2FormerForUniversalSegmentation,\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# 0) Paths & Hyperparameters\n",
    "# =========================\n",
    "DATA_ROOT = Path(r\"C:\\Users\\lenovo\\Desktop\\deep_datachallenge\")\n",
    "\n",
    "TRAIN_IMAGES_DIR = DATA_ROOT / \"X_train_uDRk9z9\" / \"images\"\n",
    "TEST_IMAGES_DIR  = DATA_ROOT / \"X_test_xNbnvIa\" / \"images\"\n",
    "UNLABELED_DIR     = DATA_ROOT / \"X_unlabeled_mtkxUlo\" / \"images\"\n",
    "Y_TRAIN_CSV       = DATA_ROOT / \"Y_train_T9NrBYo.csv\"\n",
    "\n",
    "# submission size\n",
    "TARGET_H = 160\n",
    "TARGET_W = 272\n",
    "\n",
    "# model size\n",
    "MODEL_H = 224\n",
    "MODEL_W = 224\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "IGNORE_INDEX = -1\n",
    "\n",
    "BATCH_SIZE_L = 2         # labeled batch\n",
    "BATCH_SIZE_U = 2         # unlabeled batch\n",
    "LR = 5e-5\n",
    "WEIGHT_DECAY = 1e-4\n",
    "EPOCHS = 10\n",
    "\n",
    "# semi-supervised hyperparams\n",
    "PSEUDO_TH = 0.85         # ()\n",
    "LAMBDA_U = 0.5           # loss(0.2~1.0)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "PRETRAINED = \"facebook/mask2former-swin-tiny-ade-semantic\"\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Utils\n",
    "# =========================\n",
    "def parse_well_id(name: str) -> int:\n",
    "    \"\"\"well_12_section_0_patch_0 -> 12\"\"\"\n",
    "    m = re.search(r\"well_(\\d+)_\", name)\n",
    "    return int(m.group(1)) if m else -1\n",
    "\n",
    "\n",
    "def minmax_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    x = x.astype(np.float32)\n",
    "    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    mn = float(x.min())\n",
    "    mx = float(x.max())\n",
    "    if mx - mn < 1e-6:\n",
    "        return np.zeros_like(x, dtype=np.float32)\n",
    "    return (x - mn) / (mx - mn)\n",
    "\n",
    "\n",
    "def pad_to_160x272(img: np.ndarray, fill_value: float = 0.0) -> np.ndarray:\n",
    "    h, w = img.shape\n",
    "    assert h == TARGET_H, f\"Expected height {TARGET_H}, got {h}\"\n",
    "    if w == TARGET_W:\n",
    "        return img\n",
    "    if w < TARGET_W:\n",
    "        out = np.full((TARGET_H, TARGET_W), fill_value, dtype=img.dtype)\n",
    "        out[:, :w] = img\n",
    "        return out\n",
    "    return img[:, :TARGET_W]\n",
    "\n",
    "\n",
    "def decode_mask_from_csv_row(row_values: np.ndarray) -> np.ndarray:\n",
    "    valid = row_values[row_values != IGNORE_INDEX]\n",
    "    assert len(valid) % TARGET_H == 0, f\"Valid mask length {len(valid)} not divisible by 160\"\n",
    "    w = len(valid) // TARGET_H\n",
    "    return valid.reshape(TARGET_H, w).astype(np.int64)\n",
    "\n",
    "\n",
    "def pad_mask_to_160x272(mask: np.ndarray) -> np.ndarray:\n",
    "    h, w = mask.shape\n",
    "    assert h == TARGET_H\n",
    "    if w == TARGET_W:\n",
    "        return mask\n",
    "    out = np.full((TARGET_H, TARGET_W), IGNORE_INDEX, dtype=np.int64)\n",
    "    out[:, :w] = mask\n",
    "    return out\n",
    "\n",
    "\n",
    "def resize_image_torch(img_1hw: torch.Tensor, h: int, w: int) -> torch.Tensor:\n",
    "    \"\"\"(1,H,W)->(1,h,w) bilinear\"\"\"\n",
    "    x = img_1hw.unsqueeze(0)  # (1,1,H,W)\n",
    "    x = F.interpolate(x, size=(h, w), mode=\"bilinear\", align_corners=False)\n",
    "    return x.squeeze(0)\n",
    "\n",
    "\n",
    "def resize_mask_torch(mask_hw: torch.Tensor, h: int, w: int) -> torch.Tensor:\n",
    "    \"\"\"(H,W)->(h,w) nearest\"\"\"\n",
    "    y = mask_hw.unsqueeze(0).unsqueeze(0).float()\n",
    "    y = F.interpolate(y, size=(h, w), mode=\"nearest\")\n",
    "    return y.squeeze(0).squeeze(0).long()\n",
    "\n",
    "\n",
    "def semantic_to_mask2former_targets(\n",
    "    semantic_mask: torch.Tensor,\n",
    "    num_classes: int,\n",
    "    ignore_index: int = -1,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    semantic_mask: (H,W) with ignore_index\n",
    "    return:\n",
    "      class_labels: (N,)\n",
    "      mask_labels:  (N,H,W) float(0/1)\n",
    "    \"\"\"\n",
    "    valid = semantic_mask != ignore_index\n",
    "    if valid.sum() == 0:\n",
    "        class_labels = torch.tensor([0], dtype=torch.long)\n",
    "        mask_labels = torch.zeros((1, semantic_mask.shape[0], semantic_mask.shape[1]), dtype=torch.float32)\n",
    "        return class_labels, mask_labels\n",
    "\n",
    "    present = torch.unique(semantic_mask[valid]).tolist()\n",
    "    present = [int(c) for c in present if 0 <= int(c) < num_classes]\n",
    "    if len(present) == 0:\n",
    "        class_labels = torch.tensor([0], dtype=torch.long)\n",
    "        mask_labels = torch.zeros((1, semantic_mask.shape[0], semantic_mask.shape[1]), dtype=torch.float32)\n",
    "        return class_labels, mask_labels\n",
    "\n",
    "    masks, classes = [], []\n",
    "    for c in present:\n",
    "        m = (semantic_mask == c) & valid\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        masks.append(m.float())\n",
    "        classes.append(c)\n",
    "\n",
    "    if len(classes) == 0:\n",
    "        class_labels = torch.tensor([0], dtype=torch.long)\n",
    "        mask_labels = torch.zeros((1, semantic_mask.shape[0], semantic_mask.shape[1]), dtype=torch.float32)\n",
    "        return class_labels, mask_labels\n",
    "\n",
    "    class_labels = torch.tensor(classes, dtype=torch.long)\n",
    "    mask_labels = torch.stack(masks, dim=0).float()\n",
    "    return class_labels, mask_labels\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) \n",
    "# =========================\n",
    "def aug_weak(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" + \"\"\"\n",
    "    # x: (1,224,224)\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        x = torch.flip(x, dims=[2])\n",
    "    noise = 0.02 * torch.randn_like(x)\n",
    "    return torch.clamp(x + noise, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def aug_strong(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\" +  + \"\"\"\n",
    "    if torch.rand(1).item() < 0.5:\n",
    "        x = torch.flip(x, dims=[2])\n",
    "    # brightness/contrast\n",
    "    contrast = 0.8 + 0.4 * torch.rand(1).item()   # [0.8,1.2]\n",
    "    brightness = -0.1 + 0.2 * torch.rand(1).item() # [-0.1,0.1]\n",
    "    x = x * contrast + brightness\n",
    "    # noise\n",
    "    noise = 0.05 * torch.randn_like(x)\n",
    "    x = x + noise\n",
    "    return torch.clamp(x, 0.0, 1.0)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Dataset\n",
    "# =========================\n",
    "class LabeledDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, y_csv_path: Path):\n",
    "        self.image_paths = sorted(images_dir.glob(\"*.npy\"))\n",
    "        self.names = [p.stem for p in self.image_paths]\n",
    "        self.y_df = pd.read_csv(y_csv_path, index_col=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        name = self.names[idx]\n",
    "        img = np.load(self.image_paths[idx])\n",
    "        raw_w = int(img.shape[1])\n",
    "\n",
    "        img = minmax_normalize(img)\n",
    "        img = pad_to_160x272(img, fill_value=0.0)\n",
    "        img_t = torch.from_numpy(img).unsqueeze(0).float()         # (1,160,272)\n",
    "        img_t = resize_image_torch(img_t, MODEL_H, MODEL_W)        # (1,224,224)\n",
    "\n",
    "        row = self.y_df.loc[name].values.astype(np.int64)\n",
    "        mask = decode_mask_from_csv_row(row)                       # (160,w)\n",
    "        mask = pad_mask_to_160x272(mask)                           # (160,272)\n",
    "        mask_t = torch.from_numpy(mask).long()                     # (160,272)\n",
    "        mask_t = resize_mask_torch(mask_t, MODEL_H, MODEL_W)        # (224,224)\n",
    "\n",
    "        return {\"name\": name, \"image\": img_t, \"mask\": mask_t, \"raw_w\": raw_w}\n",
    "\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path):\n",
    "        self.image_paths = sorted(images_dir.glob(\"*.npy\"))\n",
    "        self.names = [p.stem for p in self.image_paths]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        name = self.names[idx]\n",
    "        img = np.load(self.image_paths[idx])\n",
    "        raw_w = int(img.shape[1])\n",
    "\n",
    "        img = minmax_normalize(img)\n",
    "        img = pad_to_160x272(img, fill_value=0.0)\n",
    "        img_t = torch.from_numpy(img).unsqueeze(0).float()         # (1,160,272)\n",
    "        img_t = resize_image_torch(img_t, MODEL_H, MODEL_W)         # (1,224,224)\n",
    "\n",
    "        #  base  collate \n",
    "        return {\"name\": name, \"image\": img_t, \"raw_w\": raw_w}\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) Collate\n",
    "# =========================\n",
    "def collate_labeled(batch: List[Dict]) -> Dict:\n",
    "    names = [b[\"name\"] for b in batch]\n",
    "    raw_ws = torch.tensor([b[\"raw_w\"] for b in batch], dtype=torch.long)\n",
    "\n",
    "    imgs_1 = torch.stack([b[\"image\"] for b in batch], dim=0)       # (B,1,224,224)\n",
    "    pixel_values = imgs_1.repeat(1, 3, 1, 1)                       # (B,3,224,224)\n",
    "    pixel_mask = torch.ones((pixel_values.shape[0], MODEL_H, MODEL_W), dtype=torch.long)\n",
    "\n",
    "    class_labels_list, mask_labels_list = [], []\n",
    "    for b in batch:\n",
    "        y = b[\"mask\"]  # (224,224)\n",
    "        cls, msk = semantic_to_mask2former_targets(y, NUM_CLASSES, IGNORE_INDEX)\n",
    "        class_labels_list.append(cls)\n",
    "        mask_labels_list.append(msk)\n",
    "\n",
    "    return {\n",
    "        \"names\": names,\n",
    "        \"raw_ws\": raw_ws,\n",
    "        \"pixel_values\": pixel_values,\n",
    "        \"pixel_mask\": pixel_mask,\n",
    "        \"class_labels\": class_labels_list,\n",
    "        \"mask_labels\": mask_labels_list,\n",
    "    }\n",
    "\n",
    "\n",
    "def collate_unlabeled(batch: List[Dict]) -> Dict:\n",
    "    names = [b[\"name\"] for b in batch]\n",
    "\n",
    "    imgs = [b[\"image\"] for b in batch]  # list of (1,224,224)\n",
    "\n",
    "    # weak / strong augmentation\n",
    "    imgs_w = torch.stack([aug_weak(x.clone()) for x in imgs], dim=0)    # (B,1,224,224)\n",
    "    imgs_s = torch.stack([aug_strong(x.clone()) for x in imgs], dim=0)  # (B,1,224,224)\n",
    "\n",
    "    pixel_values_w = imgs_w.repeat(1, 3, 1, 1)  # (B,3,224,224)\n",
    "    pixel_values_s = imgs_s.repeat(1, 3, 1, 1)\n",
    "\n",
    "    pixel_mask = torch.ones((pixel_values_w.shape[0], MODEL_H, MODEL_W), dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"names\": names,\n",
    "        \"pixel_values_w\": pixel_values_w,\n",
    "        \"pixel_values_s\": pixel_values_s,\n",
    "        \"pixel_mask\": pixel_mask,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Model builder\n",
    "# =========================\n",
    "def build_model(num_classes: int):\n",
    "    id2label = {0: \"class0\", 1: \"class1\", 2: \"class2\"}\n",
    "    label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "    processor = AutoImageProcessor.from_pretrained(PRETRAINED)\n",
    "    model = Mask2FormerForUniversalSegmentation.from_pretrained(\n",
    "        PRETRAINED,\n",
    "        ignore_mismatched_sizes=True,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        num_labels=num_classes,\n",
    "    )\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 6) Pseudo label from Mask2Former outputs\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def pseudo_from_outputs(outputs, num_classes: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "     Mask2Former \n",
    "    class_probs (softmax) * mask_probs (sigmoid) -> per-pixel scores\n",
    "\n",
    "    outputs.class_queries_logits: (B, Q, C+1)  ( no-object)\n",
    "    outputs.masks_queries_logits: (B, Q, H, W)\n",
    "\n",
    "    return:\n",
    "      pseudo: (B,H,W) long  (0..C-1)\n",
    "      conf:   (B,H,W) float (max score)\n",
    "    \"\"\"\n",
    "    class_logits = outputs.class_queries_logits  # (B,Q,C+1)\n",
    "    mask_logits = outputs.masks_queries_logits   # (B,Q,H,W)\n",
    "\n",
    "    class_prob = class_logits.softmax(dim=-1)[..., :num_classes]   # (B,Q,C)\n",
    "    mask_prob = mask_logits.sigmoid()                              # (B,Q,H,W)\n",
    "\n",
    "    # (B,C,H,W)  einsum: sum_q class_prob[b,q,c] * mask_prob[b,q,h,w]\n",
    "    score = torch.einsum(\"bqc,bqhw->bchw\", class_prob, mask_prob)\n",
    "    conf, pseudo = torch.max(score, dim=1)  # (B,H,W)\n",
    "    return pseudo.long(), conf.float()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 7) Train / Validate (Semi-Supervised)\n",
    "# =========================\n",
    "def train_one_epoch_semi(model, labeled_loader, unlabeled_loader, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    total_l, total_u = 0.0, 0.0\n",
    "    n_l, n_u = 0, 0\n",
    "\n",
    "    unlabeled_iter = iter(unlabeled_loader)\n",
    "\n",
    "    for batch_l in labeled_loader:\n",
    "        # ---- labeled step ----\n",
    "        pixel_values = batch_l[\"pixel_values\"].to(DEVICE)\n",
    "        pixel_mask = batch_l[\"pixel_mask\"].to(DEVICE)\n",
    "        class_labels = [x.to(DEVICE) for x in batch_l[\"class_labels\"]]\n",
    "        mask_labels = [x.to(DEVICE) for x in batch_l[\"mask_labels\"]]\n",
    "\n",
    "        out_l = model(\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            class_labels=class_labels,\n",
    "            mask_labels=mask_labels,\n",
    "        )\n",
    "        loss_l = out_l.loss\n",
    "\n",
    "        # ---- unlabeled step (pseudo-label) ----\n",
    "        try:\n",
    "            batch_u = next(unlabeled_iter)\n",
    "        except StopIteration:\n",
    "            unlabeled_iter = iter(unlabeled_loader)\n",
    "            batch_u = next(unlabeled_iter)\n",
    "\n",
    "        pv_w = batch_u[\"pixel_values_w\"].to(DEVICE)  # weak\n",
    "        pv_s = batch_u[\"pixel_values_s\"].to(DEVICE)  # strong\n",
    "        pm_u = batch_u[\"pixel_mask\"].to(DEVICE)\n",
    "\n",
    "        # teacher prediction on weak\n",
    "        model.eval()\n",
    "        out_u_teacher = model(pixel_values=pv_w, pixel_mask=pm_u)\n",
    "        pseudo, conf = pseudo_from_outputs(out_u_teacher, NUM_CLASSES)  # (B,224,224)\n",
    "\n",
    "        #  IGNORE\n",
    "        pseudo = pseudo.clone()\n",
    "        pseudo[conf < PSEUDO_TH] = IGNORE_INDEX\n",
    "\n",
    "        #  pseudo semantic mask -> mask2former targets(list)\n",
    "        class_labels_u, mask_labels_u = [], []\n",
    "        for i in range(pseudo.shape[0]):\n",
    "            cls_i, msk_i = semantic_to_mask2former_targets(pseudo[i], NUM_CLASSES, IGNORE_INDEX)\n",
    "            class_labels_u.append(cls_i.to(DEVICE))\n",
    "            mask_labels_u.append(msk_i.to(DEVICE))\n",
    "\n",
    "        model.train()\n",
    "        out_u_student = model(\n",
    "            pixel_values=pv_s,\n",
    "            pixel_mask=pm_u,\n",
    "            class_labels=class_labels_u,\n",
    "            mask_labels=mask_labels_u,\n",
    "        )\n",
    "        loss_u = out_u_student.loss\n",
    "\n",
    "        # ---- total loss ----\n",
    "        loss = loss_l + LAMBDA_U * loss_u\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_l += float(loss_l.item()) * pixel_values.size(0)\n",
    "        total_u += float(loss_u.item()) * pv_s.size(0)\n",
    "        n_l += pixel_values.size(0)\n",
    "        n_u += pv_s.size(0)\n",
    "\n",
    "    return total_l / max(1, n_l), total_u / max(1, n_u)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, loader):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\n",
    "        class_labels = [x.to(DEVICE) for x in batch[\"class_labels\"]]\n",
    "        mask_labels = [x.to(DEVICE) for x in batch[\"mask_labels\"]]\n",
    "\n",
    "        out = model(\n",
    "            pixel_values=pixel_values,\n",
    "            pixel_mask=pixel_mask,\n",
    "            class_labels=class_labels,\n",
    "            mask_labels=mask_labels,\n",
    "        )\n",
    "        total += float(out.loss.item()) * pixel_values.size(0)\n",
    "        n += pixel_values.size(0)\n",
    "\n",
    "    return total / max(1, n)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 8) Inference & submission\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def predict_and_make_submission(model, processor, test_images_dir: Path, out_csv_path: Path):\n",
    "    model.eval()\n",
    "\n",
    "    #  UnlabeledDataset \n",
    "    test_ds = UnlabeledDataset(test_images_dir)\n",
    "\n",
    "    def collate_test(batch: List[Dict]) -> Dict:\n",
    "        names = [b[\"name\"] for b in batch]\n",
    "        raw_ws = torch.tensor([b[\"raw_w\"] for b in batch], dtype=torch.long)\n",
    "        imgs_1 = torch.stack([b[\"image\"] for b in batch], dim=0)    # (B,1,224,224)\n",
    "        pixel_values = imgs_1.repeat(1, 3, 1, 1)\n",
    "        pixel_mask = torch.ones((pixel_values.shape[0], MODEL_H, MODEL_W), dtype=torch.long)\n",
    "        return {\"names\": names, \"raw_ws\": raw_ws, \"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask}\n",
    "\n",
    "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_test)\n",
    "\n",
    "    preds_dict = {}\n",
    "\n",
    "    for batch in test_loader:\n",
    "        name = batch[\"names\"][0]\n",
    "        raw_w = int(batch[\"raw_ws\"][0].item())\n",
    "\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        pixel_mask = batch[\"pixel_mask\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "        #  processor \n",
    "        seg_list = processor.post_process_semantic_segmentation(outputs, target_sizes=[(MODEL_H, MODEL_W)])\n",
    "        seg_224 = seg_list[0].to(torch.int64)  # (224,224)\n",
    "\n",
    "        seg_224 = seg_224.unsqueeze(0).unsqueeze(0).float()\n",
    "        seg_160_272 = F.interpolate(seg_224, size=(TARGET_H, TARGET_W), mode=\"nearest\").squeeze(0).squeeze(0)\n",
    "        seg_160_272 = seg_160_272.cpu().numpy().astype(np.int64)\n",
    "\n",
    "        pred = seg_160_272[:, :raw_w]\n",
    "\n",
    "        padded = np.full((TARGET_H * TARGET_W,), IGNORE_INDEX, dtype=np.int64)\n",
    "        padded[: TARGET_H * raw_w] = pred.flatten()\n",
    "        preds_dict[name] = padded\n",
    "\n",
    "    sub = pd.DataFrame(preds_dict, dtype=\"int64\").T\n",
    "    sub.to_csv(out_csv_path)\n",
    "    print(f\"[OK] submission saved to: {out_csv_path}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 9) Main\n",
    "# =========================\n",
    "def main():\n",
    "    print(f\"DEVICE: {DEVICE}\")\n",
    "    print(f\"Labeled train dir: {TRAIN_IMAGES_DIR}\")\n",
    "    print(f\"Unlabeled dir:     {UNLABELED_DIR}\")\n",
    "    print(f\"Test dir:          {TEST_IMAGES_DIR}\")\n",
    "    print(f\"Pretrained:        {PRETRAINED}\")\n",
    "    print(f\"Pseudo TH={PSEUDO_TH}, lambda_u={LAMBDA_U}\")\n",
    "\n",
    "    # ---- labeled dataset (well1-6) ----\n",
    "    ds_all = LabeledDataset(TRAIN_IMAGES_DIR, Y_TRAIN_CSV)\n",
    "\n",
    "    # split by well (val=6)\n",
    "    train_idx, val_idx = [], []\n",
    "    for i, name in enumerate(ds_all.names):\n",
    "        w = parse_well_id(name)\n",
    "        if w == 6:\n",
    "            val_idx.append(i)\n",
    "        else:\n",
    "            train_idx.append(i)\n",
    "\n",
    "    train_ds = Subset(ds_all, train_idx)  # well1-5\n",
    "    val_ds   = Subset(ds_all, val_idx)    # well6\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=BATCH_SIZE_L,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_labeled,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=BATCH_SIZE_L,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_labeled,\n",
    "    )\n",
    "\n",
    "    # ---- unlabeled dataset (well12-14) ----\n",
    "    unlab_ds = UnlabeledDataset(UNLABELED_DIR)\n",
    "    unlab_loader = DataLoader(\n",
    "        unlab_ds,\n",
    "        batch_size=BATCH_SIZE_U,\n",
    "        shuffle=True,\n",
    "        num_workers=0,\n",
    "        collate_fn=collate_unlabeled,\n",
    "    )\n",
    "\n",
    "    print(f\"Labeled train: {len(train_ds)} | Val: {len(val_ds)} | Unlabeled: {len(unlab_ds)}\")\n",
    "\n",
    "    model, processor = build_model(NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    best_val = 1e9\n",
    "    best_path = DATA_ROOT / \"best_mask2former_semi.pth\"\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        tr_l, tr_u = train_one_epoch_semi(model, train_loader, unlab_loader, optimizer)\n",
    "        va = valid_one_epoch(model, val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch:02d}/{EPOCHS} | train_l={tr_l:.4f} | train_u={tr_u:.4f} | val={va:.4f}\")\n",
    "\n",
    "        if va < best_val:\n",
    "            best_val = va\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  -> Best saved: {best_path}\")\n",
    "\n",
    "    # ---- inference ----\n",
    "    out_csv = DATA_ROOT / \"submission.csv\"\n",
    "    model.load_state_dict(torch.load(best_path, map_location=DEVICE, weights_only=True))\n",
    "    predict_and_make_submission(model, processor, TEST_IMAGES_DIR, out_csv)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fcbe86",
   "metadata": {},
   "source": [
    "# La solution finale choisie---un modle semi-supervis et auto-supervis pr-entran de type Segformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e6fc93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 0  Imports and Constants\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "X_TEST_DIR  = Path(r\"C:\\Users\\asus\\Desktop\\ECN\\DEEP\\DataChallenge\\data\\X_test_xNbnvIa\")\n",
    "X_TRAIN_DIR = Path(r\"C:\\Users\\asus\\Desktop\\ECN\\DEEP\\DataChallenge\\data\\X_train_uDRk9z9\")\n",
    "X_UNLAB_DIR = Path(r\"C:\\Users\\asus\\Desktop\\ECN\\DEEP\\DataChallenge\\data\\X_unlabeled_mtkxUlo\")\n",
    "Y_TRAIN_CSV = Path(r\"C:\\Users\\asus\\Desktop\\ECN\\DEEP\\DataChallenge\\data\\Y_train_T9NrBYo.csv\")\n",
    "SAMPLE_SUB  = Path(r\"C:\\Users\\asus\\Desktop\\ECN\\DEEP\\DataChallenge\\data\\submission_csv_file_random_example_3qPSCtv.csv\")\n",
    "\n",
    "# ====== Outputs ======\n",
    "OUT_DIR = Path(r\"exp_outputs\\Exp04_SSL_SegFormer_Semi\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SSL_DIR  = OUT_DIR / \"ssl_pretrain\"\n",
    "SUP_DIR  = OUT_DIR / \"supervised_finetune\"\n",
    "SEMI_DIR = OUT_DIR / \"semi_train\"\n",
    "for d in [SSL_DIR, SUP_DIR, SEMI_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ====== constants ======\n",
    "NUM_CLASSES  = 3\n",
    "IGNORE_INDEX = 255\n",
    "H            = 160\n",
    "W_PAD        = 288  # pad to 288, later crop to 160/272\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56de4ac",
   "metadata": {},
   "source": [
    "## Fonctions utilitaires de lecture et de prtraitement des donnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fc10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1  utilities: file listing (no double counting), parse name, load/pad X, load/pad Y\n",
    "NAME_RE = re.compile(r\"well_(\\d+)_section_(\\d+)_patch_(\\d+)$\")\n",
    "\n",
    "def parse_name(stem: str):\n",
    "    m = NAME_RE.match(stem)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Bad patch name: {stem}\")\n",
    "    return int(m.group(1)), int(m.group(2)), int(m.group(3))\n",
    "\n",
    "def list_npy_files(dir_path: Path):\n",
    "    # de-dup robustly; avoids Windows *.npy/*.NPY double counting\n",
    "    files = list(dir_path.rglob(\"*.npy\")) + list(dir_path.rglob(\"*.NPY\"))\n",
    "    uniq = sorted({Path(p).resolve() for p in files})\n",
    "    return [Path(p) for p in uniq]\n",
    "\n",
    "def load_x(path: Path) -> np.ndarray:\n",
    "    x = np.load(path)\n",
    "    if x.ndim == 3 and x.shape[0] == 1:\n",
    "        x = x[0]\n",
    "    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0).astype(np.float32)\n",
    "    mn, mx = float(x.min()), float(x.max())\n",
    "    if mx > mn:\n",
    "        x = (x - mn) / (mx - mn)\n",
    "    else:\n",
    "        x = np.zeros_like(x, dtype=np.float32)\n",
    "    return x  # (160,w)\n",
    "\n",
    "def pad_x_to_wpad(x: np.ndarray) -> np.ndarray:\n",
    "    h, w = x.shape\n",
    "    out = np.zeros((h, W_PAD), dtype=np.float32)\n",
    "    out[:, :w] = x\n",
    "    return out\n",
    "\n",
    "def make_valid_mask(w: int) -> np.ndarray:\n",
    "    valid = np.zeros((H, W_PAD), dtype=np.bool_)\n",
    "    valid[:, :w] = True\n",
    "    return valid\n",
    "\n",
    "y_df = pd.read_csv(Y_TRAIN_CSV, index_col=0)\n",
    "\n",
    "def restore_mask_from_row(row_values: np.ndarray) -> np.ndarray:\n",
    "    vals = row_values[row_values != -1]\n",
    "    return vals.reshape(H, -1).astype(np.int64)  # (160,160) or (160,272)\n",
    "\n",
    "def pad_mask_to_wpad(mask: np.ndarray, w: int) -> np.ndarray:\n",
    "    out = np.full((H, W_PAD), IGNORE_INDEX, dtype=np.int64)\n",
    "    out[:, :w] = mask\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8962316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train(all images): 4410\n",
      "train(labeled):    4410\n",
      "unlabeled:         1980\n",
      "SSL pool:          6390\n",
      "test:              972\n"
     ]
    }
   ],
   "source": [
    "# Cell 2  build manifests (train/unlab/test) + SSL pool \n",
    "def build_manifest(x_dir: Path) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for p in list_npy_files(x_dir):\n",
    "        stem = p.stem\n",
    "        try:\n",
    "            well, section, patch = parse_name(stem)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        arr = np.load(p, mmap_mode=\"r\")\n",
    "        if arr.ndim == 3 and arr.shape[0] == 1:\n",
    "            w = int(arr.shape[2])\n",
    "        elif arr.ndim == 2:\n",
    "            w = int(arr.shape[1])\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected shape {arr.shape} for {p}\")\n",
    "        rows.append({\"name\": stem, \"well\": well, \"section\": section, \"patch\": patch, \"w\": w, \"path\": str(p)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "train_df = build_manifest(X_TRAIN_DIR)\n",
    "unlab_df = build_manifest(X_UNLAB_DIR)\n",
    "test_df  = build_manifest(X_TEST_DIR)\n",
    "\n",
    "# labeled train only\n",
    "train_labeled_df = train_df[train_df[\"name\"].isin(y_df.index)].reset_index(drop=True)\n",
    "\n",
    "# SSL uses: all train images (even if labeled) + unlabeled\n",
    "ssl_df = pd.concat([train_df, unlab_df], axis=0, ignore_index=True)\n",
    "ssl_df = ssl_df.drop_duplicates(subset=[\"path\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"train(all images):\", len(train_df))\n",
    "print(\"train(labeled):   \", len(train_labeled_df))\n",
    "print(\"unlabeled:        \", len(unlab_df))\n",
    "print(\"SSL pool:         \", len(ssl_df))\n",
    "print(\"test:             \", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38893609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssl batch: torch.Size([32, 1, 160, 288])\n"
     ]
    }
   ],
   "source": [
    "# Cell 3  SSL augmentations (SimSiam): two random views from same image\n",
    "def ssl_aug(x: torch.Tensor) -> torch.Tensor:\n",
    "    # x: (B,1,H,W_PAD) in [0,1]\n",
    "    B, _, Hh, Ww = x.shape\n",
    "\n",
    "    # intensity jitter\n",
    "    a = torch.empty((B,1,1,1), device=x.device).uniform_(0.85, 1.15)\n",
    "    b = torch.empty((B,1,1,1), device=x.device).uniform_(-0.08, 0.08)\n",
    "    out = torch.clamp(x * a + b, 0.0, 1.0)\n",
    "\n",
    "    # noise\n",
    "    sigma = torch.empty((B,1,1,1), device=x.device).uniform_(0.0, 0.06)\n",
    "    out = torch.clamp(out + torch.randn_like(out) * sigma, 0.0, 1.0)\n",
    "\n",
    "    # random horizontal flip\n",
    "    if torch.rand(()) < 0.5:\n",
    "        out = torch.flip(out, dims=[3])\n",
    "\n",
    "    # cutout\n",
    "    for i in range(B):\n",
    "        if torch.rand((), device=x.device).item() < 0.5:\n",
    "            ch = int(torch.randint(low=10, high=50, size=(1,), device=x.device).item())\n",
    "            cw = int(torch.randint(low=10, high=80, size=(1,), device=x.device).item())\n",
    "            y0 = int(torch.randint(low=0, high=Hh-ch+1, size=(1,), device=x.device).item())\n",
    "            x0 = int(torch.randint(low=0, high=Ww-cw+1, size=(1,), device=x.device).item())\n",
    "            out[i, :, y0:y0+ch, x0:x0+cw] = 0.0\n",
    "\n",
    "    return out\n",
    "\n",
    "class SSLDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        x = load_x(Path(row[\"path\"]))      # (160,w)\n",
    "        x = pad_x_to_wpad(x)               # (160,288)\n",
    "        x_t = torch.from_numpy(x).unsqueeze(0)  # (1,160,288)\n",
    "        # return raw tensor; augment will be done on GPU in training step for speed\n",
    "        return x_t\n",
    "\n",
    "ssl_loader = DataLoader(SSLDataset(ssl_df), batch_size=32, shuffle=True, num_workers=0, pin_memory=(DEVICE==\"cuda\"))\n",
    "xb = next(iter(ssl_loader))\n",
    "print(\"ssl batch:\", xb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b114f9c",
   "metadata": {},
   "source": [
    "## Dfinition du modle auto-supervis (architecture de base SimSiam + SegFormer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f7feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimSiam backbone last hidden: 512\n"
     ]
    }
   ],
   "source": [
    "# Cell 4  SimSiam with SegFormer backbone\n",
    "from transformers import SegformerModel\n",
    "\n",
    "BACKBONE = \"nvidia/segformer-b2-finetuned-ade-512-512\"\n",
    "\n",
    "def global_pool(feat: torch.Tensor) -> torch.Tensor:\n",
    "    # feat: (B,C,H,W) -> (B,C)\n",
    "    return feat.mean(dim=(2,3))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "            nn.BatchNorm1d(out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int, out_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.BatchNorm1d(hidden),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SimSiamSegFormer(nn.Module):\n",
    "    def __init__(self, backbone_name: str, proj_dim=256, pred_dim=256, hidden=1024):\n",
    "        super().__init__()\n",
    "        self.backbone = SegformerModel.from_pretrained(backbone_name)\n",
    "        # infer feature dim: segformer config has hidden_sizes per stage; last stage is strongest\n",
    "        feat_dim = self.backbone.config.hidden_sizes[-1]\n",
    "        self.projector = MLP(feat_dim, hidden, proj_dim)\n",
    "        self.predictor = Predictor(proj_dim, hidden//2, pred_dim)\n",
    "\n",
    "    def encode(self, x3):\n",
    "        # x3: (B,3,160,288)\n",
    "        out = self.backbone(pixel_values=x3, output_hidden_states=True)\n",
    "        # last stage feature map is hidden_states[-1] with shape (B,C,H',W')\n",
    "        feat = out.hidden_states[-1]\n",
    "        v = global_pool(feat)\n",
    "        z = self.projector(v)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        z1 = self.encode(x1)\n",
    "        z2 = self.encode(x2)\n",
    "        p1 = self.predictor(z1)\n",
    "        p2 = self.predictor(z2)\n",
    "        return p1, p2, z1.detach(), z2.detach()\n",
    "\n",
    "def neg_cos(p, z):\n",
    "    p = F.normalize(p, dim=1)\n",
    "    z = F.normalize(z, dim=1)\n",
    "    return -(p * z).sum(dim=1).mean()\n",
    "\n",
    "ssl_model = SimSiamSegFormer(BACKBONE).to(DEVICE)\n",
    "print(\"SimSiam backbone last hidden:\", ssl_model.backbone.config.hidden_sizes[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d104765",
   "metadata": {},
   "source": [
    "## Pr-formation auto-supervise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d562f5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\2460274430.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
      "[Exp04-SSL] ep1:   0%|          | 0/200 [00:00<?, ?it/s]C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\2460274430.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep01/10 loss=-0.6239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep02/10 loss=-0.6491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep03/10 loss=-0.6651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep04/10 loss=-0.6547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep05/10 loss=-0.6441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep06/10 loss=-0.6519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep07/10 loss=-0.6514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep08/10 loss=-0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep09/10 loss=-0.6519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep10/10 loss=-0.6435\n",
      "[Exp04-SSL] best loss: -0.6650720976887734\n",
      "saved ssl_ckpt: exp_outputs\\Exp04_SSL_SegFormer_Semi\\ssl_pretrain\\ssl_best.pt\n",
      "saved ssl_backbone: exp_outputs\\Exp04_SSL_SegFormer_Semi\\ssl_pretrain\\segformer_backbone_ssl.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Cell 5  SSL pretrain loop (SimSiam) -> save backbone weights\n",
    "SSL_EPOCHS = 10\n",
    "SSL_LR = 3e-4\n",
    "SSL_WD = 1e-4\n",
    "\n",
    "opt = torch.optim.AdamW(ssl_model.parameters(), lr=SSL_LR, weight_decay=SSL_WD)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "best_ssl = 1e9\n",
    "ssl_ckpt = SSL_DIR / \"ssl_best.pt\"\n",
    "ssl_backbone = SSL_DIR / \"segformer_backbone_ssl.pt\"\n",
    "\n",
    "ssl_model.train()\n",
    "for ep in range(1, SSL_EPOCHS+1):\n",
    "    loss_sum, n = 0.0, 0\n",
    "    for x in tqdm(ssl_loader, desc=f\"[Exp04-SSL] ep{ep}\", leave=False):\n",
    "        x = x.to(DEVICE)  # (B,1,160,288)\n",
    "\n",
    "        # two views on GPU\n",
    "        x1 = ssl_aug(x)\n",
    "        x2 = ssl_aug(x)\n",
    "\n",
    "        # segformer needs 3 channels\n",
    "        x1 = x1.repeat(1,3,1,1)\n",
    "        x2 = x2.repeat(1,3,1,1)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "            p1, p2, z1, z2 = ssl_model(x1, x2)\n",
    "            loss = 0.5 * (neg_cos(p1, z2) + neg_cos(p2, z1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_sum += float(loss.item()) * x.size(0)\n",
    "        n += x.size(0)\n",
    "\n",
    "    ep_loss = loss_sum / max(1, n)\n",
    "    print(f\"[Exp04-SSL] ep{ep:02d}/{SSL_EPOCHS} loss={ep_loss:.4f}\")\n",
    "\n",
    "    if ep_loss < best_ssl:\n",
    "        best_ssl = ep_loss\n",
    "        torch.save({\"model\": ssl_model.state_dict()}, ssl_ckpt)\n",
    "        # save ONLY backbone weights for later init\n",
    "        torch.save(ssl_model.backbone.state_dict(), ssl_backbone)\n",
    "\n",
    "print(\"[Exp04-SSL] best loss:\", best_ssl)\n",
    "print(\"saved ssl_ckpt:\", ssl_ckpt)\n",
    "print(\"saved ssl_backbone:\", ssl_backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c75a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\2460274430.py:7: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
      "[Exp04-SSL] ep1:   0%|          | 0/200 [00:00<?, ?it/s]C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\2460274430.py:28: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep01/10 loss=-0.6239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep02/10 loss=-0.6491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep03/10 loss=-0.6651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep04/10 loss=-0.6547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep05/10 loss=-0.6441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep06/10 loss=-0.6519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep07/10 loss=-0.6514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep08/10 loss=-0.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep09/10 loss=-0.6519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SSL] ep10/10 loss=-0.6435\n",
      "[Exp04-SSL] best loss: -0.6650720976887734\n",
      "saved ssl_ckpt: exp_outputs\\Exp04_SSL_SegFormer_Semi\\ssl_pretrain\\ssl_best.pt\n",
      "saved ssl_backbone: exp_outputs\\Exp04_SSL_SegFormer_Semi\\ssl_pretrain\\segformer_backbone_ssl.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Cell 5  SSL pretrain loop (SimSiam) -> save backbone weights\n",
    "SSL_EPOCHS = 10\n",
    "SSL_LR = 3e-4\n",
    "SSL_WD = 1e-4\n",
    "\n",
    "opt = torch.optim.AdamW(ssl_model.parameters(), lr=SSL_LR, weight_decay=SSL_WD)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "best_ssl = 1e9\n",
    "ssl_ckpt = SSL_DIR / \"ssl_best.pt\"\n",
    "ssl_backbone = SSL_DIR / \"segformer_backbone_ssl.pt\"\n",
    "\n",
    "ssl_model.train()\n",
    "for ep in range(1, SSL_EPOCHS+1):\n",
    "    loss_sum, n = 0.0, 0\n",
    "    for x in tqdm(ssl_loader, desc=f\"[Exp04-SSL] ep{ep}\", leave=False):\n",
    "        x = x.to(DEVICE)  # (B,1,160,288)\n",
    "\n",
    "        # two views on GPU\n",
    "        x1 = ssl_aug(x)\n",
    "        x2 = ssl_aug(x)\n",
    "\n",
    "        # segformer needs 3 channels\n",
    "        x1 = x1.repeat(1,3,1,1)\n",
    "        x2 = x2.repeat(1,3,1,1)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "            p1, p2, z1, z2 = ssl_model(x1, x2)\n",
    "            loss = 0.5 * (neg_cos(p1, z2) + neg_cos(p2, z1))\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "\n",
    "        loss_sum += float(loss.item()) * x.size(0)\n",
    "        n += x.size(0)\n",
    "\n",
    "    ep_loss = loss_sum / max(1, n)\n",
    "    print(f\"[Exp04-SSL] ep{ep:02d}/{SSL_EPOCHS} loss={ep_loss:.4f}\")\n",
    "\n",
    "    if ep_loss < best_ssl:\n",
    "        best_ssl = ep_loss\n",
    "        torch.save({\"model\": ssl_model.state_dict()}, ssl_ckpt)\n",
    "        # save ONLY backbone weights for later init\n",
    "        torch.save(ssl_model.backbone.state_dict(), ssl_backbone)\n",
    "\n",
    "print(\"[Exp04-SSL] best loss:\", best_ssl)\n",
    "print(\"saved ssl_ckpt:\", ssl_ckpt)\n",
    "print(\"saved ssl_backbone:\", ssl_backbone)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98879239",
   "metadata": {},
   "source": [
    "## Formation semi-supervise (pseudo-tiquette d'enseignant EMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14864297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b2-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b2-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\3083726334.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sup_state = torch.load(best_path, map_location=DEVICE)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\3083726334.py:76: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
      "[Exp04-SEMI] train ep1 (lam_u=0.20):   0%|          | 0/516 [00:00<?, ?it/s]C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\3083726334.py:107: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SEMI] ep01/10 val_mIoU=0.7896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SEMI] ep02/10 val_mIoU=0.8001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SEMI] ep03/10 val_mIoU=0.7937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SEMI] ep04/10 val_mIoU=0.7986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SEMI] ep05/10 val_mIoU=0.8034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SEMI] ep06/10 val_mIoU=0.7909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SEMI] ep07/10 val_mIoU=0.7913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SEMI] ep08/10 val_mIoU=0.7929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SEMI] ep09/10 val_mIoU=0.7953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04-SEMI] ep10/10 val_mIoU=0.8005\n",
      "[Exp04-SEMI] BEST val mIoU: 0.8033703847063912 saved: exp_outputs\\Exp04_SSL_SegFormer_Semi\\semi\\best_state_dict.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Cell 7  Semi-supervised (EMA Teacher) starting from Exp04 supervised best\n",
    "# unlabeled loader (reuse unlab_df)\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        w = int(row[\"w\"])\n",
    "        x = load_x(Path(row[\"path\"]))\n",
    "        x = pad_x_to_wpad(x)\n",
    "        x_t = torch.from_numpy(x).unsqueeze(0)\n",
    "        valid = torch.from_numpy(make_valid_mask(w))\n",
    "        return x_t, valid\n",
    "\n",
    "unlab_loader = DataLoader(UnlabeledDataset(unlab_df), batch_size=8, shuffle=True, num_workers=0, pin_memory=(DEVICE==\"cuda\"))\n",
    "\n",
    "def weak_aug(x: torch.Tensor) -> torch.Tensor:\n",
    "    B = x.size(0)\n",
    "    a = torch.empty((B,1,1,1), device=x.device).uniform_(0.95, 1.05)\n",
    "    b = torch.empty((B,1,1,1), device=x.device).uniform_(-0.03, 0.03)\n",
    "    return torch.clamp(x * a + b, 0.0, 1.0)\n",
    "\n",
    "def strong_aug(x: torch.Tensor) -> torch.Tensor:\n",
    "    B, _, Hh, Ww = x.shape\n",
    "    a = torch.empty((B,1,1,1), device=x.device).uniform_(0.85, 1.15)\n",
    "    b = torch.empty((B,1,1,1), device=x.device).uniform_(-0.08, 0.08)\n",
    "    out = torch.clamp(x * a + b, 0.0, 1.0)\n",
    "    sigma = torch.empty((B,1,1,1), device=x.device).uniform_(0.0, 0.06)\n",
    "    out = torch.clamp(out + torch.randn_like(out) * sigma, 0.0, 1.0)\n",
    "    for i in range(B):\n",
    "        if torch.rand((), device=x.device).item() < 0.5:\n",
    "            ch = int(torch.randint(low=10, high=50, size=(1,), device=x.device).item())\n",
    "            cw = int(torch.randint(low=10, high=80, size=(1,), device=x.device).item())\n",
    "            y0 = int(torch.randint(low=0, high=Hh-ch+1, size=(1,), device=x.device).item())\n",
    "            x0 = int(torch.randint(low=0, high=Ww-cw+1, size=(1,), device=x.device).item())\n",
    "            out[i, :, y0:y0+ch, x0:x0+cw] = 0.0\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(teacher, student, alpha: float):\n",
    "    for t_p, s_p in zip(teacher.parameters(), student.parameters()):\n",
    "        t_p.data.mul_(alpha).add_(s_p.data, alpha=1.0 - alpha)\n",
    "    for t_b, s_b in zip(teacher.buffers(), student.buffers()):\n",
    "        t_b.copy_(s_b)\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for b in loader:\n",
    "            yield b\n",
    "\n",
    "# init student/teacher from supervised best\n",
    "student = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    BACKBONE, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "teacher = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    BACKBONE, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "\n",
    "sup_state = torch.load(best_path, map_location=DEVICE)\n",
    "student.load_state_dict(sup_state)\n",
    "teacher.load_state_dict(sup_state)\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "# unlabeled CE loss (ignore_index)\n",
    "ce_u = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
    "\n",
    "def rampup(epoch: int, ramp_epochs=5):\n",
    "    return min(1.0, float(epoch+1)/float(ramp_epochs))\n",
    "\n",
    "SEMI_EPOCHS = 10\n",
    "LR = 6e-5\n",
    "opt = torch.optim.AdamW(student.parameters(), lr=LR, weight_decay=0.01)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "TAU = 0.95\n",
    "EMA_ALPHA = 0.996\n",
    "LAMBDA_U = 1.0\n",
    "RAMP_E = 5\n",
    "\n",
    "semi_best = SEMI_DIR / \"best_state_dict.pt\"\n",
    "best_miou = -1.0\n",
    "\n",
    "unlab_iter = cycle(unlab_loader)\n",
    "\n",
    "for ep in range(1, SEMI_EPOCHS+1):\n",
    "    lam_u = LAMBDA_U * rampup(ep-1, RAMP_E)\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    for x_l, y_l, meta in tqdm(train_loader, desc=f\"[Exp04-SEMI] train ep{ep} (lam_u={lam_u:.2f})\", leave=False):\n",
    "        x_u, valid_u = next(unlab_iter)\n",
    "\n",
    "        x_l = x_l.to(DEVICE)              # (B,1,160,288)\n",
    "        y_l = y_l.to(DEVICE)              # (B,160,288)\n",
    "        x_u = x_u.to(DEVICE)              # (B,1,160,288)\n",
    "        valid_u = valid_u.to(DEVICE)      # (B,160,288)\n",
    "\n",
    "        x_l3 = x_l.repeat(1,3,1,1)\n",
    "\n",
    "        x_u_w = weak_aug(x_u).repeat(1,3,1,1)\n",
    "        x_u_s = strong_aug(x_u).repeat(1,3,1,1)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "            # labeled\n",
    "            logits_l = student(pixel_values=x_l3).logits\n",
    "            logits_l = upsample_logits(logits_l, y_l.shape[-2:])\n",
    "            loss_l = combo_loss(logits_l, y_l, dice_w=0.5)\n",
    "\n",
    "            # teacher pseudo\n",
    "            with torch.no_grad():\n",
    "                logits_t = teacher(pixel_values=x_u_w).logits\n",
    "                logits_t = upsample_logits(logits_t, (H, W_PAD))\n",
    "                probs_t = torch.softmax(logits_t, dim=1)\n",
    "                conf, pseudo = torch.max(probs_t, dim=1)  # (B,160,288)\n",
    "                mask = (conf >= TAU) & valid_u\n",
    "                pseudo_pl = pseudo.clone()\n",
    "                pseudo_pl[~mask] = IGNORE_INDEX\n",
    "\n",
    "            # unlabeled loss\n",
    "            if lam_u > 0:\n",
    "                logits_u = student(pixel_values=x_u_s).logits\n",
    "                logits_u = upsample_logits(logits_u, (H, W_PAD))\n",
    "                loss_u = ce_u(logits_u, pseudo_pl)\n",
    "            else:\n",
    "                loss_u = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "            loss = loss_l + lam_u * loss_u\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "\n",
    "        ema_update(teacher, student, EMA_ALPHA)\n",
    "\n",
    "    # val\n",
    "    student.eval()\n",
    "    miou_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, meta in tqdm(val_loader, desc=f\"[Exp04-SEMI] val ep{ep}\", leave=False):\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            x3 = x.repeat(1,3,1,1)\n",
    "            logits = student(pixel_values=x3).logits\n",
    "            logits = upsample_logits(logits, y.shape[-2:])\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            miou_sum += float(mean_iou(pred, y).item()) * x.size(0)\n",
    "            n += x.size(0)\n",
    "    val_miou = miou_sum / max(1, n)\n",
    "    print(f\"[Exp04-SEMI] ep{ep:02d}/{SEMI_EPOCHS} val_mIoU={val_miou:.4f}\")\n",
    "\n",
    "    if val_miou > best_miou:\n",
    "        best_miou = val_miou\n",
    "        torch.save(student.state_dict(), semi_best)\n",
    "\n",
    "print(\"[Exp04-SEMI] BEST val mIoU:\", best_miou, \"saved:\", semi_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd437398",
   "metadata": {},
   "source": [
    "## Raisonnement sur l'ensemble de tests et gnration du fichier de soumission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b74c6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample rows: 972 name_col: Unnamed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b2-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\2646445382.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student.load_state_dict(torch.load(semi_best, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded semi best: exp_outputs\\Exp04_SSL_SegFormer_Semi\\semi\\best_state_dict.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved npy predictions to: exp_outputs\\Exp04_SSL_SegFormer_Semi\\test_predictions\n",
      "Saved submission: exp_outputs\\Exp04_SSL_SegFormer_Semi\\y_test_submission_MATCH_SAMPLE.csv shape: (972, 43521)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8  Predict test (by sample order) using Exp04 semi best, save npy + submission CSV\n",
    "sample = pd.read_csv(SAMPLE_SUB)\n",
    "name_col = sample.columns[0]\n",
    "ordered_names_raw = sample[name_col].astype(str).tolist()\n",
    "\n",
    "def norm_name(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    if s.lower().endswith(\".npy\"):\n",
    "        s = s[:-4]\n",
    "    return s\n",
    "\n",
    "ordered_names = [norm_name(n) for n in ordered_names_raw]\n",
    "print(\"sample rows:\", len(ordered_names), \"name_col:\", name_col)\n",
    "\n",
    "test_files = list_npy_files(X_TEST_DIR)\n",
    "test_index = {p.stem: p for p in test_files}\n",
    "test_index.update({p.stem.lower(): p for p in test_files})\n",
    "\n",
    "# load best semi student\n",
    "student = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    BACKBONE, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "student.load_state_dict(torch.load(semi_best, map_location=DEVICE))\n",
    "student.eval()\n",
    "print(\"Loaded Exp04 semi best:\", semi_best)\n",
    "\n",
    "pred_dir = OUT_DIR / \"test_predictions\"\n",
    "pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "for p in pred_dir.glob(\"*.npy\"):\n",
    "    p.unlink()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name in tqdm(ordered_names, desc=\"[Exp04] predict test\", leave=False):\n",
    "        key = name if name in test_index else name.lower()\n",
    "        if key not in test_index:\n",
    "            hits = list(X_TEST_DIR.rglob(f\"{name}.npy\")) + list(X_TEST_DIR.rglob(f\"{name}.NPY\"))\n",
    "            if len(hits) == 0:\n",
    "                raise FileNotFoundError(f\"X_test missing: {name}.npy\")\n",
    "            x_path = hits[0]\n",
    "        else:\n",
    "            x_path = test_index[key]\n",
    "\n",
    "        x = load_x(x_path)\n",
    "        w = x.shape[1]\n",
    "        x_pad = pad_x_to_wpad(x)\n",
    "        x_t = torch.from_numpy(x_pad).unsqueeze(0).unsqueeze(0).to(DEVICE)  # (1,1,160,288)\n",
    "        x_t = x_t.repeat(1,3,1,1)\n",
    "\n",
    "        logits = student(pixel_values=x_t).logits\n",
    "        logits = upsample_logits(logits, (H, W_PAD))\n",
    "        pred = torch.argmax(logits, dim=1).squeeze(0).cpu().numpy().astype(np.int64)\n",
    "        pred = pred[:, :w]\n",
    "        np.save(pred_dir / f\"{name}.npy\", pred)\n",
    "\n",
    "print(\"saved npy predictions to:\", pred_dir)\n",
    "\n",
    "# build submission CSV (exact sample format)\n",
    "size_labels = 272\n",
    "flat_len = H * size_labels\n",
    "\n",
    "pred_map = {}\n",
    "for p in pred_dir.glob(\"*.npy\"):\n",
    "    nm = p.stem\n",
    "    pred = np.load(p)\n",
    "    if pred.shape[1] != size_labels:\n",
    "        aux = -1 + np.zeros(flat_len, dtype=np.int64)\n",
    "        aux[0:H*H] = pred.flatten()\n",
    "    else:\n",
    "        aux = pred.flatten().astype(np.int64)\n",
    "    pred_map[nm] = aux\n",
    "\n",
    "missing = [n for n in ordered_names if n not in pred_map]\n",
    "assert len(missing) == 0, f\"missing predictions: {missing[:10]}\"\n",
    "\n",
    "data = np.stack([pred_map[n] for n in ordered_names], axis=0)\n",
    "col_names = [str(i) for i in range(flat_len)]\n",
    "sub_df = pd.DataFrame(data, columns=col_names)\n",
    "sub_df.insert(0, name_col, ordered_names_raw)\n",
    "\n",
    "out_csv = OUT_DIR / \"y_test_submission_MATCH_SAMPLE.csv\"\n",
    "sub_df.to_csv(out_csv, index=False)\n",
    "print(\"Saved submission:\", out_csv, \"shape:\", sub_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d31a364",
   "metadata": {},
   "source": [
    "## Exprience semi-supervise amliore opt2 --Pseudo-tiquettes de contrle de couverture + KL masqu + (phase d'infrence) TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091ef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b2-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b2-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\1779857020.py:127: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sup_state = torch.load(best_path, map_location=DEVICE)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\1779857020.py:142: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
      "[Exp04_SEMI_opt2_Q30] train ep1 (lam_u=0.05):   0%|          | 0/516 [00:00<?, ?it/s]C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\1779857020.py:170: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep01: pseudo_cov=0.302  pseudo_cls_ratio=[9.8469114e-01 1.1722528e-06 1.5307704e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep01/10 val_mIoU=0.7972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep02: pseudo_cov=0.303  pseudo_cls_ratio=[0.9826055  0.         0.01739453]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep02/10 val_mIoU=0.7923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep03: pseudo_cov=0.303  pseudo_cls_ratio=[0.98382986 0.         0.01617016]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep03/10 val_mIoU=0.7948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep04: pseudo_cov=0.306  pseudo_cls_ratio=[0.9865239  0.         0.01347608]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep04/10 val_mIoU=0.7902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep05: pseudo_cov=0.307  pseudo_cls_ratio=[0.98596615 0.         0.01403393]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep05/10 val_mIoU=0.7794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep06: pseudo_cov=0.312  pseudo_cls_ratio=[0.9874522  0.         0.01254772]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep06/10 val_mIoU=0.7911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep07: pseudo_cov=0.312  pseudo_cls_ratio=[0.98723954 0.         0.01276045]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep07/10 val_mIoU=0.7966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep08: pseudo_cov=0.312  pseudo_cls_ratio=[0.9854275  0.         0.01457244]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep08/10 val_mIoU=0.7988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep09: pseudo_cov=0.310  pseudo_cls_ratio=[9.8177379e-01 4.9080318e-06 1.8221341e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep09/10 val_mIoU=0.7903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep10: pseudo_cov=0.309  pseudo_cls_ratio=[9.7869164e-01 3.6085919e-06 2.1304812e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt2_Q30] ep10/10 val_mIoU=0.7970\n",
      "[Exp04_SEMI_opt2_Q30] BEST val mIoU: 0.7987618578804864 saved: exp_outputs\\Exp04_SEMI_opt2_Q30\\semi\\best_state_dict_opt2.pt\n",
      "Outputs in: exp_outputs\\Exp04_SEMI_opt2_Q30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Cell 7b  Semi-supervised v2 (EMA + coverage-controlled pseudo labels) [NEW EXP, NEW FILE NAMES]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "EXP_NAME = \"Exp04_SEMI_opt2_Q30\"\n",
    "OUT_DIR2 = Path(rf\"exp_outputs\\{EXP_NAME}\")\n",
    "OUT_DIR2.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEMI_DIR2 = OUT_DIR2 / \"semi\"\n",
    "SEMI_DIR2.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- unlabeled loader ----------\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        w = int(row[\"w\"])\n",
    "        x = load_x(Path(row[\"path\"]))\n",
    "        x = pad_x_to_wpad(x)                      # (160,288)\n",
    "        x_t = torch.from_numpy(x).unsqueeze(0)    # (1,160,288)\n",
    "        valid = torch.from_numpy(make_valid_mask(w))  # (160,288) bool\n",
    "        return x_t, valid\n",
    "\n",
    "unlab_loader2 = DataLoader(\n",
    "    UnlabeledDataset(unlab_df),\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=(DEVICE==\"cuda\")\n",
    ")\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for b in loader:\n",
    "            yield b\n",
    "\n",
    "# ---------- augmentations ( strong aug) ----------\n",
    "def weak_aug(x: torch.Tensor) -> torch.Tensor:\n",
    "    B = x.size(0)\n",
    "    a = torch.empty((B,1,1,1), device=x.device).uniform_(0.97, 1.03)\n",
    "    b = torch.empty((B,1,1,1), device=x.device).uniform_(-0.02, 0.02)\n",
    "    return torch.clamp(x * a + b, 0.0, 1.0)\n",
    "\n",
    "def strong_aug(x: torch.Tensor) -> torch.Tensor:\n",
    "    B, _, Hh, Ww = x.shape\n",
    "    a = torch.empty((B,1,1,1), device=x.device).uniform_(0.90, 1.10)\n",
    "    b = torch.empty((B,1,1,1), device=x.device).uniform_(-0.05, 0.05)\n",
    "    out = torch.clamp(x * a + b, 0.0, 1.0)\n",
    "    sigma = torch.empty((B,1,1,1), device=x.device).uniform_(0.0, 0.03)\n",
    "    out = torch.clamp(out + torch.randn_like(out) * sigma, 0.0, 1.0)\n",
    "\n",
    "    # cutout0.250\n",
    "    for i in range(B):\n",
    "        if torch.rand((), device=x.device).item() < 0.25:\n",
    "            ch = int(torch.randint(low=8, high=24, size=(1,), device=x.device).item())\n",
    "            cw = int(torch.randint(low=12, high=48, size=(1,), device=x.device).item())\n",
    "            y0 = int(torch.randint(low=0, high=Hh-ch+1, size=(1,), device=x.device).item())\n",
    "            x0 = int(torch.randint(low=0, high=Ww-cw+1, size=(1,), device=x.device).item())\n",
    "            out[i, :, y0:y0+ch, x0:x0+cw] = 0.0\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(teacher, student, alpha: float):\n",
    "    for t_p, s_p in zip(teacher.parameters(), student.parameters()):\n",
    "        t_p.data.mul_(alpha).add_(s_p.data, alpha=1.0 - alpha)\n",
    "    for t_b, s_b in zip(teacher.buffers(), student.buffers()):\n",
    "        t_b.copy_(s_b)\n",
    "\n",
    "def rampup(epoch0: int, ramp_epochs: int):\n",
    "    if ramp_epochs <= 0:\n",
    "        return 1.0\n",
    "    return min(1.0, float(epoch0 + 1) / float(ramp_epochs))\n",
    "\n",
    "# ---------- pseudo-label selection () ----------\n",
    "TAU_BY_CLASS = torch.tensor([0.995, 0.97, 0.96], device=DEVICE)  # base\n",
    "TARGET_COVER = 0.30  # 30%0.20/0.40\n",
    "TEMP = 1.0           # sharpeningconf\n",
    "\n",
    "def build_mask_with_target_cover(conf, pseudo, valid, target_cover: float):\n",
    "    \"\"\"\n",
    "    conf:  (B,H,W)\n",
    "    pseudo:(B,H,W)\n",
    "    valid: (B,H,W) bool\n",
    "    return: mask (B,H,W) bool\n",
    "    \"\"\"\n",
    "    # 1) class-wise base threshold\n",
    "    thr_base = TAU_BY_CLASS[pseudo]           # (B,H,W)\n",
    "    mask = (conf >= thr_base) & valid\n",
    "\n",
    "    # 2)  valid  top-q  conf\n",
    "    conf_valid = conf[valid]\n",
    "    if conf_valid.numel() == 0:\n",
    "        return mask\n",
    "\n",
    "    #  top-q =>  = quantile(conf, 1-q)\n",
    "    q = 1.0 - float(target_cover)\n",
    "    q = min(max(q, 0.0), 1.0)\n",
    "    thr_q = torch.quantile(conf_valid, q)\n",
    "\n",
    "    mask = mask & (conf >= thr_q)\n",
    "    return mask\n",
    "\n",
    "def masked_kl(student_logits, teacher_probs, mask):\n",
    "    # KL( teacher || student ) on masked pixels\n",
    "    if mask.sum().item() == 0:\n",
    "        return torch.tensor(0.0, device=student_logits.device)\n",
    "    logp_s = F.log_softmax(student_logits, dim=1)                 # (B,C,H,W)\n",
    "    kl_map = F.kl_div(logp_s, teacher_probs, reduction=\"none\").sum(1)  # (B,H,W)\n",
    "    return kl_map[mask].mean()\n",
    "\n",
    "# ---------- init student/teacher from supervised best ----------\n",
    "student = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    BACKBONE, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "teacher = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    BACKBONE, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "\n",
    "sup_state = torch.load(best_path, map_location=DEVICE)\n",
    "student.load_state_dict(sup_state)\n",
    "teacher.load_state_dict(sup_state)\n",
    "teacher.eval()\n",
    "for p in teacher.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "# ---------- training hyperparams ----------\n",
    "SEMI_EPOCHS = 10\n",
    "LR = 6e-5\n",
    "EMA_ALPHA = 0.999\n",
    "LAMBDA_U  = 0.5\n",
    "RAMP_E    = 10\n",
    "\n",
    "opt = torch.optim.AdamW(student.parameters(), lr=LR, weight_decay=0.01)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "semi_best2 = SEMI_DIR2 / \"best_state_dict_opt2.pt\"\n",
    "best_miou2 = -1.0\n",
    "\n",
    "unlab_iter = cycle(unlab_loader2)\n",
    "\n",
    "for ep in range(1, SEMI_EPOCHS+1):\n",
    "    lam_u = LAMBDA_U * rampup(ep-1, RAMP_E)\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "\n",
    "    cov_sum, cov_n = 0.0, 0\n",
    "    cls_cnt = torch.zeros(NUM_CLASSES, device=DEVICE)\n",
    "\n",
    "    for x_l, y_l, meta in tqdm(train_loader, desc=f\"[{EXP_NAME}] train ep{ep} (lam_u={lam_u:.2f})\", leave=False):\n",
    "        x_u, valid_u = next(unlab_iter)\n",
    "\n",
    "        x_l = x_l.to(DEVICE)              # (B,1,160,288)\n",
    "        y_l = y_l.to(DEVICE)              # (B,160,288)\n",
    "        x_u = x_u.to(DEVICE)              # (B,1,160,288)\n",
    "        valid_u = valid_u.to(DEVICE)      # (B,160,288) bool\n",
    "\n",
    "        x_l3 = x_l.repeat(1,3,1,1)\n",
    "        x_u_w = weak_aug(x_u).repeat(1,3,1,1)\n",
    "        x_u_s = strong_aug(x_u).repeat(1,3,1,1)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "            # labeled loss\n",
    "            logits_l = student(pixel_values=x_l3).logits\n",
    "            logits_l = upsample_logits(logits_l, y_l.shape[-2:])\n",
    "            loss_l = combo_loss(logits_l, y_l, dice_w=0.5)\n",
    "\n",
    "            # teacher soft probs + mask\n",
    "            with torch.no_grad():\n",
    "                logits_t = teacher(pixel_values=x_u_w).logits\n",
    "                logits_t = upsample_logits(logits_t, (H, W_PAD))\n",
    "                probs_t = torch.softmax(logits_t / TEMP, dim=1)   # (B,C,H,W)\n",
    "                conf, pseudo = torch.max(probs_t, dim=1)          # (B,H,W)\n",
    "\n",
    "                mask = build_mask_with_target_cover(conf, pseudo, valid_u, TARGET_COVER)\n",
    "\n",
    "            # unlabeled loss (KL)\n",
    "            if lam_u > 0:\n",
    "                logits_u = student(pixel_values=x_u_s).logits\n",
    "                logits_u = upsample_logits(logits_u, (H, W_PAD))\n",
    "                loss_u = masked_kl(logits_u, probs_t, mask)\n",
    "            else:\n",
    "                loss_u = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "            loss = loss_l + lam_u * loss_u\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(opt)\n",
    "        torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "\n",
    "        ema_update(teacher, student, EMA_ALPHA)\n",
    "\n",
    "        # monitor coverage + class ratio on masked pixels\n",
    "        with torch.no_grad():\n",
    "            denom = valid_u.float().sum().clamp(min=1.0)\n",
    "            cov = (mask.float().sum() / denom).item()\n",
    "            cov_sum += cov\n",
    "            cov_n += 1\n",
    "            if mask.sum().item() > 0:\n",
    "                cls_cnt += torch.bincount(pseudo[mask], minlength=NUM_CLASSES).float()\n",
    "\n",
    "    cov_avg = cov_sum / max(1, cov_n)\n",
    "    cls_ratio = (cls_cnt / cls_cnt.sum().clamp(min=1.0)).detach().cpu().numpy()\n",
    "    print(f\"[{EXP_NAME}] ep{ep:02d}: pseudo_cov={cov_avg:.3f}  pseudo_cls_ratio={cls_ratio}\")\n",
    "\n",
    "    # val\n",
    "    student.eval()\n",
    "    miou_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, meta in tqdm(val_loader, desc=f\"[{EXP_NAME}] val ep{ep}\", leave=False):\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            x3 = x.repeat(1,3,1,1)\n",
    "            logits = student(pixel_values=x3).logits\n",
    "            logits = upsample_logits(logits, y.shape[-2:])\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            miou_sum += float(mean_iou(pred, y).item()) * x.size(0)\n",
    "            n += x.size(0)\n",
    "    val_miou = miou_sum / max(1, n)\n",
    "    print(f\"[{EXP_NAME}] ep{ep:02d}/{SEMI_EPOCHS} val_mIoU={val_miou:.4f}\")\n",
    "\n",
    "    if val_miou > best_miou2:\n",
    "        best_miou2 = val_miou\n",
    "        torch.save(student.state_dict(), semi_best2)\n",
    "\n",
    "print(f\"[{EXP_NAME}] BEST val mIoU:\", best_miou2, \"saved:\", semi_best2)\n",
    "print(\"Outputs in:\", OUT_DIR2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2151b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample rows: 972 name_col: Unnamed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b2-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\782593947.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student2.load_state_dict(torch.load(semi_best2, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: exp_outputs\\Exp04_SEMI_opt2_Q30\\semi\\best_state_dict_opt2.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved npy predictions to: exp_outputs\\Exp04_SEMI_opt2_Q30\\test_predictions_opt2\n",
      "Saved submission: exp_outputs\\Exp04_SEMI_opt2_Q30\\y_test_submission_MATCH_SAMPLE_Exp04_SEMI_opt2_Q30.csv shape: (972, 43521)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8b  Predict test (sample order) + TTA + NEW submission name\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "sample = pd.read_csv(SAMPLE_SUB)\n",
    "name_col = sample.columns[0]\n",
    "ordered_names_raw = sample[name_col].astype(str).tolist()\n",
    "\n",
    "def norm_name(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    if s.lower().endswith(\".npy\"):\n",
    "        s = s[:-4]\n",
    "    return s\n",
    "\n",
    "ordered_names = [norm_name(n) for n in ordered_names_raw]\n",
    "print(\"sample rows:\", len(ordered_names), \"name_col:\", name_col)\n",
    "\n",
    "test_files = list_npy_files(X_TEST_DIR)\n",
    "test_index = {p.stem: p for p in test_files}\n",
    "test_index.update({p.stem.lower(): p for p in test_files})\n",
    "\n",
    "# load best semi v2\n",
    "student2 = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    BACKBONE, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "student2.load_state_dict(torch.load(semi_best2, map_location=DEVICE))\n",
    "student2.eval()\n",
    "print(\"Loaded:\", semi_best2)\n",
    "\n",
    "pred_dir2 = OUT_DIR2 / \"test_predictions_opt2\"\n",
    "pred_dir2.mkdir(parents=True, exist_ok=True)\n",
    "for p in pred_dir2.glob(\"*.npy\"):\n",
    "    p.unlink()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_logits_tta(x_pad_288: np.ndarray):\n",
    "    x_t = torch.from_numpy(x_pad_288).unsqueeze(0).unsqueeze(0).to(DEVICE)  # (1,1,160,288)\n",
    "    x3 = x_t.repeat(1,3,1,1)\n",
    "\n",
    "    logits1 = student2(pixel_values=x3).logits\n",
    "    logits1 = upsample_logits(logits1, (H, W_PAD))  # (1,C,160,288)\n",
    "\n",
    "    x3f = torch.flip(x3, dims=[3])\n",
    "    logits2 = student2(pixel_values=x3f).logits\n",
    "    logits2 = upsample_logits(logits2, (H, W_PAD))\n",
    "    logits2 = torch.flip(logits2, dims=[3])\n",
    "\n",
    "    return (0.5 * (logits1 + logits2)).squeeze(0).cpu()  # (C,160,288)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name in tqdm(ordered_names, desc=f\"[{EXP_NAME}] predict test (TTA)\", leave=False):\n",
    "        key = name if name in test_index else name.lower()\n",
    "        if key not in test_index:\n",
    "            hits = list(X_TEST_DIR.rglob(f\"{name}.npy\")) + list(X_TEST_DIR.rglob(f\"{name}.NPY\"))\n",
    "            if len(hits) == 0:\n",
    "                raise FileNotFoundError(f\"X_test missing: {name}.npy\")\n",
    "            x_path = hits[0]\n",
    "        else:\n",
    "            x_path = test_index[key]\n",
    "\n",
    "        x = load_x(x_path)          # (160,w)\n",
    "        w = x.shape[1]\n",
    "        x_pad = pad_x_to_wpad(x)    # (160,288)\n",
    "\n",
    "        logits = predict_logits_tta(x_pad)  # (C,160,288)\n",
    "        pred = torch.argmax(logits, dim=0).numpy().astype(np.int64)  # (160,288)\n",
    "        pred = pred[:, :w]\n",
    "        np.save(pred_dir2 / f\"{name}.npy\", pred)\n",
    "\n",
    "print(\"saved npy predictions to:\", pred_dir2)\n",
    "\n",
    "# ---- build submission (match sample) ----\n",
    "size_labels = 272\n",
    "flat_len = H * size_labels\n",
    "\n",
    "pred_map = {}\n",
    "for p in pred_dir2.glob(\"*.npy\"):\n",
    "    nm = p.stem\n",
    "    pred = np.load(p)  # (160,160) or (160,272)\n",
    "    if pred.shape[1] != size_labels:\n",
    "        aux = -1 + np.zeros(flat_len, dtype=np.int64)\n",
    "        aux[0:H*H] = pred.flatten()\n",
    "    else:\n",
    "        aux = pred.flatten().astype(np.int64)\n",
    "    pred_map[nm] = aux\n",
    "\n",
    "missing = [n for n in ordered_names if n not in pred_map]\n",
    "assert len(missing) == 0, f\"missing predictions: {missing[:10]}\"\n",
    "\n",
    "data = np.stack([pred_map[n] for n in ordered_names], axis=0)\n",
    "col_names = [str(i) for i in range(flat_len)]\n",
    "sub_df = pd.DataFrame(data, columns=col_names)\n",
    "sub_df.insert(0, name_col, ordered_names_raw)\n",
    "\n",
    "out_csv2 = OUT_DIR2 / f\"y_test_submission_MATCH_SAMPLE_{EXP_NAME}.csv\"\n",
    "sub_df.to_csv(out_csv2, index=False)\n",
    "print(\"Saved submission:\", out_csv2, \"shape:\", sub_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209b6433",
   "metadata": {},
   "source": [
    "## Exprience semi-supervise amliore opt3 -- Pseudo-tiquettes de premier plan uniquement + KL masqu + (tape d'infrence) TTA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebd7cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b2-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b2-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\1330001677.py:118: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sup_state = torch.load(best_path, map_location=DEVICE)\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\1330001677.py:133: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
      "[Exp04_SEMI_opt3_FGOnly_Q15] train ep1 (lam_u=0.10):   0%|          | 0/516 [00:00<?, ?it/s]C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\1330001677.py:161: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep01: pseudo_cov=0.026  pseudo_cls_ratio=[0.         0.01619996 0.98380005]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep01/10 val_mIoU=0.7891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep02: pseudo_cov=0.026  pseudo_cls_ratio=[0.        0.0164028 0.9835972]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep02/10 val_mIoU=0.7992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep03: pseudo_cov=0.027  pseudo_cls_ratio=[0.         0.01387506 0.98612493]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep03/10 val_mIoU=0.7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep04: pseudo_cov=0.027  pseudo_cls_ratio=[0.         0.01628043 0.9837196 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep04/10 val_mIoU=0.7904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep05: pseudo_cov=0.027  pseudo_cls_ratio=[0.         0.01227641 0.9877236 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep05/10 val_mIoU=0.7820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep06: pseudo_cov=0.027  pseudo_cls_ratio=[0.         0.01330168 0.9866983 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep06/10 val_mIoU=0.7885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep07: pseudo_cov=0.027  pseudo_cls_ratio=[0.         0.02521074 0.97478926]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep07/10 val_mIoU=0.7909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep08: pseudo_cov=0.027  pseudo_cls_ratio=[0.         0.03387412 0.96612585]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep08/10 val_mIoU=0.7779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep09: pseudo_cov=0.027  pseudo_cls_ratio=[0.         0.03421669 0.9657833 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep09/10 val_mIoU=0.7826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep10: pseudo_cov=0.027  pseudo_cls_ratio=[0.         0.05288752 0.9471125 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Exp04_SEMI_opt3_FGOnly_Q15] ep10/10 val_mIoU=0.7940\n",
      "[Exp04_SEMI_opt3_FGOnly_Q15] BEST val mIoU: 0.7991736680269241 saved: exp_outputs\\Exp04_SEMI_opt3_FGOnly_Q15\\semi\\best_state_dict_opt3.pt\n",
      "Outputs in: exp_outputs\\Exp04_SEMI_opt3_FGOnly_Q15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Cell 7c  Semi-supervised opt3: Foreground-only pseudo label (EMA Teacher) [NEW EXP]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "EXP_NAME3 = \"Exp04_SEMI_opt3_FGOnly_Q15\"\n",
    "OUT_DIR3 = Path(rf\"exp_outputs\\{EXP_NAME3}\")\n",
    "OUT_DIR3.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEMI_DIR3 = OUT_DIR3 / \"semi\"\n",
    "SEMI_DIR3.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- unlabeled loader ----------\n",
    "class UnlabeledDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        w = int(row[\"w\"])\n",
    "        x = load_x(Path(row[\"path\"]))\n",
    "        x = pad_x_to_wpad(x)                      # (160,288)\n",
    "        x_t = torch.from_numpy(x).unsqueeze(0)    # (1,160,288)\n",
    "        valid = torch.from_numpy(make_valid_mask(w))  # (160,288) bool\n",
    "        return x_t, valid\n",
    "\n",
    "unlab_loader3 = DataLoader(\n",
    "    UnlabeledDataset(unlab_df),\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=(DEVICE==\"cuda\")\n",
    ")\n",
    "\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for b in loader:\n",
    "            yield b\n",
    "\n",
    "# ---------- augmentations () ----------\n",
    "def weak_aug(x: torch.Tensor) -> torch.Tensor:\n",
    "    B = x.size(0)\n",
    "    a = torch.empty((B,1,1,1), device=x.device).uniform_(0.97, 1.03)\n",
    "    b = torch.empty((B,1,1,1), device=x.device).uniform_(-0.02, 0.02)\n",
    "    return torch.clamp(x * a + b, 0.0, 1.0)\n",
    "\n",
    "def strong_aug(x: torch.Tensor) -> torch.Tensor:\n",
    "    B, _, Hh, Ww = x.shape\n",
    "    a = torch.empty((B,1,1,1), device=x.device).uniform_(0.90, 1.10)\n",
    "    b = torch.empty((B,1,1,1), device=x.device).uniform_(-0.05, 0.05)\n",
    "    out = torch.clamp(x * a + b, 0.0, 1.0)\n",
    "    sigma = torch.empty((B,1,1,1), device=x.device).uniform_(0.0, 0.03)\n",
    "    out = torch.clamp(out + torch.randn_like(out) * sigma, 0.0, 1.0)\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def ema_update(teacher, student, alpha: float):\n",
    "    for t_p, s_p in zip(teacher.parameters(), student.parameters()):\n",
    "        t_p.data.mul_(alpha).add_(s_p.data, alpha=1.0 - alpha)\n",
    "    for t_b, s_b in zip(teacher.buffers(), student.buffers()):\n",
    "        t_b.copy_(s_b)\n",
    "\n",
    "def rampup(epoch0: int, ramp_epochs: int):\n",
    "    if ramp_epochs <= 0:\n",
    "        return 1.0\n",
    "    return min(1.0, float(epoch0 + 1) / float(ramp_epochs))\n",
    "\n",
    "# ---------- FG-only mask builder ----------\n",
    "TAU_BY_CLASS = torch.tensor([0.999, 0.93, 0.93], device=DEVICE)  # bgFG-only1/2\n",
    "FG_TARGET_COVER = 0.15  #  top 15% 0.10/0.20\n",
    "TEMP = 1.0              #  sharpen\n",
    "\n",
    "def build_fg_mask(conf, pseudo, valid, fg_target_cover: float):\n",
    "    \"\"\"\n",
    "     (pseudo!=0)  quantile \n",
    "    \"\"\"\n",
    "    fg = (pseudo != 0) & valid\n",
    "    if fg.sum().item() == 0:\n",
    "        return torch.zeros_like(fg, dtype=torch.bool)\n",
    "\n",
    "    thr_base = TAU_BY_CLASS[pseudo]             # (B,H,W)\n",
    "    mask = fg & (conf >= thr_base)\n",
    "\n",
    "    # quantile in FG only\n",
    "    conf_fg = conf[fg]\n",
    "    if conf_fg.numel() == 0:\n",
    "        return torch.zeros_like(fg, dtype=torch.bool)\n",
    "\n",
    "    q = 1.0 - float(fg_target_cover)\n",
    "    q = min(max(q, 0.0), 1.0)\n",
    "    thr_q = torch.quantile(conf_fg, q)\n",
    "    mask = mask & (conf >= thr_q)\n",
    "    return mask\n",
    "\n",
    "def masked_kl(student_logits, teacher_probs, mask, fg_weight=2.0):\n",
    "    \"\"\"\n",
    "    maskKLmaskFG-only\n",
    "    \"\"\"\n",
    "    if mask.sum().item() == 0:\n",
    "        return torch.tensor(0.0, device=student_logits.device)\n",
    "    logp_s = F.log_softmax(student_logits, dim=1)                    # (B,C,H,W)\n",
    "    kl_map = F.kl_div(logp_s, teacher_probs, reduction=\"none\").sum(1)  # (B,H,W)\n",
    "    return fg_weight * kl_map[mask].mean()\n",
    "\n",
    "# ---------- init student/teacher from supervised best ----------\n",
    "student3 = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    BACKBONE, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "teacher3 = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    BACKBONE, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "\n",
    "sup_state = torch.load(best_path, map_location=DEVICE)\n",
    "student3.load_state_dict(sup_state)\n",
    "teacher3.load_state_dict(sup_state)\n",
    "teacher3.eval()\n",
    "for p in teacher3.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "# ---------- training hyperparams ----------\n",
    "SEMI_EPOCHS = 10\n",
    "LR = 6e-5\n",
    "EMA_ALPHA = 0.999\n",
    "LAMBDA_U  = 0.8      # FG-only\n",
    "RAMP_E    = 8\n",
    "\n",
    "opt = torch.optim.AdamW(student3.parameters(), lr=LR, weight_decay=0.01)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\"))\n",
    "\n",
    "semi_best3 = SEMI_DIR3 / \"best_state_dict_opt3.pt\"\n",
    "best_miou3 = -1.0\n",
    "\n",
    "unlab_iter = cycle(unlab_loader3)\n",
    "\n",
    "for ep in range(1, SEMI_EPOCHS+1):\n",
    "    lam_u = LAMBDA_U * rampup(ep-1, RAMP_E)\n",
    "    student3.train()\n",
    "    teacher3.eval()\n",
    "\n",
    "    cov_sum, cov_n = 0.0, 0\n",
    "    cls_cnt = torch.zeros(NUM_CLASSES, device=DEVICE)\n",
    "\n",
    "    for x_l, y_l, meta in tqdm(train_loader, desc=f\"[{EXP_NAME3}] train ep{ep} (lam_u={lam_u:.2f})\", leave=False):\n",
    "        x_u, valid_u = next(unlab_iter)\n",
    "\n",
    "        x_l = x_l.to(DEVICE)\n",
    "        y_l = y_l.to(DEVICE)\n",
    "        x_u = x_u.to(DEVICE)\n",
    "        valid_u = valid_u.to(DEVICE)\n",
    "\n",
    "        x_l3 = x_l.repeat(1,3,1,1)\n",
    "        x_u_w = weak_aug(x_u).repeat(1,3,1,1)\n",
    "        x_u_s = strong_aug(x_u).repeat(1,3,1,1)\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "            # labeled\n",
    "            logits_l = student3(pixel_values=x_l3).logits\n",
    "            logits_l = upsample_logits(logits_l, y_l.shape[-2:])\n",
    "            loss_l = combo_loss(logits_l, y_l, dice_w=0.5)\n",
    "\n",
    "            # teacher\n",
    "            with torch.no_grad():\n",
    "                logits_t = teacher3(pixel_values=x_u_w).logits\n",
    "                logits_t = upsample_logits(logits_t, (H, W_PAD))\n",
    "                probs_t = torch.softmax(logits_t / TEMP, dim=1)\n",
    "                conf, pseudo = torch.max(probs_t, dim=1)\n",
    "\n",
    "                mask_fg = build_fg_mask(conf, pseudo, valid_u, FG_TARGET_COVER)\n",
    "\n",
    "            # unlabeled (FG-only KL)\n",
    "            if lam_u > 0:\n",
    "                logits_u = student3(pixel_values=x_u_s).logits\n",
    "                logits_u = upsample_logits(logits_u, (H, W_PAD))\n",
    "                loss_u = masked_kl(logits_u, probs_t, mask_fg, fg_weight=2.0)\n",
    "            else:\n",
    "                loss_u = torch.tensor(0.0, device=DEVICE)\n",
    "\n",
    "            loss = loss_l + lam_u * loss_u\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(opt)\n",
    "        torch.nn.utils.clip_grad_norm_(student3.parameters(), 1.0)\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "\n",
    "        ema_update(teacher3, student3, EMA_ALPHA)\n",
    "\n",
    "        # monitor\n",
    "        with torch.no_grad():\n",
    "            denom = valid_u.float().sum().clamp(min=1.0)\n",
    "            cov = (mask_fg.float().sum() / denom).item()\n",
    "            cov_sum += cov\n",
    "            cov_n += 1\n",
    "            if mask_fg.sum().item() > 0:\n",
    "                cls_cnt += torch.bincount(pseudo[mask_fg], minlength=NUM_CLASSES).float()\n",
    "\n",
    "    cov_avg = cov_sum / max(1, cov_n)\n",
    "    cls_ratio = (cls_cnt / cls_cnt.sum().clamp(min=1.0)).detach().cpu().numpy()\n",
    "    print(f\"[{EXP_NAME3}] ep{ep:02d}: pseudo_cov={cov_avg:.3f}  pseudo_cls_ratio={cls_ratio}\")\n",
    "\n",
    "    # val\n",
    "    student3.eval()\n",
    "    miou_sum, n = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, meta in tqdm(val_loader, desc=f\"[{EXP_NAME3}] val ep{ep}\", leave=False):\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            x3 = x.repeat(1,3,1,1)\n",
    "            logits = student3(pixel_values=x3).logits\n",
    "            logits = upsample_logits(logits, y.shape[-2:])\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            miou_sum += float(mean_iou(pred, y).item()) * x.size(0)\n",
    "            n += x.size(0)\n",
    "    val_miou = miou_sum / max(1, n)\n",
    "    print(f\"[{EXP_NAME3}] ep{ep:02d}/{SEMI_EPOCHS} val_mIoU={val_miou:.4f}\")\n",
    "\n",
    "    if val_miou > best_miou3:\n",
    "        best_miou3 = val_miou\n",
    "        torch.save(student3.state_dict(), semi_best3)\n",
    "\n",
    "print(f\"[{EXP_NAME3}] BEST val mIoU:\", best_miou3, \"saved:\", semi_best3)\n",
    "print(\"Outputs in:\", OUT_DIR3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d3111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample rows: 972 name_col: Unnamed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b2-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([3, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_10968\\3104168265.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student_pred.load_state_dict(torch.load(semi_best3, map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: exp_outputs\\Exp04_SEMI_opt3_FGOnly_Q15\\semi\\best_state_dict_opt3.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved npy predictions to: exp_outputs\\Exp04_SEMI_opt3_FGOnly_Q15\\test_predictions_opt3\n",
      "Saved submission: exp_outputs\\Exp04_SEMI_opt3_FGOnly_Q15\\y_test_submission_MATCH_SAMPLE_Exp04_SEMI_opt3_FGOnly_Q15.csv shape: (972, 43521)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8c  Predict test (sample order) + TTA + NEW submission name (opt3)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "sample = pd.read_csv(SAMPLE_SUB)\n",
    "name_col = sample.columns[0]\n",
    "ordered_names_raw = sample[name_col].astype(str).tolist()\n",
    "\n",
    "def norm_name(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    if s.lower().endswith(\".npy\"):\n",
    "        s = s[:-4]\n",
    "    return s\n",
    "\n",
    "ordered_names = [norm_name(n) for n in ordered_names_raw]\n",
    "print(\"sample rows:\", len(ordered_names), \"name_col:\", name_col)\n",
    "\n",
    "test_files = list_npy_files(X_TEST_DIR)\n",
    "test_index = {p.stem: p for p in test_files}\n",
    "test_index.update({p.stem.lower(): p for p in test_files})\n",
    "\n",
    "# load best opt3\n",
    "student_pred = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    BACKBONE, num_labels=NUM_CLASSES, ignore_mismatched_sizes=True\n",
    ").to(DEVICE)\n",
    "student_pred.load_state_dict(torch.load(semi_best3, map_location=DEVICE))\n",
    "student_pred.eval()\n",
    "print(\"Loaded:\", semi_best3)\n",
    "\n",
    "pred_dir3 = OUT_DIR3 / \"test_predictions_opt3\"\n",
    "pred_dir3.mkdir(parents=True, exist_ok=True)\n",
    "for p in pred_dir3.glob(\"*.npy\"):\n",
    "    p.unlink()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_logits_tta(x_pad_288: np.ndarray):\n",
    "    x_t = torch.from_numpy(x_pad_288).unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "    x3 = x_t.repeat(1,3,1,1)\n",
    "\n",
    "    logits1 = student_pred(pixel_values=x3).logits\n",
    "    logits1 = upsample_logits(logits1, (H, W_PAD))\n",
    "\n",
    "    x3f = torch.flip(x3, dims=[3])\n",
    "    logits2 = student_pred(pixel_values=x3f).logits\n",
    "    logits2 = upsample_logits(logits2, (H, W_PAD))\n",
    "    logits2 = torch.flip(logits2, dims=[3])\n",
    "\n",
    "    return (0.5 * (logits1 + logits2)).squeeze(0).cpu()  # (C,160,288)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name in tqdm(ordered_names, desc=f\"[{EXP_NAME3}] predict test (TTA)\", leave=False):\n",
    "        key = name if name in test_index else name.lower()\n",
    "        if key not in test_index:\n",
    "            hits = list(X_TEST_DIR.rglob(f\"{name}.npy\")) + list(X_TEST_DIR.rglob(f\"{name}.NPY\"))\n",
    "            if len(hits) == 0:\n",
    "                raise FileNotFoundError(f\"X_test missing: {name}.npy\")\n",
    "            x_path = hits[0]\n",
    "        else:\n",
    "            x_path = test_index[key]\n",
    "\n",
    "        x = load_x(x_path)         # (160,w)\n",
    "        w = x.shape[1]\n",
    "        x_pad = pad_x_to_wpad(x)   # (160,288)\n",
    "\n",
    "        logits = predict_logits_tta(x_pad)\n",
    "        pred = torch.argmax(logits, dim=0).numpy().astype(np.int64)  # (160,288)\n",
    "        pred = pred[:, :w]\n",
    "        np.save(pred_dir3 / f\"{name}.npy\", pred)\n",
    "\n",
    "print(\"saved npy predictions to:\", pred_dir3)\n",
    "\n",
    "# build submission\n",
    "size_labels = 272\n",
    "flat_len = H * size_labels\n",
    "\n",
    "pred_map = {}\n",
    "for p in pred_dir3.glob(\"*.npy\"):\n",
    "    nm = p.stem\n",
    "    pred = np.load(p)\n",
    "    if pred.shape[1] != size_labels:\n",
    "        aux = -1 + np.zeros(flat_len, dtype=np.int64)\n",
    "        aux[0:H*H] = pred.flatten()\n",
    "    else:\n",
    "        aux = pred.flatten().astype(np.int64)\n",
    "    pred_map[nm] = aux\n",
    "\n",
    "missing = [n for n in ordered_names if n not in pred_map]\n",
    "assert len(missing) == 0, f\"missing predictions: {missing[:10]}\"\n",
    "\n",
    "data = np.stack([pred_map[n] for n in ordered_names], axis=0)\n",
    "col_names = [str(i) for i in range(flat_len)]\n",
    "sub_df = pd.DataFrame(data, columns=col_names)\n",
    "sub_df.insert(0, name_col, ordered_names_raw)\n",
    "\n",
    "out_csv3 = OUT_DIR3 / f\"y_test_submission_MATCH_SAMPLE_{EXP_NAME3}.csv\"\n",
    "sub_df.to_csv(out_csv3, index=False)\n",
    "print(\"Saved submission:\", out_csv3, \"shape:\", sub_df.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DEEPCUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
